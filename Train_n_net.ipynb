{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bUILT-iN pACKAGES\n",
    "import sys\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "## lAST sUMMER\n",
    "from romtoslp import *\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "import MatDB\n",
    "import time\n",
    "import bz2\n",
    "import zlib\n",
    "\n",
    "## lAST yEAR\n",
    "# from word_definite import *\n",
    "# from nnet import *\n",
    "# from heap_n_PrimMST import *\n",
    "# from word_definite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import winsound\n",
    "def playBeep():\n",
    "    for i in range(3):\n",
    "        winsound.Beep(2200, 300)\n",
    "        winsound.Beep(2600, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'), encoding=u'utf-8')\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'), encoding=u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_4k_1k = pickle.load(open('../SmallDataset_4K_1K.p', 'rb'))\n",
    "TrainFiles = dataset_4k_1k['TrainFiles']\n",
    "TestFiles = dataset_4k_1k['TestFiles']\n",
    "\n",
    "dataset_6k_3k = pickle.load(open('../SmallDataset_6K_3K.p', 'rb'))\n",
    "TrainFiles_2 = dataset_6k_3k['TrainFiles']\n",
    "TestFiles_2 = dataset_6k_3k['TestFiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matDB = MatDB.MatDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from MatDB import *\n",
    "import word_definite as WD\n",
    "from heap_n_PrimMST import *\n",
    "from nnet import *\n",
    "\"\"\"\n",
    "################################################################################################\n",
    "######################  CREATE SEVERAL DATA STRUCTURES FROM SENTENCE/DCS  ######################\n",
    "###########################  NODELIST, ADJACENCY LIST, GRAPH, HEAP #############################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "def GetTrainingKit(sentenceObj, dcsObj):\n",
    "    nodelist = GetNodes(sentenceObj)\n",
    "    \n",
    "    # Nodelist with only the correct_nodes\n",
    "    nodelist2 = GetNodes(sentenceObj)\n",
    "    nodelist2_to_correct_mapping = {}\n",
    "    nodelist_correct = []\n",
    "    search_key = 0\n",
    "    first_key = 0\n",
    "    for chunk_id in range(len(dcsObj.lemmas)):\n",
    "        while nodelist2[first_key].chunk_id != chunk_id:\n",
    "            first_key += 1\n",
    "        for j in range(len(dcsObj.lemmas[chunk_id])):\n",
    "            search_key = first_key\n",
    "            while (nodelist2[search_key].lemma != rom_slp(dcsObj.lemmas[chunk_id][j])) or (nodelist2[search_key].cng != dcsObj.cng[chunk_id][j]):\n",
    "                search_key += 1\n",
    "                if search_key >= len(nodelist2) or nodelist2[search_key].chunk_id > chunk_id:\n",
    "                    break\n",
    "    #         print((rom_slp(dcsObj.lemmas[chunk_id][j]), dcsObj.cng[chunk_id][j]))\n",
    "    #         print(nodelist[search_key])\n",
    "            nodelist2_to_correct_mapping[len(nodelist_correct)] = search_key\n",
    "            nodelist_correct.append(nodelist2[search_key])\n",
    "    return (nodelist, nodelist_correct, nodelist2_to_correct_mapping)\n",
    "    \n",
    "\n",
    "def GetGraph(nodelist, neuralnet):\n",
    "    if not neuralnet.outer_relu:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat, SigmoidGateOutput)\n",
    "    else:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nEURAL nET wILL bE sAVED hERE:  outputs/train_nnet_t506193514122.p\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  GET A FILENAME TO SAVE WEIGHTS  ################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "import time\n",
    "st = str(int((time.time() * 1e6) % 1e13))\n",
    "log_name = 'logs/train_nnet_t{}.out'.format(st)\n",
    "p_name = 'outputs/train_nnet_t{}.p'.format(st)\n",
    "print('nEURAL nET wILL bE sAVED hERE: ', p_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingStatus = defaultdict(lambda: bool(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  TRAIN FUNCTION  ################################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "def train(loaded_SKT, loaded_DCS, n_trainset = -1, iterationPerBatch = 10, filePerBatch = 20, _debug = True):\n",
    "    # Train\n",
    "    if n_trainset == -1:\n",
    "        totalBatchToTrain = 20\n",
    "    else:\n",
    "        totalBatchToTrain = math.ceil(n_trainset/filePerBatch)\n",
    "    \n",
    "    for iterout in range(totalBatchToTrain):\n",
    "        # Change current batch\n",
    "        trainer.Save(p_name)\n",
    "        print('Batch: ', iterout)\n",
    "        files_for_batch = TrainFiles[iterout*filePerBatch:(iterout + 1)*filePerBatch]\n",
    "        print(files_for_batch)\n",
    "        # trainer.Load('outputs/neuralnet_trained.p')\n",
    "        \n",
    "        # Run few times on same set of files\n",
    "        for iterin in range(iterationPerBatch):\n",
    "            print('ITERATION IN', iterin)        \n",
    "            for fn in files_for_batch:\n",
    "                sentenceObj = loaded_SKT[fn]\n",
    "                dcsObj = loaded_DCS[fn]\n",
    "                if trainingStatus[sentenceObj.sent_id]:\n",
    "                    continue\n",
    "                # trainer.Save('outputs/saved_trainer.p')\n",
    "                try:\n",
    "                    trainer.Train(sentenceObj, dcsObj, _debug)\n",
    "                except (IndexError, KeyError) as e:\n",
    "                    print('\\x1b[31mFailed: {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "            sys.stdout.flush() # Flush IO buffer \n",
    "    trainer.Save(p_name)\n",
    "    \n",
    "    sys.stdout.flush() # Flush IO buffer \n",
    "                \n",
    "def test(loaded_SKT, loaded_DCS, n_testSet = -1, _testFiles = None, n_checkpt = 100):\n",
    "    total_lemma = 0;\n",
    "    correct_lemma = 0;\n",
    "\n",
    "    total_word = 0;\n",
    "    total_output_nodes = 0\n",
    "    correct_word = 0;\n",
    "    file_counter = 0\n",
    "    if _testFiles is None:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = TestFiles\n",
    "        else:\n",
    "            _testFiles = TestFiles[0:n_testSet]\n",
    "    else:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = _testFiles\n",
    "        else:\n",
    "            _testFiles = _testFiles[0:n_testSet]\n",
    "            \n",
    "    recalls = []\n",
    "    recalls_of_word = []\n",
    "    precisions = []\n",
    "    precisions_of_words = []\n",
    "    for fn in _testFiles:\n",
    "        if file_counter % n_checkpt == 0:\n",
    "            print(file_counter,' Checkpoint... ')\n",
    "            sys.stdout.flush() # Flush IO buffer \n",
    "        file_counter += 1\n",
    "        sentenceObj = loaded_SKT[fn]\n",
    "        dcsObj = loaded_DCS[fn]        \n",
    "        try:\n",
    "            (word_match, lemma_match, n_dcsWords, n_output_nodes) = trainer.Test(sentenceObj, dcsObj)\n",
    "            \n",
    "            recalls.append(lemma_match/n_dcsWords)\n",
    "            recalls_of_word.append(word_match/n_dcsWords)\n",
    "            \n",
    "            precisions.append(lemma_match/n_output_nodes)\n",
    "            precisions_of_words.append(word_match/n_output_nodes)\n",
    "            \n",
    "            total_lemma += n_dcsWords\n",
    "            total_word += n_dcsWords\n",
    "            \n",
    "            total_output_nodes += n_output_nodes            \n",
    "            \n",
    "            correct_lemma += lemma_match\n",
    "            correct_word += word_match\n",
    "        except (IndexError, KeyError) as e:\n",
    "            print('Failed!')        \n",
    "\n",
    "    print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "    print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "    print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "    print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))\n",
    "    \n",
    "    return (recalls, recalls_of_word, precisions, precisions_of_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOW-OLD FUNCTION\n",
    "# def GetLoss(_mst_adj_graph, _mask_de_correct_edges, _negLogLikelies):\n",
    "#     _negLogLikelies = _negLogLikelies.copy()\n",
    "#     _negLogLikelies[~_mst_adj_graph] = 0\n",
    "#     _negLogLikelies[~_mask_de_correct_edges] *= -1 # BAKA!!! Check before you try to fix this again\n",
    "#     return np.sum(_negLogLikelies)    \n",
    "\n",
    "# NEW FUNCTION\n",
    "def GetLoss(_mst_adj_graph, _mask_de_correct_edges, _WScalarMat):\n",
    "    _WScalarMat = _WScalarMat.copy()\n",
    "    _WScalarMat[_mst_adj_graph&(~_mask_de_correct_edges)] *= -1 # BAKA!!! Check before you try to fix this again\n",
    "    _WScalarMat[~_mst_adj_graph] = 0\n",
    "    return np.sum(_WScalarMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.hidden_layer_size = 300\n",
    "        self._edge_vector_dim = WD._edge_vector_dim\n",
    "        # self._full_cnglist = list(WD.mat_cngCount_1D)\n",
    "        self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True)\n",
    "        self.history = defaultdict(lambda: list())\n",
    "        \n",
    "    def Reset(self):\n",
    "        self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size)\n",
    "        self.history = defaultdict(lambda: list())\n",
    "        \n",
    "    def Save(self, filename):\n",
    "        #pickle.dump({'nnet': self.neuralnet, 'history': dict(self.history)}, open(filename, 'wb'))\n",
    "        return\n",
    "        \n",
    "    \n",
    "    def Load(self, filename):\n",
    "        loader = pickle.load(open(filename, 'rb'))\n",
    "        self.neuralnet.U = loader['U']\n",
    "        self.neuralnet.W = loader['W']\n",
    "        \n",
    "    def Test(self, sentenceObj, dcsObj):\n",
    "        neuralnet = self.neuralnet\n",
    "        minScore = np.inf\n",
    "        minMst = None\n",
    "        \n",
    "        # startT = time.time()\n",
    "        try:\n",
    "            (nodelist, nodelist_correct, _) = GetTrainingKit(sentenceObj, dcsObj)\n",
    "            # nodelist = GetNodes(sentenceObj)\n",
    "        except IndexError:\n",
    "            print('\\x1b[31mError with {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "            return (0, 0, 0)\n",
    "            \n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "        conflicts_Dict_correct = Get_Conflicts(nodelist_correct)\n",
    "        \n",
    "        # print('Load Nodelist + GetConflicts Time: ', time.time() - startT)\n",
    "        startT = time.time()\n",
    "        \n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "        featVMat_correct = Get_Feat_Vec_Matrix(nodelist_correct, conflicts_Dict_correct)\n",
    "        \n",
    "        print('Get Edge Features Time: ', time.time() - startT)\n",
    "        # startT = time.time()\n",
    "        \n",
    "        if not self.neuralnet.outer_relu:\n",
    "            (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        else:\n",
    "            WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        \n",
    "        # print('NeuralNet Time: ', time.time() - startT)\n",
    "        # startT = time.time()\n",
    "        \n",
    "        # Get all MST\n",
    "        for source in range(len(nodelist)):\n",
    "            (mst_nodes, mst_adj_graph, _) = MST(nodelist, WScalarMat, conflicts_Dict, source)\n",
    "            # print('.', end = '')\n",
    "            score = GetMSTWeight(mst_nodes, WScalarMat)\n",
    "            if(score < minScore):\n",
    "                minScore = score\n",
    "                minMst = mst_nodes\n",
    "        dcsLemmas = [[rom_slp(l) for l in arr]for arr in dcsObj.lemmas]\n",
    "        word_match = 0\n",
    "        lemma_match = 0\n",
    "        n_output_nodes = 0\n",
    "        for chunk_id, wdSplit in minMst.items():\n",
    "            for wd in wdSplit:\n",
    "                n_output_nodes += 1\n",
    "                # Match lemma\n",
    "                search_result = [i for i, j in enumerate(dcsLemmas[chunk_id]) if j == wd.lemma]\n",
    "                if len(search_result) > 0:\n",
    "                    lemma_match += 1\n",
    "                # Match CNG\n",
    "                for i in search_result:\n",
    "                    if(dcsObj.cng[chunk_id][i] == str(wd.cng)):\n",
    "                        word_match += 1\n",
    "                        # print(wd.lemma, wd.cng)\n",
    "                        break\n",
    "        dcsLemmas = [l for arr in dcsObj.lemmas for l in arr]\n",
    "        \n",
    "        # print('All MST Time: ', time.time() - startT)\n",
    "        # print('Node Count: ', len(nodelist))\n",
    "#         print('\\nFull Match: {}, Partial Match: {}, OutOf {}, NodeCount: {}, '.\\\n",
    "#               format(word_match, lemma_match, len(dcsLemmas), len(nodelist)))\n",
    "        return (word_match, lemma_match, len(dcsLemmas), n_output_nodes)\n",
    "    \n",
    "    def Train(self, sentenceObj, dcsObj, _debug = True):\n",
    "        # Hyperparameter for hinge loss: m\n",
    "        M_hinge = 14\n",
    "        \n",
    "        \"\"\" Pre-Process DCS and SKT to get all Nodes etc. \"\"\"\n",
    "        try:\n",
    "            (nodelist, nodelist_correct, nodelist_to_correct_mapping) = GetTrainingKit(sentenceObj, dcsObj)\n",
    "        except IndexError as e:\n",
    "            # print('\\x1b[31mError with {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "            # print(e)\n",
    "            return\n",
    "        \n",
    "        \"\"\" FORM MAXIMUM(ENERGY) SPANNING TREE OF THE GOLDEN GRAPH : WORST GOLD STRUCTURE \"\"\"\n",
    "        if not self.neuralnet.outer_relu:\n",
    "            raise Exception('Support for Non-outer-relu removed')\n",
    "        else:\n",
    "            (conflicts_Dict_correct, featVMat_correct, WScalarMat_correct) = GetGraph(nodelist_correct, self.neuralnet)\n",
    "        source = 0\n",
    "        \n",
    "        \"\"\" Find the max spanning tree : negative Weight matrix passed \"\"\"\n",
    "        (max_st_gold_ndict, max_st_adj_gold_small, _) = MST(nodelist_correct, -WScalarMat_correct, conflicts_Dict_correct, source)\n",
    "        energy_gold_max_ST = np.sum(WScalarMat_correct[max_st_adj_gold_small])\n",
    "        \n",
    "        \"\"\" Convert correct spanning tree graph adj matrix to full marix dimensions \"\"\"\n",
    "        \"\"\" Create full-size adjacency matrix for correct_mst_small \"\"\"\n",
    "        nodelen = len(nodelist)\n",
    "        max_st_adj_gold = np.ndarray((nodelen, nodelen), np.bool)*False # T_STAR\n",
    "        for i in range(max_st_adj_gold_small.shape[0]):\n",
    "            for j in range(max_st_adj_gold_small.shape[1]):\n",
    "                max_st_adj_gold[nodelist_to_correct_mapping[i], nodelist_to_correct_mapping[j]] = max_st_adj_gold_small[i, j]\n",
    "        \n",
    "        \"\"\" Delta(Margin) Function : MASK FOR WHICH NODES IN NODELIST BELONG TO DCS \"\"\"\n",
    "        gold_nodes_mask = np.array([False]*len(nodelist))\n",
    "        for i in range(len(nodelist_correct)):\n",
    "            gold_nodes_mask[nodelist_to_correct_mapping[nodelist_correct[i].id]] = True\n",
    "        margin_f = lambda nodes_mask: np.sum(nodes_mask&gold_nodes_mask)**1.7\n",
    "        \n",
    "        \"\"\" FOR ALL POSSIBLE MST FROM THE COMPLETE GRAPH \"\"\"\n",
    "        if not self.neuralnet.outer_relu:\n",
    "            raise Exception('Support for Non-outer-relu removed')\n",
    "        else:\n",
    "            (conflicts_Dict, featVMat, WScalarMat) = GetGraph(nodelist, self.neuralnet)\n",
    "\n",
    "        \"\"\" For each node - Find MST with that source\"\"\"\n",
    "        min_STx = None # Min Energy spanning tree with worst margin with gold_STx\n",
    "        min_marginalized_energy = np.inf\n",
    "        \n",
    "        for source in range(len(nodelist)):\n",
    "            (mst_nodes, mst_adj_graph, mst_nodes_bool) = MST(nodelist, WScalarMat, conflicts_Dict, source) # T_X\n",
    "            # print('.', end = '')\n",
    "           \n",
    "            marginalized_dist = np.sum(WScalarMat[mst_adj_graph]) - margin_f(mst_nodes_bool)\n",
    "            if marginalized_dist < min_marginalized_energy:\n",
    "                min_marginalized_energy = marginalized_dist\n",
    "                min_STx = mst_adj_graph\n",
    "            # Energy diff should all be negative\n",
    "#             print('Source: [{}], Del:{}, Energy_margin: {:.3f}, Energy: {:.3f}, GE:{:.3f}'.\\\n",
    "#                   format(source, margin_f(mst_nodes_bool), marginalized_dist,  np.sum(WScalarMat[mst_adj_graph]), energy_gold_max_ST))\n",
    "\n",
    "        \"\"\" Gradient Descent \"\"\"\n",
    "        # FOR MOST OFFENdING Y\n",
    "        doBpp = False\n",
    "        \n",
    "        Total_Loss = energy_gold_max_ST - min_marginalized_energy\n",
    "#         print('Total Loss: ', Total_Loss)\n",
    "        if Total_Loss > 0:\n",
    "            doBpp = True\n",
    "            dLdOut = np.zeros_like(WScalarMat)\n",
    "            \n",
    "            if not self.neuralnet.outer_relu:\n",
    "                # For new loss function with sigmoid on neuralnet\n",
    "                raise Exception('Support for relu has been removed')\n",
    "            else:\n",
    "                # For new loss function with ReLU on neuralnet\n",
    "                dLdOut[max_st_adj_gold] = 1\n",
    "                dLdOut[min_STx] = -1\n",
    "\n",
    "        if doBpp:\n",
    "            if _debug:\n",
    "                print('{}. '.format(sentenceObj.sent_id), end = '')\n",
    "            self.neuralnet.Back_Prop(dLdOut, len(nodelist), featVMat, _debug)\n",
    "        else:\n",
    "            trainingStatus[sentenceObj.sent_id] = True # Means tranining done on this file\n",
    "            #print('Don\\'t do BPP')\n",
    "            pass\n",
    "        \n",
    "        Total_Loss /= len(nodelist)\n",
    "        self.history[sentenceObj.sent_id].append(Total_Loss)\n",
    "#         print(\"\\nFileKey: %s, Loss: %6.3f, Loss2: %6.3f\" % (sentenceObj.sent_id, Total_Loss, Loss_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trainer.neuralnet.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = None\n",
    "def InitModule(_matDB):\n",
    "    global WD, trainer\n",
    "    WD.word_definite_extInit(_matDB)\n",
    "    trainer = Trainer()\n",
    "InitModule(matDB)\n",
    "trainingStatus = defaultdict(lambda: bool(False))\n",
    "trainer.Load('outputs/train_nnet_t427031523027.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8, 9, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "# if __name__ == '__main__':\n",
    "# loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'))\n",
    "# loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'))\n",
    "# main(loaded_SKT, loaded_DCS)\n",
    "np.set_printoptions(suppress=False)\n",
    "for _ in range(10):\n",
    "    trainer.Train(loaded_SKT['170095.p2'], loaded_DCS['170095.p2'], _debug = False)\n",
    "trainer.Test(loaded_SKT['170095.p2'], loaded_DCS['170095.p2'])\n",
    "# print (\"Not Implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "['141141.p2', '206744.p2', '30352.p2', '43009.p2', '5774.p2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Batch:  1\n",
      "['205691.p2', '216192.p2', '352667.p2', '59768.p2', '35561.p2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Batch:  2\n",
      "['433461.p2', '54567.p2', '165647.p2', '32144.p2', '410602.p2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Batch:  3\n",
      "['289063.p2', '236819.p2', '347229.p2', '24734.p2', '147078.p2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Batch:  4\n",
      "['345767.p2', '43597.p2', '153835.p2', '169097.p2', '32122.p2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-08a939cc564c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Complete Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainingStatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Reset it after 3 epochs of full-training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_SKT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_DCS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilePerBatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterationPerBatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_debug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplayBeep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-6a3be6cd54c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loaded_SKT, loaded_DCS, n_trainset, iterationPerBatch, filePerBatch, _debug)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[1;31m# trainer.Save('outputs/saved_trainer.p')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdcsObj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_debug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\x1b[31mFailed: {} \\x1b[0m'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-1c45c6f57bc0>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, sentenceObj, dcsObj, _debug)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Support for Non-outer-relu removed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[1;33m(\u001b[0m\u001b[0mconflicts_Dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatVMat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWScalarMat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodelist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneuralnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[1;34m\"\"\" For each node - Find MST with that source\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8b8230b98478>\u001b[0m in \u001b[0;36mGetGraph\u001b[0;34m(nodelist, neuralnet)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mconflicts_Dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGet_Conflicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mfeatVMat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGet_Feat_Vec_Matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodelist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicts_Dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mWScalarMat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGet_W_Scalar_Matrix_from_FeatVect_Matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatVMat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodelist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicts_Dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneuralnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\Bishal Santra\\Documents\\Sanskrit_Project\\Bishal\\Clean_n_clear\\word_definite.py\u001b[0m in \u001b[0;36mGet_Feat_Vec_Matrix\u001b[0;34m(nodelist_new, conflicts_Dict)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mfeatVMat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGet_Features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodelist_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodelist_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatVMat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\Bishal Santra\\Documents\\Sanskrit_Project\\Bishal\\Clean_n_clear\\word_definite.py\u001b[0m in \u001b[0;36mGet_Features\u001b[0;34m(node1, node2)\u001b[0m\n\u001b[1;32m   3439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3440\u001b[0m     \u001b[0mfeats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeats\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-25\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-25\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3441\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complete Training\n",
    "trainingStatus = defaultdict(lambda: bool(False)) # Reset it after 3 epochs of full-training set\n",
    "train(loaded_SKT, loaded_DCS, n_trainset = 100, filePerBatch=5, iterationPerBatch=5, _debug=False)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test on training set\n",
    "test(loaded_SKT, loaded_DCS, n_testSet=100, _testFiles=TrainFiles, n_checkpt = 10)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Checkpoint... \n",
      "10  Checkpoint... \n",
      "20  Checkpoint... \n",
      "30  Checkpoint... \n"
     ]
    }
   ],
   "source": [
    "# POST TEST 1K SET\n",
    "_ = test(loaded_SKT, loaded_DCS, n_testSet = 100, _testFiles = TestFiles, n_checkpt = 10)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test on Larger Set\n",
    "_ = test(loaded_SKT, loaded_DCS, n_testSet = 3000, _testFiles = TestFiles_2, n_checkpt = 100)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Weights Saved: ', p_name)\n",
    "pickle.dump({\n",
    "        'U': trainer.neuralnet.U,\n",
    "        'W': trainer.neuralnet.W,\n",
    "        'n': trainer.neuralnet.n,\n",
    "        'd': trainer.neuralnet.d\n",
    "    }, open(p_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.neuralnet.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.neuralnet.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Edge Features Time:  6.570472478866577\n",
      "Get Edge Features Time:  1.6002552509307861\n",
      "Get Edge Features Time:  0.5128428936004639\n",
      "Get Edge Features Time:  0.16260337829589844\n",
      "Get Edge Features Time:  0.30678248405456543\n",
      "Get Edge Features Time:  1.3092880249023438\n",
      "Get Edge Features Time:  0.8916211128234863\n",
      "Get Edge Features Time:  1.8754069805145264\n",
      "Get Edge Features Time:  1.14072585105896\n",
      "Get Edge Features Time:  0.27155017852783203\n"
     ]
    }
   ],
   "source": [
    "for fn in TestFiles[:10]:\n",
    "    _ = trainer.Test(loaded_SKT[fn], loaded_DCS[fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD_Node[C: 0, P: 0, Adi @(31) => Adi]\n",
      "WD_Node[C: 1, P: 0, ca @(2) => ca]\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "WD.word_definite_extInit(matDB)\n",
    "# node2 = WD.word_definite('tIkzRam', 'tIkzRa', 31, 0, 0)\n",
    "node1 = WD.word_definite('Adi', 'Adi', 31, 0, 0)\n",
    "node2 = WD.word_definite('ca', 'ca', 2, 0, 1)\n",
    "# node1 = WD.word_definite('koRam', 'koRa', 31, 0, 1)\n",
    "print(node1)\n",
    "print(node2)\n",
    "\n",
    "feats = WD.Get_Features(node1, node2)\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7031.25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.nbytes*900/(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implement Pooled Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Train_n_Save_NNet\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(loaded_SKT, loaded_DCS, n_testSet = -1, _testFiles = None, n_checkpt = 100):\n",
    "    total_lemma = 0;\n",
    "    correct_lemma = 0;\n",
    "\n",
    "    total_word = 0;\n",
    "    total_output_nodes = 0\n",
    "    correct_word = 0;\n",
    "    file_counter = 0\n",
    "    if _testFiles is None:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = TestFiles\n",
    "        else:\n",
    "            _testFiles = TestFiles[0:n_testSet]\n",
    "    else:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = _testFiles\n",
    "        else:\n",
    "            _testFiles = _testFiles[0:n_testSet]\n",
    "            \n",
    "    recalls = []\n",
    "    recalls_of_word = []\n",
    "    precisions = []\n",
    "    precisions_of_words = []\n",
    "    for fn in _testFiles:\n",
    "        if file_counter % n_checkpt == 0:\n",
    "            print(file_counter,' Checkpoint... ')\n",
    "            sys.stdout.flush() # Flush IO buffer \n",
    "        file_counter += 1\n",
    "        sentenceObj = loaded_SKT[fn]\n",
    "        dcsObj = loaded_DCS[fn]        \n",
    "        try:\n",
    "            (word_match, lemma_match, n_dcsWords, n_output_nodes) = trainer.Test(sentenceObj, dcsObj)\n",
    "            \n",
    "            recalls.append(lemma_match/n_dcsWords)\n",
    "            recalls_of_word.append(word_match/n_dcsWords)\n",
    "            \n",
    "            precisions.append(lemma_match/n_output_nodes)\n",
    "            precisions_of_words.append(word_match/n_output_nodes)\n",
    "            \n",
    "            total_lemma += n_dcsWords\n",
    "            total_word += n_dcsWords\n",
    "            \n",
    "            total_output_nodes += n_output_nodes            \n",
    "            \n",
    "            correct_lemma += lemma_match\n",
    "            correct_word += word_match\n",
    "        except (IndexError, KeyError) as e:\n",
    "            print('Failed!')        \n",
    "\n",
    "    print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "    print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "    print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "    print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))\n",
    "    \n",
    "    return (recalls, recalls_of_word, precisions, precisions_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = test(loaded_SKT, loaded_DCS, n_testSet = 3000, _testFiles = TestFiles_2, n_checkpt = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######   THE END\n",
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multiprocessing Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import os\n",
    "import poolTest\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'), encoding=u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done:  9608\n",
      "Done:  5124\n",
      "Done:  13260\n",
      "Done:  7332\n"
     ]
    }
   ],
   "source": [
    "# USING PROCESS OBJECT\n",
    "glob_arr = list(range(100))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "proc_count = 4\n",
    "\n",
    "procs = [None]*proc_count\n",
    "\n",
    "for i in range(proc_count):\n",
    "    procs[i] = Process(target = poolTest.f, args = (i, ))\n",
    "for proc in procs:\n",
    "    proc.start()\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "    print('Done: ', proc.pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sktObjList = list(loaded_SKT.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('14741', 8),\n",
       " ('21171', 9),\n",
       " ('44699', 5),\n",
       " ('286767', 3),\n",
       " ('268297', 3),\n",
       " ('253892', 1),\n",
       " ('42110', 5),\n",
       " ('138854', 3),\n",
       " ('42273', 6),\n",
       " ('320783', 4),\n",
       " ('273356', 5),\n",
       " ('33423', 9),\n",
       " ('40411', 6),\n",
       " ('343323', 4),\n",
       " ('249124', 5),\n",
       " ('249672', 6),\n",
       " ('214118', 6),\n",
       " ('39926', 1),\n",
       " ('437631', 2),\n",
       " ('289985', 5)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USING POOL\n",
    "p = Pool(4)\n",
    "chunk_counts = p.map(poolTest.counter, sktObjList[:20])\n",
    "p.close()\n",
    "\n",
    "chunk_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
