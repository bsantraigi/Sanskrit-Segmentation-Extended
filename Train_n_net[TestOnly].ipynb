{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bUILT-iN pACKAGES\n",
    "import sys, os, time, bz2, zlib, pickle, math, json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "## lAST sUMMER\n",
    "from romtoslp import *\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "import MatDB\n",
    "\n",
    "\n",
    "## lAST yEAR\n",
    "# from word_definite import *\n",
    "from nnet import *\n",
    "# from heap_n_PrimMST import *\n",
    "# from word_definite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import winsound\n",
    "def playBeep():\n",
    "    for i in range(3):\n",
    "        winsound.Beep(2200, 300)\n",
    "        winsound.Beep(2600, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'), encoding=u'utf-8')\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'), encoding=u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset_4k_1k = pickle.load(open('../SmallDataset_4K_1K.p', 'rb'))\n",
    "# TrainFiles = dataset_4k_1k['TrainFiles']\n",
    "# TestFiles = dataset_4k_1k['TestFiles']\n",
    "\n",
    "# dataset_6k_3k = pickle.load(open('../SmallDataset_6K_3K.p', 'rb'))\n",
    "# TrainFiles_2 = dataset_6k_3k['TrainFiles']\n",
    "# TestFiles_2 = dataset_6k_3k['TestFiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# matDB = MatDB.MatDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2286\n",
      "2287\n"
     ]
    }
   ],
   "source": [
    "loader = pickle.load(open('../bz2Dataset_10K.p', 'rb'))\n",
    "TrainFiles = loader['TrainFiles']\n",
    "TestFiles = loader['TestFiles']\n",
    "print(len(TrainFiles))\n",
    "print(len(TestFiles))\n",
    "def open_dsbz2(filename):\n",
    "    with bz2.BZ2File(filename, 'r') as f:\n",
    "        loader = pickle.load(f)\n",
    "    \n",
    "    conflicts_Dict_correct = loader['conflicts_Dict_correct']\n",
    "    nodelist_to_correct_mapping = loader['nodelist_to_correct_mapping']\n",
    "    nodelist_correct = loader['nodelist_correct']\n",
    "    featVMat_correct = loader['featVMat_correct']\n",
    "    featVMat = loader['featVMat']\n",
    "    conflicts_Dict = loader['conflicts_Dict']\n",
    "    nodelist = loader['nodelist']\n",
    "    \n",
    "    return (nodelist_correct, conflicts_Dict_correct, featVMat_correct, nodelist_to_correct_mapping,\\\n",
    "            nodelist, conflicts_Dict, featVMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from MatDB import *\n",
    "import word_definite as WD\n",
    "from heap_n_PrimMST import *\n",
    "from nnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "######################  CREATE SEVERAL DATA STRUCTURES FROM SENTENCE/DCS  ######################\n",
    "###########################  NODELIST, ADJACENCY LIST, GRAPH, HEAP #############################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "def GetTrainingKit(sentenceObj, dcsObj):\n",
    "    nodelist = GetNodes(sentenceObj)\n",
    "    \n",
    "    # Nodelist with only the correct_nodes\n",
    "    nodelist2 = GetNodes(sentenceObj)\n",
    "    nodelist2_to_correct_mapping = {}\n",
    "    nodelist_correct = []\n",
    "    search_key = 0\n",
    "    first_key = 0\n",
    "    for chunk_id in range(len(dcsObj.lemmas)):\n",
    "        while nodelist2[first_key].chunk_id != chunk_id:\n",
    "            first_key += 1\n",
    "        for j in range(len(dcsObj.lemmas[chunk_id])):\n",
    "            search_key = first_key\n",
    "            while (nodelist2[search_key].lemma != rom_slp(dcsObj.lemmas[chunk_id][j])) or (nodelist2[search_key].cng != dcsObj.cng[chunk_id][j]):\n",
    "                search_key += 1\n",
    "                if search_key >= len(nodelist2) or nodelist2[search_key].chunk_id > chunk_id:\n",
    "                    break\n",
    "    #         print((rom_slp(dcsObj.lemmas[chunk_id][j]), dcsObj.cng[chunk_id][j]))\n",
    "    #         print(nodelist[search_key])\n",
    "            nodelist2_to_correct_mapping[len(nodelist_correct)] = search_key\n",
    "            nodelist_correct.append(nodelist2[search_key])\n",
    "    return (nodelist, nodelist_correct, nodelist2_to_correct_mapping)\n",
    "    \n",
    "\n",
    "def GetGraph(nodelist, neuralnet):\n",
    "    if not neuralnet.outer_relu:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat, SigmoidGateOutput)\n",
    "    else:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nEURAL nET wILL bE sAVED hERE:  outputs/train_nnet_t615546148216.p\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  GET A FILENAME TO SAVE WEIGHTS  ################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "import time\n",
    "st = str(int((time.time() * 1e6) % 1e13))\n",
    "log_name = 'logs/train_nnet_t{}.out'.format(st)\n",
    "p_name = 'outputs/train_nnet_t{}.p'.format(st)\n",
    "print('nEURAL nET wILL bE sAVED hERE: ', p_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingStatus = defaultdict(lambda: bool(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  TRAIN FUNCTION  ################################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "def train(loaded_SKT, loaded_DCS, n_trainset = -1, iterationPerBatch = 10, filePerBatch = 20, _debug = True):\n",
    "    # Train\n",
    "    if n_trainset == -1:\n",
    "        totalBatchToTrain = 20\n",
    "    else:\n",
    "        totalBatchToTrain = math.ceil(n_trainset/filePerBatch)\n",
    "    \n",
    "    for iterout in range(totalBatchToTrain):\n",
    "        # Change current batch\n",
    "        trainer.Save(p_name)\n",
    "        print('Batch: ', iterout)\n",
    "        files_for_batch = TrainFiles[iterout*filePerBatch:(iterout + 1)*filePerBatch]\n",
    "        print(files_for_batch)\n",
    "        # trainer.Load('outputs/neuralnet_trained.p')\n",
    "        \n",
    "        # Run few times on same set of files\n",
    "        for iterin in range(iterationPerBatch):\n",
    "            print('ITERATION IN', iterin)        \n",
    "            for fn in files_for_batch:\n",
    "                sentenceObj = loaded_SKT[fn]\n",
    "                dcsObj = loaded_DCS[fn]\n",
    "                if trainingStatus[sentenceObj.sent_id]:\n",
    "                    continue\n",
    "                # trainer.Save('outputs/saved_trainer.p')\n",
    "                try:\n",
    "                    trainer.Train(sentenceObj, dcsObj, _debug)\n",
    "                except (IndexError, KeyError) as e:\n",
    "                    print('\\x1b[31mFailed: {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "            sys.stdout.flush() # Flush IO buffer \n",
    "    trainer.Save(p_name)\n",
    "    \n",
    "    sys.stdout.flush() # Flush IO buffer \n",
    "                \n",
    "def test(loaded_SKT, loaded_DCS, n_testSet = -1, _testFiles = None, n_checkpt = 100):\n",
    "    total_lemma = 0;\n",
    "    correct_lemma = 0;\n",
    "\n",
    "    total_word = 0;\n",
    "    total_output_nodes = 0\n",
    "    correct_word = 0;\n",
    "    file_counter = 0\n",
    "    if _testFiles is None:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = TestFiles\n",
    "        else:\n",
    "            _testFiles = TestFiles[0:n_testSet]\n",
    "    else:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = _testFiles\n",
    "        else:\n",
    "            _testFiles = _testFiles[0:n_testSet]\n",
    "            \n",
    "    recalls = []\n",
    "    recalls_of_word = []\n",
    "    precisions = []\n",
    "    precisions_of_words = []\n",
    "    for fn in _testFiles:\n",
    "        if file_counter % n_checkpt == 0:\n",
    "            print(file_counter,' Checkpoint... ')\n",
    "            if file_counter > 0:\n",
    "                print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "                print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "                print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "                print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))\n",
    "            sys.stdout.flush() # Flush IO buffer \n",
    "        \n",
    "        file_counter += 1\n",
    "        \n",
    "        testFileName = fn.replace('.ds.bz2', '.p2')\n",
    "        sentenceObj = loaded_SKT[testFileName]\n",
    "        dcsObj = loaded_DCS[testFileName]    \n",
    "        \n",
    "        try:\n",
    "            (word_match, lemma_match, n_dcsWords, n_output_nodes) = trainer.Test(sentenceObj, dcsObj)\n",
    "            \n",
    "            recalls.append(lemma_match/n_dcsWords)\n",
    "            recalls_of_word.append(word_match/n_dcsWords)\n",
    "            \n",
    "            precisions.append(lemma_match/n_output_nodes)\n",
    "            precisions_of_words.append(word_match/n_output_nodes)\n",
    "            \n",
    "            total_lemma += n_dcsWords\n",
    "            total_word += n_dcsWords\n",
    "            \n",
    "            total_output_nodes += n_output_nodes            \n",
    "            \n",
    "            correct_lemma += lemma_match\n",
    "            correct_word += word_match\n",
    "        except (IndexError, KeyError) as e:\n",
    "            print('Failed!')        \n",
    "\n",
    "    print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "    print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "    print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "    print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))\n",
    "    \n",
    "    return (recalls, recalls_of_word, precisions, precisions_of_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOW-OLD FUNCTION\n",
    "# def GetLoss(_mst_adj_graph, _mask_de_correct_edges, _negLogLikelies):\n",
    "#     _negLogLikelies = _negLogLikelies.copy()\n",
    "#     _negLogLikelies[~_mst_adj_graph] = 0\n",
    "#     _negLogLikelies[~_mask_de_correct_edges] *= -1 # BAKA!!! Check before you try to fix this again\n",
    "#     return np.sum(_negLogLikelies)    \n",
    "\n",
    "# NEW FUNCTION\n",
    "def GetLoss(_mst_adj_graph, _mask_de_correct_edges, _WScalarMat):\n",
    "    _WScalarMat = _WScalarMat.copy()\n",
    "    _WScalarMat[_mst_adj_graph&(~_mask_de_correct_edges)] *= -1 # BAKA!!! Check before you try to fix this again\n",
    "    _WScalarMat[~_mst_adj_graph] = 0\n",
    "    return np.sum(_WScalarMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.hidden_layer_size = 300\n",
    "        self._edge_vector_dim = 1000\n",
    "        # self._edge_vector_dim = WD._edge_vector_dim\n",
    "        # self._full_cnglist = list(WD.mat_cngCount_1D)\n",
    "        \n",
    "        self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True)\n",
    "        self.history = defaultdict(lambda: list())\n",
    "        \n",
    "    def Reset(self):\n",
    "        self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size)\n",
    "        self.history = defaultdict(lambda: list())\n",
    "        \n",
    "    def Save(self, filename):\n",
    "        #pickle.dump({'nnet': self.neuralnet, 'history': dict(self.history)}, open(filename, 'wb'))\n",
    "        return\n",
    "        \n",
    "    \n",
    "    def Load(self, filename):\n",
    "        loader = pickle.load(open(filename, 'rb'))\n",
    "        self.neuralnet.U = loader['U']\n",
    "        self.neuralnet.W = loader['W']\n",
    "        self.neuralnet.B1 = loader['B1']\n",
    "        self.neuralnet.B2 = loader['B2']\n",
    "        self.neuralnet.n = loader['n']\n",
    "        self.neuralnet.d = loader['d']\n",
    "        \n",
    "    def Test(self, sentenceObj, dcsObj):\n",
    "        neuralnet = self.neuralnet\n",
    "        minScore = np.inf\n",
    "        minMst = None\n",
    "        \n",
    "        dsbz2_name = sentenceObj.sent_id + '.ds.bz2'\n",
    "        (nodelist_correct, conflicts_Dict_correct, featVMat_correct, nodelist_to_correct_mapping,\\\n",
    "            nodelist, conflicts_Dict, featVMat) = open_dsbz2('../NewData/skt_dcs_DS.bz2_10K/' + dsbz2_name)\n",
    "        \n",
    "        # startT = time.time()\n",
    "#         try:\n",
    "#             (nodelist, nodelist_correct, _) = GetTrainingKit(sentenceObj, dcsObj)\n",
    "#             # nodelist = GetNodes(sentenceObj)\n",
    "#         except IndexError:\n",
    "#             print('\\x1b[31mError with {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "#             return (0, 0, 0)\n",
    "            \n",
    "#         conflicts_Dict = Get_Conflicts(nodelist)\n",
    "#         conflicts_Dict_correct = Get_Conflicts(nodelist_correct)\n",
    "        \n",
    "#         # print('Load Nodelist + GetConflicts Time: ', time.time() - startT)\n",
    "#         startT = time.time()\n",
    "        \n",
    "#         featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "#         featVMat_correct = Get_Feat_Vec_Matrix(nodelist_correct, conflicts_Dict_correct)\n",
    "        \n",
    "        # print('Get Edge Features Time: ', time.time() - startT)\n",
    "        # startT = time.time()\n",
    "        \n",
    "        if not self.neuralnet.outer_relu:\n",
    "            (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        else:\n",
    "            WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        \n",
    "        # print('NeuralNet Time: ', time.time() - startT)\n",
    "        # startT = time.time()\n",
    "        \n",
    "        # Get all MST\n",
    "        for source in range(len(nodelist)):\n",
    "            (mst_nodes, mst_adj_graph, _) = MST(nodelist, WScalarMat, conflicts_Dict, source)\n",
    "            # print('.', end = '')\n",
    "            score = GetMSTWeight(mst_nodes, WScalarMat)\n",
    "            if(score < minScore):\n",
    "                minScore = score\n",
    "                minMst = mst_nodes\n",
    "        dcsLemmas = [[rom_slp(l) for l in arr]for arr in dcsObj.lemmas]\n",
    "        word_match = 0\n",
    "        lemma_match = 0\n",
    "        n_output_nodes = 0\n",
    "        for chunk_id, wdSplit in minMst.items():\n",
    "            for wd in wdSplit:\n",
    "                n_output_nodes += 1\n",
    "                # Match lemma\n",
    "                search_result = [i for i, j in enumerate(dcsLemmas[chunk_id]) if j == wd.lemma]\n",
    "                if len(search_result) > 0:\n",
    "                    lemma_match += 1\n",
    "                # Match CNG\n",
    "                for i in search_result:\n",
    "                    if(dcsObj.cng[chunk_id][i] == str(wd.cng)):\n",
    "                        word_match += 1\n",
    "                        # print(wd.lemma, wd.cng)\n",
    "                        break\n",
    "        dcsLemmas = [l for arr in dcsObj.lemmas for l in arr]\n",
    "        \n",
    "        # print('All MST Time: ', time.time() - startT)\n",
    "        # print('Node Count: ', len(nodelist))\n",
    "#         print('\\nFull Match: {}, Partial Match: {}, OutOf {}, NodeCount: {}, '.\\\n",
    "#               format(word_match, lemma_match, len(dcsLemmas), len(nodelist)))\n",
    "        return (word_match, lemma_match, len(dcsLemmas), n_output_nodes)\n",
    "    \n",
    "    def Train(self, sentenceObj, dcsObj, _debug = True):\n",
    "        # Hyperparameter for hinge loss: m\n",
    "        M_hinge = 14\n",
    "        \n",
    "        \"\"\" Pre-Process DCS and SKT to get all Nodes etc. \"\"\"\n",
    "        try:\n",
    "            (nodelist, nodelist_correct, nodelist_to_correct_mapping) = GetTrainingKit(sentenceObj, dcsObj)\n",
    "        except IndexError as e:\n",
    "            # print('\\x1b[31mError with {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "            # print(e)\n",
    "            return\n",
    "        \n",
    "        \"\"\" FORM MAXIMUM(ENERGY) SPANNING TREE OF THE GOLDEN GRAPH : WORST GOLD STRUCTURE \"\"\"\n",
    "        if not self.neuralnet.outer_relu:\n",
    "            raise Exception('Support for Non-outer-relu removed')\n",
    "        else:\n",
    "            (conflicts_Dict_correct, featVMat_correct, WScalarMat_correct) = GetGraph(nodelist_correct, self.neuralnet)\n",
    "        source = 0\n",
    "        \n",
    "        \"\"\" Find the max spanning tree : negative Weight matrix passed \"\"\"\n",
    "        (max_st_gold_ndict, max_st_adj_gold_small, _) = MST(nodelist_correct, -WScalarMat_correct, conflicts_Dict_correct, source)\n",
    "        energy_gold_max_ST = np.sum(WScalarMat_correct[max_st_adj_gold_small])\n",
    "        \n",
    "        \"\"\" Convert correct spanning tree graph adj matrix to full marix dimensions \"\"\"\n",
    "        \"\"\" Create full-size adjacency matrix for correct_mst_small \"\"\"\n",
    "        nodelen = len(nodelist)\n",
    "        max_st_adj_gold = np.ndarray((nodelen, nodelen), np.bool)*False # T_STAR\n",
    "        for i in range(max_st_adj_gold_small.shape[0]):\n",
    "            for j in range(max_st_adj_gold_small.shape[1]):\n",
    "                max_st_adj_gold[nodelist_to_correct_mapping[i], nodelist_to_correct_mapping[j]] = max_st_adj_gold_small[i, j]\n",
    "        \n",
    "        \"\"\" Delta(Margin) Function : MASK FOR WHICH NODES IN NODELIST BELONG TO DCS \"\"\"\n",
    "        gold_nodes_mask = np.array([False]*len(nodelist))\n",
    "        for i in range(len(nodelist_correct)):\n",
    "            gold_nodes_mask[nodelist_to_correct_mapping[nodelist_correct[i].id]] = True\n",
    "        margin_f = lambda nodes_mask: np.sum(nodes_mask&gold_nodes_mask)**1.7\n",
    "        \n",
    "        \"\"\" FOR ALL POSSIBLE MST FROM THE COMPLETE GRAPH \"\"\"\n",
    "        if not self.neuralnet.outer_relu:\n",
    "            raise Exception('Support for Non-outer-relu removed')\n",
    "        else:\n",
    "            (conflicts_Dict, featVMat, WScalarMat) = GetGraph(nodelist, self.neuralnet)\n",
    "\n",
    "        \"\"\" For each node - Find MST with that source\"\"\"\n",
    "        min_STx = None # Min Energy spanning tree with worst margin with gold_STx\n",
    "        min_marginalized_energy = np.inf\n",
    "        \n",
    "        for source in range(len(nodelist)):\n",
    "            (mst_nodes, mst_adj_graph, mst_nodes_bool) = MST(nodelist, WScalarMat, conflicts_Dict, source) # T_X\n",
    "            # print('.', end = '')\n",
    "           \n",
    "            marginalized_dist = np.sum(WScalarMat[mst_adj_graph]) - margin_f(mst_nodes_bool)\n",
    "            if marginalized_dist < min_marginalized_energy:\n",
    "                min_marginalized_energy = marginalized_dist\n",
    "                min_STx = mst_adj_graph\n",
    "            # Energy diff should all be negative\n",
    "#             print('Source: [{}], Del:{}, Energy_margin: {:.3f}, Energy: {:.3f}, GE:{:.3f}'.\\\n",
    "#                   format(source, margin_f(mst_nodes_bool), marginalized_dist,  np.sum(WScalarMat[mst_adj_graph]), energy_gold_max_ST))\n",
    "\n",
    "        \"\"\" Gradient Descent \"\"\"\n",
    "        # FOR MOST OFFENdING Y\n",
    "        doBpp = False\n",
    "        \n",
    "        Total_Loss = energy_gold_max_ST - min_marginalized_energy\n",
    "#         print('Total Loss: ', Total_Loss)\n",
    "        if Total_Loss > 0:\n",
    "            doBpp = True\n",
    "            dLdOut = np.zeros_like(WScalarMat)\n",
    "            \n",
    "            if not self.neuralnet.outer_relu:\n",
    "                # For new loss function with sigmoid on neuralnet\n",
    "                raise Exception('Support for relu has been removed')\n",
    "            else:\n",
    "                # For new loss function with ReLU on neuralnet\n",
    "                dLdOut[max_st_adj_gold] = 1\n",
    "                dLdOut[min_STx] = -1\n",
    "\n",
    "        if doBpp:\n",
    "            if _debug:\n",
    "                print('{}. '.format(sentenceObj.sent_id), end = '')\n",
    "            self.neuralnet.Back_Prop(dLdOut, len(nodelist), featVMat, _debug)\n",
    "        else:\n",
    "            trainingStatus[sentenceObj.sent_id] = True # Means tranining done on this file\n",
    "            #print('Don\\'t do BPP')\n",
    "            pass\n",
    "        \n",
    "        Total_Loss /= len(nodelist)\n",
    "        self.history[sentenceObj.sent_id].append(Total_Loss)\n",
    "#         print(\"\\nFileKey: %s, Loss: %6.3f, Loss2: %6.3f\" % (sentenceObj.sent_id, Total_Loss, Loss_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trainer.neuralnet.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = None\n",
    "def InitModule(_matDB):\n",
    "    global trainer\n",
    "#     global WD, trainer\n",
    "#     WD.word_definite_extInit(_matDB)\n",
    "    trainer = Trainer()\n",
    "# InitModule(matDB)\n",
    "InitModule(None)\n",
    "trainingStatus = defaultdict(lambda: bool(False))\n",
    "trainer.Load('outputs/train_nnet_t615405139045.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Checkpoint... \n",
      "100  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.795034632034632\n",
      "Avg. Micro Recall of Words: 0.6499693362193362\n",
      "Avg. Micro Precision of Lemmas: 0.7221556221556221\n",
      "Avg. Micro Precision of Words: 0.5950846930846931\n",
      "200  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.7623722943722945\n",
      "Avg. Micro Recall of Words: 0.6288672438672439\n",
      "Avg. Micro Precision of Lemmas: 0.6783061036186037\n",
      "Avg. Micro Precision of Words: 0.563080704018204\n",
      "300  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.7574946429040631\n",
      "Avg. Micro Recall of Words: 0.6275578940651404\n",
      "Avg. Micro Precision of Lemmas: 0.671889356014356\n",
      "Avg. Micro Precision of Words: 0.5591926453176452\n",
      "400  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.7527695211390864\n",
      "Avg. Micro Recall of Words: 0.6257832293511641\n",
      "Avg. Micro Precision of Lemmas: 0.6686679882617383\n",
      "Avg. Micro Precision of Words: 0.5582507665945166\n",
      "500  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.7578154690199447\n",
      "Avg. Micro Recall of Words: 0.6253570242819603\n",
      "Avg. Micro Precision of Lemmas: 0.6697601168439403\n",
      "Avg. Micro Precision of Words: 0.5553482923938807\n",
      "600  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.7593919423540054\n",
      "Avg. Micro Recall of Words: 0.6269925566800034\n",
      "Avg. Micro Precision of Lemmas: 0.6721450419134243\n",
      "Avg. Micro Precision of Words: 0.5579876059398118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-89d09542fac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# POST TEST 1K SET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_SKT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_DCS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_testSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_testFiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTestFiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_checkpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplayBeep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9e49aaa63e06>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loaded_SKT, loaded_DCS, n_testSet, _testFiles, n_checkpt)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[1;33m(\u001b[0m\u001b[0mword_match\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemma_match\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_dcsWords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_output_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceObj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdcsObj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mrecalls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma_match\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn_dcsWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-8f028772df42>\u001b[0m in \u001b[0;36mTest\u001b[0;34m(self, sentenceObj, dcsObj)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdsbz2_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentenceObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.ds.bz2'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[1;33m(\u001b[0m\u001b[0mnodelist_correct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicts_Dict_correct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatVMat_correct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodelist_to_correct_mapping\u001b[0m\u001b[1;33m,\u001b[0m            \u001b[0mnodelist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicts_Dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatVMat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_dsbz2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../NewData/skt_dcs_DS.bz2_10K/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdsbz2_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[1;31m# startT = time.time()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6bbe341b5848>\u001b[0m in \u001b[0;36mopen_dsbz2\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTestFiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_dsbz2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\program files\\anaconda3\\lib\\bz2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, buffering, compresslevel)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# POST TEST 1K SET\n",
    "_ = test(loaded_SKT, loaded_DCS, n_testSet = -1, _testFiles = TestFiles, n_checkpt = 100)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Checkpoint... \n",
      "100  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.8342474110856464\n",
      "Avg. Micro Recall of Words: 0.6902850988880401\n",
      "Avg. Micro Precision of Lemmas: 0.7935565741448094\n",
      "Avg. Micro Precision of Words: 0.6568280281809694\n",
      "200  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.8162580850522027\n",
      "Avg. Micro Recall of Words: 0.6711782637297344\n",
      "Avg. Micro Precision of Lemmas: 0.7702312230253406\n",
      "Avg. Micro Precision of Words: 0.6333768151782858\n",
      "300  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.8242222010016128\n",
      "Avg. Micro Recall of Words: 0.6679902951079423\n",
      "Avg. Micro Precision of Lemmas: 0.7753377706171825\n",
      "Avg. Micro Precision of Words: 0.6278708589885061\n",
      "400  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.821674587259146\n",
      "Avg. Micro Recall of Words: 0.6636445070452425\n",
      "Avg. Micro Precision of Lemmas: 0.7661912789008377\n",
      "Avg. Micro Precision of Words: 0.6185762142269495\n",
      "500  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.8173821661998132\n",
      "Avg. Micro Recall of Words: 0.6617141626347509\n",
      "Avg. Micro Precision of Lemmas: 0.7618576074252544\n",
      "Avg. Micro Precision of Words: 0.6163631025836908\n",
      "600  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.813829715219421\n",
      "Avg. Micro Recall of Words: 0.6614886540474776\n",
      "Avg. Micro Precision of Lemmas: 0.7577150336047396\n",
      "Avg. Micro Precision of Words: 0.6159272247360483\n",
      "700  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.8145787513140453\n",
      "Avg. Micro Recall of Words: 0.662011887505585\n",
      "Avg. Micro Precision of Lemmas: 0.7576052359020023\n",
      "Avg. Micro Precision of Words: 0.6152591030331257\n",
      "800  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.814902162412321\n",
      "Avg. Micro Recall of Words: 0.6606719512749892\n",
      "Avg. Micro Precision of Lemmas: 0.7582445435354643\n",
      "Avg. Micro Precision of Words: 0.6146624114021811\n",
      "900  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.814061983071013\n",
      "Avg. Micro Recall of Words: 0.6578952698201923\n",
      "Avg. Micro Precision of Lemmas: 0.7579832851307702\n",
      "Avg. Micro Precision of Words: 0.6125098458896413\n",
      "1000  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.8140204673035941\n",
      "Avg. Micro Recall of Words: 0.661042647600078\n",
      "Avg. Micro Precision of Lemmas: 0.7593061687389053\n",
      "Avg. Micro Precision of Words: 0.6169661484579643\n",
      "1100  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.8144648705206776\n",
      "Avg. Micro Recall of Words: 0.6586956185284604\n",
      "Avg. Micro Precision of Lemmas: 0.7606327560488803\n",
      "Avg. Micro Precision of Words: 0.6157404773466736\n",
      "Avg. Micro Recall of Lemmas: 0.814061418937103\n",
      "Avg. Micro Recall of Words: 0.6576527020617837\n",
      "Avg. Micro Precision of Lemmas: 0.7599419886709003\n",
      "Avg. Micro Precision of Words: 0.6146414702843545\n"
     ]
    }
   ],
   "source": [
    "# POST TEST 1K SET\n",
    "_ = test(loaded_SKT, loaded_DCS, n_testSet = -1, _testFiles = TestFiles, n_checkpt = 100)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 7, 6)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "# if __name__ == '__main__':\n",
    "# loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'))\n",
    "# loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'))\n",
    "# main(loaded_SKT, loaded_DCS)\n",
    "np.set_printoptions(suppress=False)\n",
    "# for _ in range(10):\n",
    "#     trainer.Train(loaded_SKT['170095.p2'], loaded_DCS['170095.p2'], _debug = False)\n",
    "testFileName = TestFiles[1].replace('.ds.bz2', '.p2')\n",
    "trainer.Test(loaded_SKT[testFileName], loaded_DCS[testFileName])\n",
    "# print (\"Not Implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test on training set\n",
    "test(loaded_SKT, loaded_DCS, n_testSet=100, _testFiles=TrainFiles, n_checkpt = 10)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test on Larger Set\n",
    "_ = test(loaded_SKT, loaded_DCS, n_testSet = 3000, _testFiles = TestFiles_2, n_checkpt = 100)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00132348, -0.0012933 , -0.00129663,  0.00704216, -0.00363552,\n",
       "        -0.0003557 , -0.00155312, -0.00353331,  0.00443802,  0.0058609 ,\n",
       "         0.0054556 ,  0.00615163, -0.00899183,  0.00402657,  0.00291749,\n",
       "        -0.00627881, -0.00069057,  0.00182278, -0.00744417,  0.01043658,\n",
       "         0.00144622, -0.00120951,  0.00911377, -0.00445913, -0.00470522,\n",
       "        -0.00745741,  0.00209969, -0.00221724,  0.00957119,  0.0033588 ,\n",
       "        -0.0077902 ,  0.00202125, -0.00377856, -0.00337754,  0.00458549,\n",
       "         0.00051476, -0.00748544,  0.00683321, -0.00365689, -0.00354552,\n",
       "        -0.00547771,  0.00667848, -0.00522963,  0.00454416,  0.00239613,\n",
       "        -0.00281474, -0.00025603,  0.01023962,  0.0080348 , -0.00888692,\n",
       "         0.00554297, -0.0023863 , -0.00118438, -0.00247854,  0.0031558 ,\n",
       "        -0.0006583 ,  0.00741233, -0.00312393,  0.00939598, -0.00517568,\n",
       "        -0.00265694,  0.00080384,  0.00387427, -0.00313638, -0.00840351,\n",
       "         0.00345248, -0.00342628,  0.00026122, -0.00469237,  0.00221208,\n",
       "        -0.00044185, -0.00399797,  0.00651054,  0.00105232, -0.00010128,\n",
       "        -0.00169445, -0.00351887,  0.00497202,  0.00291256,  0.00918755,\n",
       "        -0.00329016, -0.0066922 ,  0.00809125,  0.0039394 ,  0.00494576,\n",
       "         0.0102924 , -0.00743056, -0.00558015, -0.00299934,  0.00547127,\n",
       "         0.00616017,  0.00492741,  0.00243913,  0.00040229,  0.0045353 ,\n",
       "         0.00588532, -0.00276526, -0.00645426,  0.00741063, -0.00111914,\n",
       "         0.00840535, -0.00638612, -0.0089105 ,  0.00517075, -0.00262591,\n",
       "         0.00448262,  0.00611918, -0.00481155, -0.00318247, -0.00283947,\n",
       "         0.00311699,  0.00254478,  0.00153683,  0.00557885, -0.00703259,\n",
       "         0.00812154, -0.00706448,  0.01056163,  0.00614696, -0.00902272,\n",
       "        -0.00432923,  0.00197741, -0.0067011 ,  0.00310542,  0.00313152,\n",
       "         0.0033283 ,  0.00495651,  0.00357455,  0.0016668 ,  0.00564975,\n",
       "         0.00429302, -0.00887486,  0.00098933,  0.00592826,  0.01037006,\n",
       "        -0.00468681, -0.00309224, -0.00030967,  0.00842239,  0.01056239,\n",
       "        -0.00316753,  0.00547294, -0.00749664,  0.00905418,  0.00035823,\n",
       "         0.00235339,  0.00887994,  0.00504784,  0.00568745,  0.00861416,\n",
       "        -0.00422255, -0.00367646,  0.00033552, -0.00855993,  0.00137433,\n",
       "        -0.00699022, -0.00044778,  0.00780642, -0.00268695,  0.00968622,\n",
       "         0.00039359,  0.00382721, -0.00433833,  0.00310688,  0.00807666,\n",
       "        -0.00067196, -0.00864212,  0.00862518,  0.01023494, -0.00010497,\n",
       "        -0.00863099,  0.00418579, -0.00672112, -0.00310717,  0.0098113 ,\n",
       "        -0.00221305, -0.00863717,  0.00072858,  0.00387069,  0.00890397,\n",
       "        -0.00633896, -0.00389832,  0.00625035,  0.00237332, -0.00211744,\n",
       "         0.00766883,  0.00336291, -0.00603038,  0.0104934 ,  0.00750277,\n",
       "        -0.0001369 , -0.0071964 ,  0.00816715,  0.00624037, -0.00651936,\n",
       "         0.01070197,  0.00358561,  0.01028344, -0.00193093, -0.00139346,\n",
       "        -0.00891496,  0.00583342,  0.00560592, -0.00608633, -0.00267405,\n",
       "         0.00578191, -0.0067878 ,  0.00339442, -0.00481972,  0.00820225,\n",
       "         0.00086261,  0.00449363,  0.00751361, -0.00906564,  0.0101629 ,\n",
       "        -0.00086233,  0.00091018, -0.00302715,  0.00108602,  0.00287726,\n",
       "         0.00007009,  0.00590182,  0.00011893, -0.00546603, -0.00558224,\n",
       "        -0.00201725,  0.00613124,  0.00897432, -0.00551336,  0.00291525,\n",
       "        -0.00908337, -0.00229808, -0.00413489,  0.00455171, -0.00623821,\n",
       "         0.00022129, -0.00472935, -0.00353796, -0.00485519, -0.0042748 ,\n",
       "        -0.00534301,  0.00861313,  0.00601048,  0.00788932,  0.00136127,\n",
       "         0.00682651,  0.0095909 , -0.00304757,  0.00625742,  0.00305045,\n",
       "        -0.00026578, -0.00725728,  0.00211483, -0.00602504,  0.00095718,\n",
       "         0.00152679, -0.00038244, -0.00259653,  0.00037742, -0.00023435,\n",
       "        -0.00530157,  0.0049094 ,  0.00827352, -0.00074337, -0.00447646,\n",
       "        -0.0048842 , -0.00011467,  0.00106145,  0.00112294,  0.00687567,\n",
       "        -0.00697907,  0.00735909, -0.00521183, -0.00008917, -0.00253238,\n",
       "        -0.0074199 ,  0.00291317,  0.00139567, -0.00807741,  0.00019206,\n",
       "        -0.00084141,  0.00523352,  0.00520398,  0.00418116,  0.01060281,\n",
       "         0.00953181, -0.00894534,  0.00681447, -0.00175471,  0.00663059,\n",
       "         0.00617793,  0.01070598, -0.00264758, -0.00593946,  0.00812425,\n",
       "        -0.00306819,  0.00811147, -0.00654072,  0.00487177,  0.00675834]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.neuralnet.B1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06452455, -0.02857236,  0.01472452, -0.00755044, -0.09776624,\n",
       "        -0.06166409,  0.02289677,  0.02390144,  0.06995325,  0.04512473,\n",
       "         0.01836133,  0.06802282, -0.04300666,  0.02439816,  0.01180296,\n",
       "        -0.00902472,  0.03318761,  0.09060296,  0.03839495,  0.0494596 ,\n",
       "        -0.07754387,  0.07959374, -0.05095214,  0.04285856, -0.02589167,\n",
       "        -0.03495725,  0.01524659,  0.01116058, -0.0846886 ,  0.01480398,\n",
       "         0.09798082, -0.06181292,  0.05676597,  0.02613602, -0.09279112,\n",
       "         0.08595271, -0.03718871,  0.06505159, -0.08233761,  0.04990364,\n",
       "        -0.09886274,  0.01561371, -0.07005505, -0.02691714,  0.00569123,\n",
       "         0.0037455 , -0.08979445, -0.04951974, -0.08207702, -0.01927568,\n",
       "        -0.04446576,  0.00030422, -0.03167278, -0.07880898, -0.02903044,\n",
       "         0.0309716 , -0.03966193,  0.08606616,  0.055974  , -0.03070683,\n",
       "         0.07197797, -0.09282673, -0.07244675, -0.09485229, -0.0035742 ,\n",
       "        -0.0126654 ,  0.04187079,  0.00895554,  0.06001716, -0.00720929,\n",
       "         0.04548384, -0.04563422,  0.04761924, -0.0720406 ,  0.06260136,\n",
       "         0.02136418,  0.07677576,  0.07337026, -0.04295061,  0.09062853,\n",
       "         0.02120646, -0.08890458, -0.09789053,  0.04520516,  0.0074074 ,\n",
       "        -0.03183657, -0.04223492,  0.03054933,  0.07558353,  0.07646712,\n",
       "         0.00671347,  0.04123395, -0.05390346,  0.09544892, -0.06667404,\n",
       "        -0.06495075,  0.01679286,  0.02711033, -0.05226952, -0.00157038,\n",
       "         0.01546204,  0.01941922,  0.05121193,  0.02182049,  0.01908784,\n",
       "         0.08973795, -0.09917212,  0.09469642,  0.00618473, -0.01600217,\n",
       "         0.09532645,  0.05665595, -0.06670936, -0.04140075, -0.05693406,\n",
       "         0.09351854,  0.09340388, -0.04249677, -0.06450162, -0.04928683,\n",
       "         0.00844415,  0.0670987 , -0.06161836, -0.06054164,  0.06786447,\n",
       "         0.00192813,  0.09132223,  0.09682236,  0.0882021 , -0.03044179,\n",
       "        -0.07223131, -0.03643066,  0.07851332,  0.0488575 , -0.0731038 ,\n",
       "         0.03422264,  0.09468268, -0.0110046 , -0.06055489, -0.05543553,\n",
       "         0.02756871,  0.08473613, -0.04757554, -0.04848301,  0.02897827,\n",
       "         0.06718712, -0.01755025, -0.09265912, -0.09383464, -0.07667766,\n",
       "        -0.08275271,  0.04359574, -0.00534947, -0.07704159,  0.07221395,\n",
       "        -0.04178067,  0.08060007, -0.00111211, -0.02561264,  0.07156347,\n",
       "         0.05335433, -0.08053256, -0.03174069, -0.02830407, -0.08844666,\n",
       "        -0.01383114, -0.05071738,  0.00389185, -0.00766119, -0.00899485,\n",
       "        -0.0451945 , -0.08104882,  0.03611349, -0.077317  , -0.01977329,\n",
       "         0.01201993,  0.08136726,  0.07427931,  0.08453442, -0.02168851,\n",
       "        -0.05921358,  0.00873364, -0.04779951, -0.07573905, -0.09800781,\n",
       "        -0.06149416,  0.0340777 ,  0.0274529 ,  0.004121  ,  0.02904386,\n",
       "         0.05337181,  0.05559102, -0.00926907,  0.00100338, -0.00718232,\n",
       "        -0.05226771,  0.04714711, -0.01579996,  0.01267315, -0.0654262 ,\n",
       "        -0.053744  , -0.06046303, -0.01036908, -0.07216994, -0.01070744,\n",
       "        -0.07677485,  0.03202503, -0.00698479, -0.01831093,  0.01967966,\n",
       "        -0.03516179, -0.0534964 ,  0.00672449, -0.09551817, -0.03007254,\n",
       "        -0.05014936,  0.05336878,  0.07970785, -0.08743254, -0.0296016 ,\n",
       "        -0.05775407,  0.0747709 ,  0.0406461 , -0.04983229, -0.05410806,\n",
       "         0.05408205,  0.05188788, -0.0915738 , -0.02065729,  0.02932453,\n",
       "         0.04574367, -0.09928537, -0.01271688,  0.03527399,  0.03029883,\n",
       "         0.08110945, -0.06891261,  0.08878633,  0.02852643,  0.01296325,\n",
       "        -0.07388612,  0.02797892,  0.03178643, -0.0761878 , -0.00171765,\n",
       "        -0.04239387,  0.06005957,  0.08594308, -0.02277575, -0.01050437,\n",
       "         0.0421166 ,  0.00187787,  0.01240324, -0.02026744, -0.00104561,\n",
       "         0.02159984,  0.04043112, -0.05134712,  0.01425972,  0.05200989,\n",
       "        -0.02443368, -0.01718386, -0.07221139, -0.0458908 , -0.07502972,\n",
       "        -0.01365299,  0.01986831, -0.08616047, -0.0975233 , -0.03246827,\n",
       "         0.06632028, -0.00443561, -0.03304723,  0.05680546,  0.04235517,\n",
       "         0.07491712, -0.08465536,  0.01318715, -0.05815596, -0.09994786,\n",
       "         0.00158425, -0.09764888,  0.04011282, -0.03928319,  0.03563855,\n",
       "        -0.04293161,  0.0569386 , -0.08918492,  0.07830303, -0.08488082,\n",
       "        -0.09687796, -0.0073583 ,  0.0543706 ,  0.03305112, -0.08472185,\n",
       "        -0.04930307, -0.09632661, -0.00100791, -0.00042956, -0.0514429 ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.neuralnet.U.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######   THE END\n",
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MP based asynchronous testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import poolTest\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Evaluate(result_arr):\n",
    "    print('Files Processed: ', len(result_arr))\n",
    "    recalls = []\n",
    "    recalls_of_word = []\n",
    "    precisions = []\n",
    "    precisions_of_words = []\n",
    "    for entry in result_arr:\n",
    "        (word_match, lemma_match, n_dcsWords, n_output_nodes) = entry\n",
    "        recalls.append(lemma_match/n_dcsWords)\n",
    "        recalls_of_word.append(word_match/n_dcsWords)\n",
    "\n",
    "        precisions.append(lemma_match/n_output_nodes)\n",
    "        precisions_of_words.append(word_match/n_output_nodes)\n",
    "    print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "    print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "    print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "    print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelFile = 'outputs/train_nnet_t699570015755.p'\n",
    "# Backup the model file\n",
    "copyfile(modelFile, modelFile + '.bk')\n",
    "\n",
    "# Create Queue, Result array\n",
    "queue = mp.Queue()\n",
    "result_arr = []\n",
    "\n",
    "# Start 6 workers - 8 slows down the pc\n",
    "proc_count = 6\n",
    "procs = [None]*proc_count\n",
    "for i in range(proc_count):\n",
    "    vpid = i\n",
    "    procs[i] = mp.Process(target = poolTest.pooled_Test, args = (modelFile, vpid, queue, 700))\n",
    "# Start Processes\n",
    "for i in range(proc_count):\n",
    "    procs[i].start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files Processed:  3109\n",
      "Avg. Micro Recall of Lemmas: 0.8118148775236205\n",
      "Avg. Micro Recall of Words: 0.6677115932552932\n",
      "Avg. Micro Precision of Lemmas: 0.752277642040589\n",
      "Avg. Micro Precision of Words: 0.6196714721302771\n"
     ]
    }
   ],
   "source": [
    "# Fetch partial results\n",
    "while not queue.empty():\n",
    "    result_arr.append(queue.get())\n",
    "# Evaluate results till now\n",
    "Evaluate(result_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process with\t vpid: 0\t ->\t pid: 7620\t ->\t running status: False\n",
      "Process with\t vpid: 1\t ->\t pid: 13092\t ->\t running status: False\n",
      "Process with\t vpid: 2\t ->\t pid: 6652\t ->\t running status: False\n",
      "Process with\t vpid: 3\t ->\t pid: 13224\t ->\t running status: False\n",
      "Process with\t vpid: 4\t ->\t pid: 5996\t ->\t running status: False\n",
      "Process with\t vpid: 5\t ->\t pid: 12072\t ->\t running status: False\n"
     ]
    }
   ],
   "source": [
    "# Check status\n",
    "for i in range(proc_count):\n",
    "    p = procs[i]\n",
    "    print('Process with\\t vpid: {}\\t ->\\t pid: {}\\t ->\\t running status: {}'.format(i, p.pid, p.is_alive()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Properly Join\n",
    "for i in range(proc_count):\n",
    "    procs[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Force Terminate\n",
    "for p in procs:\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.8208722741433023"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([a[2] for a in result_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean([a[0] for a in result_arr])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
