{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sentences, DCS\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "D = json.load(open('../NewData/data7.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "print(len(D.keys()))\n",
    "d_keys = list(D.keys())\n",
    "print(d_keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-200\n",
      "-230\n",
      "-220\n",
      "-240\n",
      "-190\n",
      "-210\n"
     ]
    }
   ],
   "source": [
    "for k in d_keys:\n",
    "    ki = abs(int(k))\n",
    "    if(ki >= 190 and ki <= 240):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _getPathValue(terminal_1, path_type, terminal_2):\n",
    "    p = 0;\n",
    "    return p\n",
    "        \n",
    "def GetPathVector(terminal_1, terminal_2):\n",
    "    x = np.array([0]*len(all_path_types))\n",
    "    for i in range(len(all_path_types)):\n",
    "        path = all_path_types[i]\n",
    "        x[i] = _getPathValue(terminal_1, path, terminal_2)        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting CNG GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "import json\n",
    "import numpy as np\n",
    "from romtoslp import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100078"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT.p', 'rb'))\n",
    "len(loaded_SKT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cng_Groups(sentenceObj, dDict):\n",
    "    #print('SKT ANALYZE')\n",
    "    #print('-'*15)\n",
    "    #print(sentenceObj.sentence)\n",
    "    zz = 0\n",
    "    # (chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain) = SentencePreprocess(sentenceObj)\n",
    "    # for cid in chunkDict.keys():\n",
    "    #     print('Analyzing:', rom_slp(sentenceObj.chunk[cid].chunk_name))\n",
    "    #     for pos in chunkDict[cid].keys():\n",
    "    #         tupIds = chunkDict[cid][pos]\n",
    "    #         for ti in tupIds:\n",
    "    #             print('%d :' % (pos, ), end = ' ')\n",
    "    #             print(tuplesMain[ti][0][1], end=' ')\n",
    "    #             for tup in tuplesMain[ti]:\n",
    "    #                 print([zz, tup[2], tup[3]], end=' ')\n",
    "    #                 zz += 1\n",
    "    #             print('')\n",
    "    #     print('-'*25)\n",
    "\n",
    "    for chunk in sentenceObj.chunk:\n",
    "        # print(\"Analyzing \", rom_slp(chunk.chunk_name))\n",
    "        for pos in chunk.chunk_words.keys():\n",
    "            for word_sense in chunk.chunk_words[pos]:\n",
    "                #word_sense = fix_w_new(word_sense)\n",
    "                #print(word_sense.forms)\n",
    "                for form_dict in word_sense.forms:\n",
    "                    if isinstance(form_dict, dict):                        \n",
    "                        for key, val_arr in form_dict.items():\n",
    "                            #print('Key: ', key)\n",
    "                            #print('Val Array: ', val_arr)\n",
    "                            for val in val_arr:\n",
    "                                if isinstance(val, list):\n",
    "                                    if len(val) > 1:\n",
    "                                        print('Key: ', key)\n",
    "                                        print('Bad Val: ', val, ' Size: ', len(val))\n",
    "                                    dDict[key].add(val[0])\n",
    "                                else:\n",
    "                                    dDict[key].add(val)\n",
    "                # for formsDict in word_sense.forms:\n",
    "                #     print(getCNGs(formsDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DD = defaultdict(lambda:set())\n",
    "i = 0\n",
    "for key, val in loaded_SKT.items():\n",
    "    i += 1\n",
    "    cng_Groups(val, DD)\n",
    "    if i==1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(dict(DD), open('cng_grps_dd.p', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Verb CNG Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from IPython.display import display\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lem2CNG and CNG2Lem Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_lem2cng = pickle.load(open('../NewData/mat_lem2cng.p', 'rb'), encoding='utf-8')\n",
    "mat_cng2lem = pickle.load(open('../NewData/mat_cng2lem.p', 'rb'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, -35, 116, 175, -123, 28, 36, -83, -52, 99]\n",
      "276\n"
     ]
    }
   ],
   "source": [
    "all_cngs = list(int(vc) for vc in list(mat_cng2lem.keys()))\n",
    "print(all_cngs[0:10])\n",
    "print(len(all_cngs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {'1': {0},\n",
       "             '10': {1, 2, 3, 4, 6, 7, 8, 9},\n",
       "             '11': {1, 2, 3, 4, 9},\n",
       "             '12': {1, 2, 3, 4, 5, 7, 9},\n",
       "             '13': {1, 2, 3, 6, 9},\n",
       "             '14': {1, 2, 3, 7, 9},\n",
       "             '15': {0, 1, 2, 3, 4, 7, 9},\n",
       "             '16': {1, 2, 3, 4, 6, 7, 8, 9},\n",
       "             '17': {1, 3, 6, 9},\n",
       "             '19': {0},\n",
       "             '2': {0, 1, 2, 3, 4, 5, 6, 7, 8, 9},\n",
       "             '20': {0},\n",
       "             '21': {0},\n",
       "             '22': {0},\n",
       "             '23': {0},\n",
       "             '24': {0},\n",
       "             '25': {1, 2, 3, 5, 6, 7, 9},\n",
       "             '26': {0},\n",
       "             '27': {1, 2, 3, 6, 8, 9},\n",
       "             '28': {1, 3, 6, 9},\n",
       "             '29': {3},\n",
       "             '3': {0, 1, 2, 3, 4, 5, 6, 7, 8, 9},\n",
       "             '30': {1, 2, 3, 6, 7, 9},\n",
       "             '31': {1, 2, 3, 6, 7, 8, 9},\n",
       "             '4': {1, 2, 3, 4, 5, 6, 7, 8, 9},\n",
       "             '5': {0, 1, 2, 3, 4, 5, 6, 7, 8, 9},\n",
       "             '6': {1, 2, 3, 4, 5, 6, 7, 8, 9},\n",
       "             '7': {1, 2, 3, 4, 6, 7, 8, 9},\n",
       "             '8': {1, 2, 3, 6, 7, 8, 9},\n",
       "             '9': {0, 1, 2, 3, 4, 6, 7, 9}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tense_classes = defaultdict(lambda:set())\n",
    "for vc in all_cngs:\n",
    "    if vc < 0:\n",
    "        tense_classes[str(-math.floor(vc/10))].add((-vc)%10)\n",
    "tense_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'du': [4, 5, 6],\n",
       " 'fp': [1, 4, 7],\n",
       " 'pl': [7, 8, 9],\n",
       " 'sg': [1, 2, 3],\n",
       " 'sp': [2, 5, 8],\n",
       " 'tp': [3, 6, 9]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'du': [4, 5, 6],\n",
       " 'du_fp': {4},\n",
       " 'du_sp': {5},\n",
       " 'du_tp': {6},\n",
       " 'fp': [1, 4, 7],\n",
       " 'pl': [7, 8, 9],\n",
       " 'pl_fp': {7},\n",
       " 'pl_sp': {8},\n",
       " 'pl_tp': {9},\n",
       " 'sg': [1, 2, 3],\n",
       " 'sg_fp': {1},\n",
       " 'sg_sp': {2},\n",
       " 'sg_tp': {3},\n",
       " 'sp': [2, 5, 8],\n",
       " 'tp': [3, 6, 9]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# num_person: encoding style\n",
    "verb_groups = defaultdict(lambda: set())\n",
    "number_forms = dict()\n",
    "number_forms['sg'] = [1, 2, 3]\n",
    "number_forms['du'] = [4, 5, 6]\n",
    "number_forms['pl'] = [7, 8, 9]\n",
    "\n",
    "person_forms = dict()\n",
    "person_forms['fp'] = [1, 4, 7]\n",
    "person_forms['sp'] = [2, 5, 8]\n",
    "person_forms['tp'] = [3, 6, 9]\n",
    "\n",
    "comb_forms = dict()\n",
    "comb_forms2 = dict()\n",
    "for key_num, num_vals in number_forms.items():\n",
    "    for key_person, person_vals in person_forms.items():\n",
    "        s = set()\n",
    "        for n in num_vals:\n",
    "            for p in person_vals:\n",
    "                if n==p:\n",
    "                    s.add(n)\n",
    "        comb_forms2['%s_%s'%(key_num, key_person)] = s\n",
    "comb_forms.update(number_forms)\n",
    "comb_forms.update(person_forms)\n",
    "comb_forms2.update(number_forms)\n",
    "comb_forms2.update(person_forms)\n",
    "display(comb_forms)\n",
    "display(comb_forms2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verb_groups = dict()\n",
    "for tense in tense_classes.keys():\n",
    "    for form, mods in comb_forms.items():\n",
    "        new_key = '%s_%s'%(tense, form)\n",
    "        s = set()\n",
    "        for modulo in mods:\n",
    "            if modulo in tense_classes[tense]:\n",
    "                s.add(int(tense)*10 + modulo)\n",
    "        if len(s) > 0:\n",
    "            verb_groups[new_key] = list(s)\n",
    "\n",
    "for form, mods in comb_forms2.items():\n",
    "    new_key = form\n",
    "    s = set()\n",
    "    for modulo in mods:\n",
    "        for tense in tense_classes.keys():\n",
    "            if modulo in tense_classes[tense]:\n",
    "                s.add(int(tense)*10 + modulo)\n",
    "    if len(s) > 0:\n",
    "        verb_groups[new_key] = list(s)\n",
    "json.dump(verb_groups, open('json_n_pickle_Files/verbgroups.json', 'w'))\n",
    "# verb_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verb_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Noun CNG Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "from IPython.display import display\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noungroups = json.load(open('json_n_pickle_Files/noungroups.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noungroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# # PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from IPython.display import display\n",
    "import json\n",
    "from ProbModels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader Started[Prob]...\n",
      "Dataloader Finished[Prob]...\n"
     ]
    }
   ],
   "source": [
    "# IMPORT THE DATALOADER FILES FIRST\n",
    "from utilities import *\n",
    "from collections import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"Dataloader Started[Prob]...\")\n",
    "\n",
    "fullCo_oc_mat = pickleFixLoad('extras/all_dcs_lemmas_matrix_countonly.p')\n",
    "unigram_counts = pickleFixLoad('../NewData/ultimate_new_lastsem/unigram_lemma_countonly.p')\n",
    "\n",
    "cng2cngFullMat = np.mat(pickle.load(open('extras/all_dcs_cngs_matrix_countonly.p','rb'), encoding = 'latin1'))\n",
    "cng2index_dict = pickleFixLoad('cng2index_dict.p')\n",
    "\n",
    "w2w_samecng_fullmat = pickle.load(open('extras/lemmas_with_same_cngs_matrix_countonly.p', 'rb'), encoding=u'utf-8')\n",
    "samecng_unigram_counts = pickle.load(open('extras/dictionary_for_lemmas_with_same_cng.p', 'rb'), encoding=u'utf-8')\n",
    "\n",
    "v2c_fullMat = pickle.load(open('extras/verbs_vs_cngs_matrix_countonly.p', 'rb'), encoding=u'utf-8')\n",
    "\n",
    "print(\"Dataloader Finished[Prob]...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbModel = ProbModels(fullCo_oc_mat = fullCo_oc_mat, unigram_counts = unigram_counts,\n",
    "               cng2cngFullMat = cng2cngFullMat, cng2index_dict = cng2index_dict,\n",
    "               w2w_samecng_fullmat=w2w_samecng_fullmat, samecng_unigram_counts = samecng_unigram_counts,\n",
    "               v2c_fullMat = v2c_fullMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66914\n",
      "avarRaka advAra pratisrotas\n",
      "2.2026413966350114e-06\n"
     ]
    }
   ],
   "source": [
    "lemma_list = list(unigram_counts.keys())\n",
    "print(len(lemma_list))\n",
    "print(lemma_list[0], lemma_list[10], lemma_list[100])\n",
    "print(pbModel.kn_word2word(lemma_list[0], lemma_list[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_co_oc_count = pbModel.total_co_oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pmi(x;y) = -log2[p(x|y)/p(x)]\n",
    "def PMI(x, y, _pbModel):\n",
    "    if(x in _pbModel.fullCo_oc_mat[y]):\n",
    "        p_x_given_y = _pbModel.fullCo_oc_mat[y][x]/_pbModel.unigram_counts[y]\n",
    "        p_x = _pbModel.unigram_counts[x]/_pbModel.unigram_total_count\n",
    "        #print(p_x_given_y, ' ', p_x)\n",
    "        return -math.log2(p_x_given_y/p_x)\n",
    "    else:\n",
    "        p_x_given_y = 1/_pbModel.unigram_counts[y]\n",
    "        p_x = _pbModel.unigram_counts[x]/_pbModel.unigram_total_count\n",
    "        #print(p_x_given_y, ' ', p_x)\n",
    "        return -math.log2(p_x_given_y/p_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.307367596880127\n",
      "-19.307367596880127\n",
      "-20.629295691767492\n",
      "-20.629295691767492\n"
     ]
    }
   ],
   "source": [
    "print(PMI('vEdiSa', 'SatruGAtin', pbModel))\n",
    "print(PMI('SatruGAtin', 'vEdiSa', pbModel))\n",
    "\n",
    "print(PMI('anupavyAyacCamAna', 'SatruGAtin', pbModel))\n",
    "print(PMI('SatruGAtin', 'anupavyAyacCamAna', pbModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_PMI_Mat = defaultdict(lambda: {})\n",
    "count = 0\n",
    "for y, _ in fullCo_oc_mat.items():\n",
    "    count += 1\n",
    "    for x, _ in fullCo_oc_mat[y].items():\n",
    "        # print(y, x, fullCo_oc_mat[y][x])\n",
    "        # Getting PMI(x | y)\n",
    "        try:\n",
    "            v = PMI(x, y, pbModel)\n",
    "        except:\n",
    "            v = None\n",
    "        if v != None:\n",
    "            full_PMI_Mat[y][x] = v\n",
    "    if(count % 10000 ==0):\n",
    "        print(\"Checkpoint: \", count)\n",
    "full_PMI_Mat = dict(full_PMI_Mat)\n",
    "pickle.dump(full_PMI_Mat, open('../NewData/ultimate_new_lastsem/full_PMI_Mat.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y, x)\n",
    "print(pbModel.fullCo_oc_mat[y][x]/pbModel.unigram_counts[y])\n",
    "print(pbModel.unigram_counts[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# # MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from IPython.display import display\n",
    "import json\n",
    "from word_definite import *\n",
    "from romtoslp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'))\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in list(loaded_SKT.keys())[100:]:\n",
    "    loaded_SKT.pop(k, None)\n",
    "    loaded_DCS.pop(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_SKT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### # Graph and Other Data Structures\n",
    "\n",
    "- load skt - Done\n",
    "- load dcs\n",
    "\n",
    "\n",
    "- create the full graph - sktwseg_utf8\n",
    "- Nodelist - Done\n",
    "- Features-Adjacency Matrix\n",
    "- Weight-Adjacency Matrix\n",
    "- Conflict Matrix - or - dictionary (since it's sparse)\n",
    "\n",
    "\n",
    "- new paths base on cng probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(vec):\n",
    "    evec = 1 + np.exp(-vec)\n",
    "    return 1/evec\n",
    "def relu(vec_x):\n",
    "    relu_x = vec_x.copy()\n",
    "    relu_x[vec_x < 0] = 0\n",
    "    return relu_x\n",
    "def d_relu(vec_x):\n",
    "    d_relu_x = vec_x.copy()\n",
    "    d_relu_x[vec_x > 0] = 1\n",
    "    d_relu_x[vec_x <= 0] = 0\n",
    "    return d_relu_x\n",
    "class NN:\n",
    "    def __init__(self, input_dimension, hidden_layer_size):\n",
    "        # d: Input feature dimension i.e. the dimension of the edge feature vectors\n",
    "        # n: Hidden layer size\n",
    "        \n",
    "        # TODO: Add Bias terms\n",
    "        \n",
    "        self.n = hidden_layer_size\n",
    "        self.d = input_dimension\n",
    "        rand_init_range = np.sqrt(6.0/(self.n+self.d))\n",
    "        self.W = np.random.uniform(-rand_init_range, rand_init_range, (self.n, self.d))\n",
    "        rand_init_range = np.sqrt(6.0/(self.n))\n",
    "        self.U = np.random.uniform(-rand_init_range, rand_init_range, (self.n, 1))\n",
    "\n",
    "    def Forward_Prop(self, x):\n",
    "        z2 = np.matmul(self.W, x)\n",
    "        a2 = relu(z2)\n",
    "        s = np.matmul(self.U.transpose(), a2)\n",
    "        return (z2, a2, s)\n",
    "    \n",
    "    def Back_Prop(self, dLdS, nodeLen, featVMat):\n",
    "        N = nodeLen\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        eta = 0.1\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if dLdS[i, j] != 0:\n",
    "                    (z2, a2, s) = Forward_Prop(featVMat[i][j])\n",
    "                    \n",
    "                    dLdU += dLdS[i, j]*a2/N*N\n",
    "                    \n",
    "                    dRelu = d_relu(a2)\n",
    "                    for k in range(self.n):\n",
    "                        dLdW[k, :] += (dLdS[i, j]/N*N)*U[k]*dRelu[k]*featVMat[i][j]\n",
    "        self.W -= eta*dLdW\n",
    "        self.U -= eta*dLdU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### # Form Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_lem2cng_countonly = pickle.load(open('../NewData/ultimate_new_lastsem/mat_lem2cng_countonly.p', 'rb'))\n",
    "mat_cng2lem_countonly = pickle.load(open('../NewData/ultimate_new_lastsem/mat_cng2lem_countonly.p', 'rb'))\n",
    "mat_lem2cng_1D = pickle.load(open('../NewData/ultimate_new_lastsem/mat_lem2cng_1D.p', 'rb'))\n",
    "mat_cng2lem_1D = pickle.load(open('../NewData/ultimate_new_lastsem/mat_cng2lem_1D.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 nodes received\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WD_Node[C: 0, P: 0, tad @(54) => tayoH],\n",
       " WD_Node[C: 0, P: 0, tad @(55) => tayoH],\n",
       " WD_Node[C: 0, P: 0, tad @(56) => tayoH],\n",
       " WD_Node[C: 0, P: 0, tad @(154) => tayoH],\n",
       " WD_Node[C: 0, P: 0, tad @(155) => tayoH],\n",
       " WD_Node[C: 0, P: 0, tad @(156) => tayoH],\n",
       " WD_Node[C: 1, P: 0, Bnna @(3) => nirBinna],\n",
       " WD_Node[C: 1, P: 0, Bd @(-190) => nirBinna],\n",
       " WD_Node[C: 1, P: 0, nirBid @(-190) => nirBid],\n",
       " WD_Node[C: 1, P: 8, hfdaya @(29) => hfdayaH],\n",
       " WD_Node[C: 1, P: 8, hfdaya @(3) => hft],\n",
       " WD_Node[C: 1, P: 8, hfd @(29) => hft],\n",
       " WD_Node[C: 1, P: 8, hfd @(30) => hft],\n",
       " WD_Node[C: 1, P: 11, ayas @(31) => ayaH],\n",
       " WD_Node[C: 1, P: 11, ayas @(71) => ayaH],\n",
       " WD_Node[C: 1, P: 11, aya @(29) => ayaH],\n",
       " WD_Node[C: 1, P: 11, ayas @(51) => ayaH],\n",
       " WD_Node[C: 2, P: 0, tarjana @(101) => tarjanEH],\n",
       " WD_Node[C: 2, P: 0, tarj @(29) => tarjan],\n",
       " WD_Node[C: 2, P: 0, tarj @(-10) => tarjan],\n",
       " WD_Node[C: 2, P: 0, tarj @(-32) => tarja],\n",
       " WD_Node[C: 2, P: 5, na @(2) => na],\n",
       " WD_Node[C: 2, P: 6,  @(-42) => EH],\n",
       " WD_Node[C: 2, P: 6,  @(-42) => EH],\n",
       " WD_Node[C: 3, P: 0, jAta @(3) => jAta],\n",
       " WD_Node[C: 3, P: 0, jan @(-190) => jAta],\n",
       " WD_Node[C: 3, P: 4, vepaTu @(29) => vepaTuH],\n",
       " WD_Node[C: 3, P: 4, vepaTu @(30) => vepaTuH]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn = 76\n",
    "sentenceObj = loaded_SKT[list(loaded_SKT.keys())[fn]]\n",
    "dcsObj = loaded_DCS[list(loaded_SKT.keys())[fn]]\n",
    "(chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain, qc_pairs) = SentencePreprocess(sentenceObj)\n",
    "\n",
    "nodelist = GetNodes(sentenceObj)\n",
    "print(\"%d nodes received\" % len(nodelist))\n",
    "display(nodelist)\n",
    "\n",
    "# Nodelist with only the correct_nodes\n",
    "nodelist2 = GetNodes(sentenceObj)\n",
    "nodelist_correct = []\n",
    "search_key = 0\n",
    "first_key = 0\n",
    "for chunk_id in range(len(dcsObj.lemmas)):\n",
    "    while nodelist2[first_key].chunk_id != chunk_id:\n",
    "        first_key += 1\n",
    "    for j in range(len(dcsObj.lemmas[chunk_id])):\n",
    "        search_key = first_key\n",
    "        \n",
    "        while (nodelist2[search_key].lemma != rom_slp(dcsObj.lemmas[chunk_id][j])) or (nodelist2[search_key].cng != int(dcsObj.cng[chunk_id][j])):\n",
    "            search_key += 1\n",
    "#         print((rom_slp(dcsObj.lemmas[chunk_id][j]), dcsObj.cng[chunk_id][j]))\n",
    "#         print(nodelist[search_key])\n",
    "        nodelist_correct.append(nodelist2[search_key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n"
     ]
    }
   ],
   "source": [
    "_edge_vector_dim = len(mat_cng2lem_1D)\n",
    "_full_cnglist = list(mat_cng2lem_1D)\n",
    "print(_edge_vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neuralnet = NN(_edge_vector_dim, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Get_Feat_Vec_Matrix(nodelist_new, conflicts_Dict):\n",
    "    nodesCount = len(nodelist_new)\n",
    "    featVMat = [[None for _ in range(nodesCount)] for _ in range(nodesCount)]\n",
    "    for i in range(nodesCount):\n",
    "        for j in range(nodesCount):\n",
    "            if j in conflicts_Dict[i] or i == j:                \n",
    "                # featVMat[i][j][:] = 1e-35\n",
    "                pass\n",
    "            else:\n",
    "                wd1 = nodelist_new[i]\n",
    "                wd2 = nodelist_new[j]\n",
    "                featVMat[i][j] = np.zeros((_edge_vector_dim, 1))\n",
    "                # print('For ', wd1, wd2)\n",
    "                for k in range(_edge_vector_dim):\n",
    "                    cng_k = _full_cnglist[k]\n",
    "                    # TODO: Some lemma's still missing\n",
    "                    try:\n",
    "                        pleft = float(mat_lem2cng_countonly[wd1.lemma][cng_k]) / mat_lem2cng_1D[wd1.lemma]\n",
    "                    except KeyError:\n",
    "                        pleft = 0\n",
    "                    try:\n",
    "                        pright = float(mat_cng2lem_countonly[cng_k][wd2.lemma]) / mat_cng2lem_1D[cng_k]\n",
    "                    except KeyError:\n",
    "                        pright = 0\n",
    "                        \n",
    "                    featVMat[i][j][k] = pleft * pright\n",
    "#                         if cng_k not in mat_lem2cng_countonly[wd1.lemma].keys() or wd2.lemma not in mat_cng2lem_countonly[cng_k].keys():\n",
    "#                             featVMat[i][j][k] = 0.7\n",
    "#                         else:                        \n",
    "#                             pleft = double(mat_lem2cng_countonly[wd1.lemma][cng_k]) / mat_lem2cng_1D[wd1.lemma]\n",
    "#                             pright = double(mat_cng2lem_countonly[cng_k][wd2.lemma]) / mat_cng2lem_1D[cng_k]\n",
    "#                             featVMat[i][j][k] = pleft*pright\n",
    "#                             print(\"Here\")\n",
    "#                             return\n",
    "                    \n",
    "                    #featVMat[i][j][k] = 0.5\n",
    "                \n",
    "    return featVMat\n",
    "\n",
    "# This function will be some neural network or a linear function or something of sorts\n",
    "def Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist_new, conflicts_Dict):\n",
    "    nodesCount = len(nodelist_new)\n",
    "    WScalarMat = 1e-25 * np.ones((nodesCount, nodesCount))   \n",
    "    for i in range(nodesCount):\n",
    "        for j in range(nodesCount):\n",
    "#             if j in conflicts_Dict[i]:\n",
    "            if featVMat[i][j] is None:\n",
    "                pass\n",
    "            else:\n",
    "                (_, _, s) = neuralnet.Forward_Prop(featVMat[i][j])\n",
    "                WScalarMat[i, j]= np.maximum(1e-25, s/_edge_vector_dim)\n",
    "    WScalarMat = -np.log2(WScalarMat)\n",
    "    return WScalarMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conflicts_Dict = Get_Conflicts(nodelist)\n",
    "conflicts_Dict_correct = Get_Conflicts(nodelist_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "featVMat_correct = Get_Feat_Vec_Matrix(nodelist_correct, conflicts_Dict_correct)\n",
    "#display(featVMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict)\n",
    "WScalarMat_correct = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat_correct, nodelist_correct, conflicts_Dict_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 83.04820237  83.04820237  83.04820237  83.04820237  83.04820237\n",
      "  83.04820237  83.04820237  83.04820237  27.98667909  24.5176578\n",
      "  24.5176578   25.27979805  25.27979805  26.01418175  26.01418175\n",
      "  33.79911828  26.01418175  30.97828461  32.11720179  32.11720179\n",
      "  32.11720179  19.78862491  83.04820237  83.04820237  27.55670283\n",
      "  22.62476649  28.236728    28.236728  ]\n",
      "[ 83.04820237  27.98667909  24.5176578   30.97828461  22.62476649\n",
      "  28.236728  ]\n"
     ]
    }
   ],
   "source": [
    "print(WScalarMat[0,:])\n",
    "print(WScalarMat_correct[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from word_definite import *\n",
    "import math\n",
    "from heap_sp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MST(nodelist, WScalarMat, conflicts_Dict, source):\n",
    "    mst_adj_graph = np.ndarray(WScalarMat.shape, np.bool)*False\n",
    "    # Reset nodes and put ids\n",
    "    for id in range(len(nodelist)):\n",
    "        nodelist[id].id = id\n",
    "        nodelist[id].dist = np.inf\n",
    "        nodelist[id].isConflicted = False\n",
    "        nodelist[id].src = -1\n",
    "        \n",
    "    # Initialize Graph and min-Heap\n",
    "    nodelist[source].dist = 0\n",
    "    for neighbour in range(len(nodelist)):\n",
    "        if neighbour != source:\n",
    "            nodelist[neighbour].dist = WScalarMat[source][neighbour]\n",
    "            nodelist[neighbour].src = source\n",
    "    h = Heap(nodelist)\n",
    "    \n",
    "    mst_nodes = defaultdict(lambda: [])\n",
    "    # Run MST only until first conflicting node is seen\n",
    "    # Conflicting node will have np.inf as dist\n",
    "    while True:\n",
    "        nextNode = h.Pop()\n",
    "        if nextNode == None:\n",
    "            break\n",
    "        print(nextNode.src, nextNode.id, nextNode)\n",
    "        mst_nodes[nextNode.chunk_id].append(nextNode)\n",
    "        if nextNode.src != -1:\n",
    "            mst_adj_graph[nextNode.src, nextNode.id] = True\n",
    "        nid = nextNode.id\n",
    "        for conId in conflicts_Dict[nid]:\n",
    "            h.Delete(nodelist[conId])\n",
    "        for neighbour in range(len(nodelist)):\n",
    "            if neighbour != nextNode.id:\n",
    "                h.Decrease_Key(nodelist[neighbour], WScalarMat[nextNode.id][neighbour], nextNode.id)\n",
    "    mst_nodes = dict(mst_nodes)\n",
    "    return (mst_nodes, mst_adj_graph)\n",
    "\n",
    "def organize_mst_nodes(mst_nodes):\n",
    "    sol_lemmas = []\n",
    "    sol_cngs = []\n",
    "    print()\n",
    "    total_tree_weight = 0\n",
    "    for key, val in mst_nodes.items():\n",
    "        print('{', end = '')\n",
    "        for node in val:\n",
    "            print(node.lemma, (node.cng, node.derived, node.src,), end = ', ')\n",
    "            if node.src != -1:\n",
    "                total_tree_weight += WScalarMat[node.src][node.id]\n",
    "        print('};', end = ' ')        \n",
    "    print()\n",
    "    print('Weight of the spanning tree: ', total_tree_weight)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 0 WD_Node[C: 0, P: 0, tad @(154) => tayoH]\n",
      "0 4 WD_Node[C: 3, P: 0, jan @(-190) => jAta]\n",
      "4 2 WD_Node[C: 1, P: 8, hfdaya @(29) => hfdayaH]\n",
      "4 1 WD_Node[C: 1, P: 0, nirBid @(-190) => nirBid]\n",
      "1 5 WD_Node[C: 3, P: 4, vepaTu @(29) => vepaTuH]\n",
      "4 3 WD_Node[C: 2, P: 0, tarjana @(101) => tarjanEH]\n",
      "\n",
      "{tad (154, 'tayoH', -1), }; {hfdaya (29, 'hfdayaH', 3), nirBid (-190, 'nirBid', 3), }; {tarjana (101, 'tarjanEH', 4), }; {jan (-190, 'jAta', 3), vepaTu (29, 'vepaTuH', 3), }; \n",
      "Weight of the spanning tree:  415.241011861\n",
      "\n",
      "SOURCE: 0\n",
      "-1 0 WD_Node[C: 0, P: 0, tad @(54) => tayoH]\n",
      "0 21 WD_Node[C: 2, P: 5, na @(2) => na]\n",
      "0 25 WD_Node[C: 3, P: 0, jan @(-190) => jAta]\n",
      "25 9 WD_Node[C: 1, P: 8, hfdaya @(29) => hfdayaH]\n",
      "25 8 WD_Node[C: 1, P: 0, nirBid @(-190) => nirBid]\n",
      "8 26 WD_Node[C: 3, P: 4, vepaTu @(29) => vepaTuH]\n",
      "9 20 WD_Node[C: 2, P: 0, tarj @(-32) => tarja]\n",
      "20 22 WD_Node[C: 2, P: 6,  @(-42) => EH]\n",
      "\n",
      "{tad (54, 'tayoH', -1), }; {hfdaya (29, 'hfdayaH', 20), nirBid (-190, 'nirBid', 26), }; {na (2, 'na', 20), tarj (-32, 'tarja', 9),  (-42, 'EH', 20), }; {jan (-190, 'jAta', 8), vepaTu (29, 'vepaTuH', 8), }; \n",
      "Weight of the spanning tree:  236.826412838\n"
     ]
    }
   ],
   "source": [
    "# Get MST for the correct nodelist\n",
    "source = 0\n",
    "(mst_nodes_correct, mst_adj_graph_correct) = MST(nodelist_correct, WScalarMat_correct, conflicts_Dict_correct, source)\n",
    "organize_mst_nodes(mst_nodes_correct)\n",
    "\n",
    "# Get all MST\n",
    "for source in range(len(nodelist)):\n",
    "    print('\\nSOURCE: {}'.format(source))\n",
    "    (mst_nodes, mst_adj_graph) = MST(nodelist, WScalarMat, conflicts_Dict, source)    \n",
    "    organize_mst_nodes(mst_nodes)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N(0, 0.0) \n",
      "\n",
      "N(1, inf) N(2, inf) \n",
      "\n",
      "N(3, inf) N(4, inf) N(5, inf) "
     ]
    }
   ],
   "source": [
    "hh = Heap(nodelist_correct)\n",
    "hh.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WD_Node[C: 0, P: 0, tad @(154) => tayoH],\n",
       " WD_Node[C: 1, P: 0, nirBid @(-190) => nirBid],\n",
       " WD_Node[C: 1, P: 8, hfdaya @(29) => hfdayaH],\n",
       " WD_Node[C: 2, P: 0, tarjana @(101) => tarjanEH],\n",
       " WD_Node[C: 3, P: 0, jan @(-190) => jAta],\n",
       " WD_Node[C: 3, P: 4, vepaTu @(29) => vepaTuH]]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodelist_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCS ANALYZE\n",
      "---------------\n",
      "tayor nirBinnahfdayas tarjanEr jAtavepaTuH    \n",
      "[['tad'], ['nirbhid', 'hṛdaya'], ['tarjana'], ['jan', 'vepathu']]\n",
      "Lemmas: ['tad', 'nirBid', 'hfdaya', 'tarjana', 'jan', 'vepaTu']\n",
      "[['154'], ['-190', '29'], ['101'], ['-190', '29']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SeeDCS(dcsObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37171\n",
      "SKT ANALYZE\n",
      "---------------\n",
      "tayor nirBinnahfdayas tarjanEr jAtavepaTuH    \n",
      "Analyzing  tayoH\n",
      "0 :  tayoH ['tad'] [{'noun': ['loc. du. n.', 'g. du. n.', 'loc. du. m.', 'g. du. m.', 'loc. du. f.', 'g. du. f.']}]\n",
      "Analyzing  nirBinnahfdayas\n",
      "0 :  nirBinna ['Bnna', 'Bd'] [{'compound': ['iic.']}, {'verb': ['pp.']}]\n",
      "0 :  nirBid ['nirBid'] [-190]\n",
      "8 :  hfdayaH ['hfdaya'] [{'noun': ['nom. sg. m.']}]\n",
      "8 :  hft ['hfdaya'] [{'compound': ['iic.']}]\n",
      "8 :  hft ['hfd'] [{'noun': ['nom. sg. f.', 'nom. sg. m.']}]\n",
      "11 :  ayaH ['ayas'] [{'noun': ['acc. sg. n.', 'nom. sg. n.']}]\n",
      "11 :  ayaH ['aya'] [{'noun': ['nom. sg. m.']}]\n",
      "11 :  ayaH ['ayas'] [{'noun': ['voc. sg. n.']}]\n",
      "Analyzing  tarjanEH\n",
      "0 :  tarjanEH ['tarjana'] [{'noun': ['i. pl. n.']}]\n",
      "0 :  tarjan ['tarj'] [{'verbform': ['\"http:sanskrit.inria.frMW106.html#H_tarj\"'], 'verb': [['ppr. [1] ac.']], 'noun': ['nom. sg. m.']}]\n",
      "0 :  tarja ['tarj'] [{'verb': ['imp. [1] ac. sg. 2']}]\n",
      "5 :  na ['na'] [{'indeclinable': ['part.']}]\n",
      "6 :  EH [''] [{'verb': ['impft. [2] ac. sg. 2']}]\n",
      "6 :  EH [''] [{'verb': ['impft. [2] ac. sg. 2']}]\n",
      "Analyzing  jAtavepaTuH\n",
      "0 :  jAta ['jAta', 'jan'] [{'compound': ['iic.']}, {'verb': ['pp.']}]\n",
      "4 :  vepaTuH ['vepaTu'] [{'noun': ['nom. sg. m.']}]\n",
      "4 :  vepaTuH ['vepaTu'] [{'noun': ['nom. sg. f.']}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use 37171.p\n",
    "print(sentenceObj.sent_id)\n",
    "SeeSentence(sentenceObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
