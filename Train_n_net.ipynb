{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bUILT-iN pACKAGES\n",
    "import sys, os, time, bz2, zlib, pickle, math, json, csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "## lAST sUMMER\n",
    "from romtoslp import *\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "import MatDB\n",
    "from heap_n_PrimMST import *\n",
    "from ECL_MST import *\n",
    "\n",
    "## lAST yEAR\n",
    "# from word_definite import *\n",
    "from nnet import *\n",
    "# from heap_n_PrimMST import *\n",
    "# from word_definite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from MatDB import *\n",
    "import word_definite as WD\n",
    "from heap_n_PrimMST import *\n",
    "from nnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'winsound'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d09fcf8a2d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwinsound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplayBeep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mwinsound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mwinsound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'winsound'"
     ]
    }
   ],
   "source": [
    "import winsound\n",
    "def playBeep():\n",
    "    for i in range(3):\n",
    "        winsound.Beep(2200, 300)\n",
    "        winsound.Beep(2600, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'), encoding=u'utf-8')\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'), encoding=u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset_4k_1k = pickle.load(open('../SmallDataset_4K_1K.p', 'rb'))\n",
    "# TrainFiles = dataset_4k_1k['TrainFiles']\n",
    "# TestFiles = dataset_4k_1k['TestFiles']\n",
    "\n",
    "# dataset_6k_3k = pickle.load(open('../SmallDataset_6K_3K.p', 'rb'))\n",
    "# TrainFiles_2 = dataset_6k_3k['TrainFiles']\n",
    "# TestFiles_2 = dataset_6k_3k['TestFiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matDB = MatDB.MatDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957 files will not be used for training\n",
      "Size of training set: 8219\n"
     ]
    }
   ],
   "source": [
    "# Create Training File List\n",
    "excluded_files = []\n",
    "with open('inputs/Baseline4_advSample.csv', 'r') as f_handle:\n",
    "    opener = csv.reader(f_handle)\n",
    "    for line in opener:\n",
    "        excluded_files.append(line[1].replace('.p', '.ds.bz2'))\n",
    "\n",
    "bz2_input_folder = '../NewData/skt_dcs_DS.bz2_1L_bigram_10K/'\n",
    "# bz2_input_folder = '/home/rs/15CS91R05/vishnu/Data/skt_dcs_DS.bz2_compat_10k_check_again/'\n",
    "all_files = []\n",
    "skipped = 0\n",
    "for f in os.listdir(bz2_input_folder):\n",
    "    if '.ds.bz2' in f:\n",
    "        if f in excluded_files:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        all_files.append(f)\n",
    "\n",
    "print(skipped, 'files will not be used for training')\n",
    "print('Size of training set:', len(all_files))\n",
    "\n",
    "TrainFiles = all_files        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_dsbz2(filename):\n",
    "    with bz2.BZ2File(filename, 'r') as f:\n",
    "        loader = pickle.load(f)\n",
    "    \n",
    "    conflicts_Dict_correct = loader['conflicts_Dict_correct']\n",
    "    nodelist_to_correct_mapping = loader['nodelist_to_correct_mapping']\n",
    "    nodelist_correct = loader['nodelist_correct']\n",
    "    featVMat_correct = loader['featVMat_correct']\n",
    "    featVMat = loader['featVMat']\n",
    "    conflicts_Dict = loader['conflicts_Dict']\n",
    "    nodelist = loader['nodelist']\n",
    "    \n",
    "    return (nodelist_correct, conflicts_Dict_correct, featVMat_correct, nodelist_to_correct_mapping,\\\n",
    "            nodelist, conflicts_Dict, featVMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "######################  CREATE SEVERAL DATA STRUCTURES FROM SENTENCE/DCS  ######################\n",
    "###########################  NODELIST, ADJACENCY LIST, GRAPH, HEAP #############################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "def GetTrainingKit(sentenceObj, dcsObj):\n",
    "    nodelist = GetNodes(sentenceObj)\n",
    "    \n",
    "    # Nodelist with only the correct_nodes\n",
    "    nodelist2 = GetNodes(sentenceObj)\n",
    "    nodelist2_to_correct_mapping = {}\n",
    "    nodelist_correct = []\n",
    "    search_key = 0\n",
    "    first_key = 0\n",
    "    for chunk_id in range(len(dcsObj.lemmas)):\n",
    "        while nodelist2[first_key].chunk_id != chunk_id:\n",
    "            first_key += 1\n",
    "        for j in range(len(dcsObj.lemmas[chunk_id])):\n",
    "            search_key = first_key\n",
    "            while (nodelist2[search_key].lemma != rom_slp(dcsObj.lemmas[chunk_id][j])) or (nodelist2[search_key].cng != dcsObj.cng[chunk_id][j]):\n",
    "                search_key += 1\n",
    "                if search_key >= len(nodelist2) or nodelist2[search_key].chunk_id > chunk_id:\n",
    "                    break\n",
    "    #         print((rom_slp(dcsObj.lemmas[chunk_id][j]), dcsObj.cng[chunk_id][j]))\n",
    "    #         print(nodelist[search_key])\n",
    "            nodelist2_to_correct_mapping[len(nodelist_correct)] = search_key\n",
    "            nodelist_correct.append(nodelist2[search_key])\n",
    "    return (nodelist, nodelist_correct, nodelist2_to_correct_mapping)\n",
    "    \n",
    "\n",
    "def GetGraph(nodelist, neuralnet):\n",
    "    if not neuralnet.outer_relu:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat, SigmoidGateOutput)\n",
    "    else:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingStatus = defaultdict(lambda: bool(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  TRAIN FUNCTION  ################################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "def train_generator(loaded_SKT, loaded_DCS, bz2_input_folder, n_trainset = -1, iterationPerBatch = 10, filePerBatch = 20, _debug = True):\n",
    "    # Train\n",
    "    if n_trainset == -1:\n",
    "        n_trainset = len(TrainFiles)\n",
    "        totalBatchToTrain = math.ceil(n_trainset/filePerBatch)\n",
    "    else:\n",
    "        totalBatchToTrain = math.ceil(n_trainset/filePerBatch)\n",
    "    \n",
    "    register_nnet(trainer.neuralnet, bz2_input_folder)\n",
    "    for iterout in range(totalBatchToTrain):\n",
    "        # Add timer\n",
    "        startT = time.time()\n",
    "\n",
    "        # Change current batch\n",
    "        if(iterout % 50 == 0):\n",
    "            trainer.Save(p_name.replace('.p', '_i{}.p'.format(iterout)))\n",
    "        else:\n",
    "            trainer.Save(p_name)\n",
    "        print('Batch: ', iterout)\n",
    "        files_for_batch = TrainFiles[iterout*filePerBatch:(iterout + 1)*filePerBatch]\n",
    "        print(files_for_batch)\n",
    "        # trainer.Load('outputs/neuralnet_trained.p')\n",
    "        try:\n",
    "            # Run few times on same set of files\n",
    "            for iterin in range(iterationPerBatch):\n",
    "                print('ITERATION IN', iterin)        \n",
    "                for fn in files_for_batch:\n",
    "                    trainFileName = fn.replace('.ds.bz2', '.p2')\n",
    "                    sentenceObj = loaded_SKT[trainFileName]\n",
    "                    dcsObj = loaded_DCS[trainFileName]\n",
    "                    if trainingStatus[sentenceObj.sent_id]:\n",
    "                        continue\n",
    "                    # trainer.Save('outputs/saved_trainer.p')\n",
    "                    try:\n",
    "                        trainer.Train(sentenceObj, dcsObj, bz2_input_folder, _debug)\n",
    "                    except (IndexError, KeyError) as e:\n",
    "                        print('\\x1b[31mFailed: {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "                sys.stdout.flush() # Flush IO buffer \n",
    "            finishT = time.time()\n",
    "            print('Avg. time taken by 1 file(1 iteration): {:.3f}'.format((finishT - startT)/(iterationPerBatch*filePerBatch)))\n",
    "        except KeyboardInterrupt:\n",
    "            print('Training paused')\n",
    "            trainer.Save(p_name)\n",
    "            yield None\n",
    "    trainer.Save(p_name)\n",
    "                \n",
    "def test(loaded_SKT, loaded_DCS, n_testSet = -1, _testFiles = None, n_checkpt = 100):\n",
    "    total_lemma = 0;\n",
    "    correct_lemma = 0;\n",
    "\n",
    "    total_word = 0;\n",
    "    total_output_nodes = 0\n",
    "    correct_word = 0;\n",
    "    file_counter = 0\n",
    "    if _testFiles is None:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = TestFiles\n",
    "        else:\n",
    "            _testFiles = TestFiles[0:n_testSet]\n",
    "    else:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = _testFiles\n",
    "        else:\n",
    "            _testFiles = _testFiles[0:n_testSet]\n",
    "            \n",
    "    recalls = []\n",
    "    recalls_of_word = []\n",
    "    precisions = []\n",
    "    precisions_of_words = []\n",
    "    for fn in _testFiles:\n",
    "        if file_counter % n_checkpt == 0:\n",
    "            print(file_counter,' Checkpoint... ')\n",
    "            sys.stdout.flush() # Flush IO buffer \n",
    "        \n",
    "        file_counter += 1\n",
    "        \n",
    "        testFileName = fn.replace('.ds.bz2', '.p2')\n",
    "        sentenceObj = loaded_SKT[testFileName]\n",
    "        dcsObj = loaded_DCS[testFileName]\n",
    "        \n",
    "        try:\n",
    "            (word_match, lemma_match, n_dcsWords, n_output_nodes) = trainer.Test(sentenceObj, dcsObj)\n",
    "            \n",
    "            recalls.append(lemma_match/n_dcsWords)\n",
    "            recalls_of_word.append(word_match/n_dcsWords)\n",
    "            \n",
    "            precisions.append(lemma_match/n_output_nodes)\n",
    "            precisions_of_words.append(word_match/n_output_nodes)\n",
    "            \n",
    "            total_lemma += n_dcsWords\n",
    "            total_word += n_dcsWords\n",
    "            \n",
    "            total_output_nodes += n_output_nodes            \n",
    "            \n",
    "            correct_lemma += lemma_match\n",
    "            correct_word += word_match\n",
    "        except (IndexError, KeyError) as e:\n",
    "            print('Failed!')        \n",
    "\n",
    "    print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "    print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "    print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "    print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))\n",
    "    \n",
    "    return (recalls, recalls_of_word, precisions, precisions_of_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOW-OLD FUNCTION\n",
    "# def GetLoss(_mst_adj_graph, _mask_de_correct_edges, _negLogLikelies):\n",
    "#     _negLogLikelies = _negLogLikelies.copy()\n",
    "#     _negLogLikelies[~_mst_adj_graph] = 0\n",
    "#     _negLogLikelies[~_mask_de_correct_edges] *= -1 # BAKA!!! Check before you try to fix this again\n",
    "#     return np.sum(_negLogLikelies)    \n",
    "\n",
    "# NEW FUNCTION\n",
    "def GetLoss(_mst_adj_graph, _mask_de_correct_edges, _WScalarMat):\n",
    "    _WScalarMat = _WScalarMat.copy()\n",
    "    _WScalarMat[_mst_adj_graph&(~_mask_de_correct_edges)] *= -1 # BAKA!!! Check before you try to fix this again\n",
    "    _WScalarMat[~_mst_adj_graph] = 0\n",
    "    return np.sum(_WScalarMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nEURAL nET wILL bE sAVED hERE:  outputs/train_t2246772831595/nnet.p\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  GET A FILENAME TO SAVE WEIGHTS  ################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "import time\n",
    "st = str(int((time.time() * 1e6) % 1e13))\n",
    "log_name = 'logs/train_nnet_t{}.out'.format(st)\n",
    "odir = 'outputs/train_t{}'.format(st)\n",
    "p_name = 'outputs/train_t{}/nnet.p'.format(st)\n",
    "print('nEURAL nET wILL bE sAVED hERE: ', p_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def register_nnet(nnet, bz2_input_folder):\n",
    "    if not os.path.isdir(odir):\n",
    "        os.mkdir(odir)\n",
    "    if not os.path.isfile('outputs/nnet_LOGS.csv'):\n",
    "        with open('outputs/nnet_LOGS.csv', 'a') as fh:\n",
    "            csv_r = csv.writer(fh)\n",
    "            csv_r.writerow(['odir', 'p_name', 'hidden_layer_size', '_edge_vector_dim'])\n",
    "    with open('outputs/nnet_LOGS.csv', 'a') as fh:\n",
    "        csv_r = csv.writer(fh)\n",
    "        if nnet.version == 'h1':\n",
    "            csv_r.writerow([odir, p_name, nnet.n, nnet.d, bz2_input_folder])\n",
    "        elif nnet.version == 'h2':\n",
    "            csv_r.writerow([odir, p_name, nnet.h1, nnet.h2, nnet.d, bz2_input_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, modelFile = None):\n",
    "        if modelFile is None:\n",
    "            singleLayer = True\n",
    "            self._edge_vector_dim = 1500            \n",
    "            if singleLayer:\n",
    "                self.hidden_layer_size = 1200\n",
    "                keep_prob = 0.7\n",
    "                self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True, keep_prob=keep_prob)\n",
    "            else:\n",
    "                # DeepR Network\n",
    "                self.hidden_layer_size = 800\n",
    "                self.hidden_layer_size2 = 800\n",
    "                self.neuralnet = NN_2(self._edge_vector_dim, self.hidden_layer_size,\\\n",
    "                                      hidden_layer_2_size = self.hidden_layer_size2, outer_relu=True)\n",
    "                self.history = defaultdict(lambda: list())\n",
    "        else:\n",
    "            loader = pickle.load(open(filename, 'rb'))\n",
    "            \n",
    "            self.neuralnet.n = loader['n']\n",
    "            self.neuralnet.d = loader['d']\n",
    "\n",
    "            self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True)\n",
    "\n",
    "            self.neuralnet.U = loader['U']\n",
    "            self.neuralnet.W = loader['W']\n",
    "            self.neuralnet.B1 = loader['B1']\n",
    "            self.neuralnet.B2 = loader['B2']\n",
    "            \n",
    "            self.history = defaultdict(lambda: list())\n",
    "            \n",
    "        # SET LEARNING RATES\n",
    "        if self.neuralnet.version == 'h1':\n",
    "            self.neuralnet.etaW = 3e-4\n",
    "            self.neuralnet.etaB1 = 1e-4\n",
    "\n",
    "            self.neuralnet.etaU = 1e-4\n",
    "            self.neuralnet.etaB2 = 1e-4\n",
    "        elif self.neuralnet.version == 'h2':\n",
    "            self.neuralnet.etaW1 = 3e-4\n",
    "            self.neuralnet.etaB1 = 1e-4\n",
    "\n",
    "            self.neuralnet.etaW2 = 1e-4\n",
    "            self.neuralnet.etaB2 = 1e-4\n",
    "            \n",
    "            self.neuralnet.etaU = 1e-4\n",
    "            self.neuralnet.etaB3 = 1e-4\n",
    "            \n",
    "            \n",
    "    def Reset(self):\n",
    "        self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size)\n",
    "        self.history = defaultdict(lambda: list())\n",
    "        \n",
    "    def Save(self, filename):\n",
    "        print('Weights Saved: ', filename)\n",
    "        if self.neuralnet.version == 'h1':\n",
    "            pickle.dump({\n",
    "                    'U': self.neuralnet.U,\n",
    "                    'W': self.neuralnet.W,\n",
    "                    'n': self.neuralnet.n,\n",
    "                    'd': self.neuralnet.d,\n",
    "                    'B1': self.neuralnet.B1,\n",
    "                    'B2': self.neuralnet.B2,\n",
    "                    'keep_prob': self.neuralnet.keep_prob,\n",
    "                    'version': self.neuralnet.version\n",
    "                }, open(filename, 'wb'))\n",
    "            return\n",
    "        elif self.neuralnet.version == 'h2':\n",
    "            pickle.dump({\n",
    "                    'U': self.neuralnet.U,\n",
    "                    'B3': self.neuralnet.B3,\n",
    "                    'W2': self.neuralnet.W2,\n",
    "                    'B2': self.neuralnet.B2,\n",
    "                    'W1': self.neuralnet.W1,\n",
    "                    'B1': self.neuralnet.B1,\n",
    "                    'h1': self.neuralnet.h1,\n",
    "                    'h2': self.neuralnet.h2,\n",
    "                    'd': self.neuralnet.d,\n",
    "                    'version': self.neuralnet.version\n",
    "                }, open(filename, 'wb'))\n",
    "            return\n",
    "        \n",
    "    \n",
    "    def Load(self, filename):\n",
    "        loader = pickle.load(open(filename, 'rb'))\n",
    "        if 'version' not in loader: # means 1 hidden layer\n",
    "            self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True)\n",
    "            self.neuralnet.U = loader['U']\n",
    "            self.neuralnet.W = loader['W']\n",
    "            self.neuralnet.B1 = loader['B1']\n",
    "            self.neuralnet.B2 = loader['B2']\n",
    "            self.neuralnet.hidden_layer_size = loader['n']\n",
    "            self.neuralnet._edge_vector_dim = loader['d']\n",
    "            if 'keep_prob' in loader:\n",
    "                self.neuralnet.keep_prob = loader['keep_prob']\n",
    "                self.neuralnet.dropout_prob = 1 - loader['keep_prob']\n",
    "            print('Keep Prob = {}, Dropout = {}'.format(self.neuralnet.keep_prob, self.neuralnet.dropout_prob))\n",
    "        else:\n",
    "            if loader['version'] == 'h1':\n",
    "                self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True)\n",
    "                self.neuralnet.U = loader['U']\n",
    "                self.neuralnet.W = loader['W']\n",
    "                self.neuralnet.B1 = loader['B1']\n",
    "                self.neuralnet.B2 = loader['B2']\n",
    "                self.neuralnet.hidden_layer_size = loader['n']\n",
    "                self.neuralnet._edge_vector_dim = loader['d']\n",
    "                if 'keep_prob' in loader:\n",
    "                    self.neuralnet.keep_prob = loader['keep_prob']\n",
    "                    self.neuralnet.dropout_prob = 1 - loader['keep_prob']\n",
    "                print('Keep Prob = {}, Dropout = {}'.format(self.neuralnet.keep_prob, self.neuralnet.dropout_prob))\n",
    "            elif loader['version'] == 'h2':\n",
    "                self.neuralnet = NN_2(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True)\n",
    "                \n",
    "                self.neuralnet.U = loader['U']\n",
    "                self.neuralnet.B3 = loader['B3']\n",
    "                \n",
    "                self.neuralnet.W2 = loader['W2']\n",
    "                self.neuralnet.B2 = loader['B2']\n",
    "                \n",
    "                self.neuralnet.W1 = loader['W1']\n",
    "                self.neuralnet.B1 = loader['B1']\n",
    "                \n",
    "                self.neuralnet.h1 = loader['h1']\n",
    "                self.neuralnet.h2 = loader['h2']\n",
    "                self.neuralnet.d = loader['d']\n",
    "    def CalculateLoss_n_Grads(self, WScalarMat, min_st_adj_worst, max_st_adj_gold, loss_type = 0, min_marginalized_energy = None):\n",
    "        doBpp = True\n",
    "        \n",
    "        # Claculate the enrgies\n",
    "        etg = np.sum(WScalarMat[max_st_adj_gold])\n",
    "        etq = np.sum(WScalarMat[min_st_adj_worst])\n",
    "        \n",
    "        if loss_type == 0:\n",
    "            # Variable Hinge Loss - CHECKED\n",
    "            L = etg - min_marginalized_energy\n",
    "            if L > 0:\n",
    "                dLdOut = np.zeros_like(WScalarMat)\n",
    "                dLdOut[max_st_adj_gold&(~min_st_adj_worst)] = 1\n",
    "                dLdOut[(~max_st_adj_gold)&min_st_adj_worst] = -1\n",
    "            else:\n",
    "                doBpp = False\n",
    "                return (L, None, doBpp)\n",
    "        elif loss_type == 1:\n",
    "            # LOg Loss\n",
    "            a = etg - etq\n",
    "            b = np.exp(a)\n",
    "            L = np.log(1 + b)\n",
    "            \n",
    "            dLdOut = np.zeros_like(WScalarMat)\n",
    "            dLdOut[max_st_adj_gold&(~min_st_adj_worst)] = 1\n",
    "            dLdOut[(~max_st_adj_gold)&min_st_adj_worst] = -1\n",
    "            \n",
    "            dLdOut *= (b/(1 + b))\n",
    "        elif loss_type == 2:\n",
    "            # Square exponential loss\n",
    "            gamma = 1\n",
    "            b = np.exp(-etq)\n",
    "            \n",
    "            L = etg**2 + gamma*b\n",
    "            \n",
    "            dLdOut = np.zeros_like(WScalarMat)\n",
    "            dLdOut[max_st_adj_gold&(~min_st_adj_worst)] = 2*etg\n",
    "            dLdOut[(~max_st_adj_gold)&min_st_adj_worst] = -gamma*b\n",
    "            pass\n",
    "        return (L, dLdOut, doBpp)\n",
    "    def Test(self, sentenceObj, dcsObj, dsbz2_name):\n",
    "        self.neuralnet.ForTesting()\n",
    "        neuralnet = self.neuralnet\n",
    "        minScore = np.inf\n",
    "        minMst = None\n",
    "        \n",
    "        # dsbz2_name = sentenceObj.sent_id + '.ds.bz2'\n",
    "        (nodelist_correct, conflicts_Dict_correct, featVMat_correct, nodelist_to_correct_mapping,\\\n",
    "            nodelist, conflicts_Dict, featVMat) = open_dsbz2(dsbz2_name)\n",
    "        \n",
    "        # if len(nodelist) > 50:\n",
    "        #     return None\n",
    "\n",
    "        if not self.neuralnet.outer_relu:\n",
    "            (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        else:\n",
    "            WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        \n",
    "        # print('NeuralNet Time: ', time.time() - startT)\n",
    "        # startT = time.time()\n",
    "        \n",
    "        # Get all MST\n",
    "        for source in range(len(nodelist)):\n",
    "            (mst_nodes, mst_adj_graph, _) = MST(nodelist, WScalarMat, conflicts_Dict, source)\n",
    "            # print('.', end = '')\n",
    "            score = GetMSTWeight(mst_adj_graph, WScalarMat)\n",
    "            if(score < minScore):\n",
    "                minScore = score\n",
    "                minMst = mst_nodes\n",
    "        dcsLemmas = [[rom_slp(l) for l in arr]for arr in dcsObj.lemmas]\n",
    "        word_match = 0\n",
    "        lemma_match = 0\n",
    "        n_output_nodes = 0\n",
    "        for chunk_id, wdSplit in minMst.items():\n",
    "            for wd in wdSplit:\n",
    "                n_output_nodes += 1\n",
    "                # Match lemma\n",
    "                search_result = [i for i, j in enumerate(dcsLemmas[chunk_id]) if j == wd.lemma]\n",
    "                if len(search_result) > 0:\n",
    "                    lemma_match += 1\n",
    "                # Match CNG\n",
    "                for i in search_result:\n",
    "                    if(dcsObj.cng[chunk_id][i] == str(wd.cng)):\n",
    "                        word_match += 1\n",
    "                        # print(wd.lemma, wd.cng)\n",
    "                        break\n",
    "        dcsLemmas = [l for arr in dcsObj.lemmas for l in arr]\n",
    "        \n",
    "        # print('All MST Time: ', time.time() - startT)\n",
    "        # print('Node Count: ', len(nodelist))\n",
    "#         print('\\nFull Match: {}, Partial Match: {}, OutOf {}, NodeCount: {}, '.\\\n",
    "#               format(word_match, lemma_match, len(dcsLemmas), len(nodelist)))\n",
    "        return (word_match, lemma_match, len(dcsLemmas), n_output_nodes)\n",
    "    \n",
    "    def Train(self, sentenceObj, dcsObj, bz2_input_folder, _debug = True):\n",
    "        self.neuralnet.ForTraining()\n",
    "        self.neuralnet.new_dropout() # renew dropout setting\n",
    "        # Hyperparameter for hinge loss: m\n",
    "        m_hinge_param = 14\n",
    "        \n",
    "        dsbz2_name = sentenceObj.sent_id + '.ds.bz2'\n",
    "        (nodelist_correct, conflicts_Dict_correct, featVMat_correct, nodelist_to_correct_mapping,\\\n",
    "            nodelist, conflicts_Dict, featVMat) = open_dsbz2(bz2_input_folder + dsbz2_name)\n",
    "        # Train for large graphs separately\n",
    "#         if len(nodelist) < 40:\n",
    "#             return\n",
    "        \n",
    "        \"\"\" FORM MAXIMUM(ENERGY) SPANNING TREE OF THE GOLDEN GRAPH : WORST GOLD STRUCTURE \"\"\"\n",
    "        WScalarMat_correct = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat_correct, nodelist_correct,\\\n",
    "                                                                      conflicts_Dict_correct, self.neuralnet)\n",
    "        source = 0\n",
    "        \"\"\" Find the max spanning tree : negative Weight matrix passed \"\"\"\n",
    "#         (max_st_gold_ndict, max_st_adj_gold_small, _) =\\\n",
    "#             MST(nodelist_correct, -WScalarMat_correct, conflicts_Dict_correct, source)\n",
    "        (max_st_gold_ndict, max_st_adj_gold_small, _) =\\\n",
    "            MST(nodelist_correct, -WScalarMat_correct, conflicts_Dict_correct, source)\n",
    "        energy_gold_max_ST = np.sum(WScalarMat_correct[max_st_adj_gold_small])\n",
    "        \n",
    "        \"\"\" Convert correct spanning tree graph adj matrix to full marix dimensions \"\"\"\n",
    "        \"\"\" Create full-size adjacency matrix for correct_mst_small \"\"\"\n",
    "        nodelen = len(nodelist)\n",
    "        max_st_adj_gold = np.ndarray((nodelen, nodelen), np.bool)*False # T_STAR\n",
    "        for i in range(max_st_adj_gold_small.shape[0]):\n",
    "            for j in range(max_st_adj_gold_small.shape[1]):\n",
    "                max_st_adj_gold[nodelist_to_correct_mapping[i], nodelist_to_correct_mapping[j]] =\\\n",
    "                    max_st_adj_gold_small[i, j]\n",
    "        \n",
    "        \"\"\" Delta(Margin) Function : MASK FOR WHICH NODES IN NODELIST BELONG TO DCS \"\"\"\n",
    "        gold_nodes_mask = np.array([False]*len(nodelist))\n",
    "        gold_nodes_mask[list(nodelist_to_correct_mapping.values())] = True\n",
    "        margin_f = lambda nodes_mask: np.sum(nodes_mask&(~gold_nodes_mask))**2\n",
    "        \n",
    "        \"\"\" FOR ALL POSSIBLE MST FROM THE COMPLETE GRAPH \"\"\"\n",
    "        WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, self.neuralnet)\n",
    "\n",
    "        \"\"\" For each node - Find MST with that source\"\"\"\n",
    "        min_STx = None # Min Energy spanning tree with worst margin with gold_STx\n",
    "        min_marginalized_energy = np.inf\n",
    "        \n",
    "        # Generate random set of nodes from which mSTs are to be considered\n",
    "        n_nodes = len(nodelist)\n",
    "        selection_prob = 0.4\n",
    "        select_flag = np.random.rand(n_nodes) < selection_prob\n",
    "        # Fix if all zeros\n",
    "        if np.sum(select_flag) == 0:\n",
    "            select_flag[np.random.randint(n_nodes)] = 1\n",
    "        \n",
    "        best_node_diff = np.Inf\n",
    "        best_energy = np.Inf\n",
    "        for source in range(len(nodelist)):\n",
    "            (mst_nodes, mst_adj_graph, mst_nodes_bool) = MST(nodelist, WScalarMat, conflicts_Dict, source) # T_X\n",
    "            # Calculate energy of spanning tree\n",
    "            en_st = np.sum(WScalarMat[mst_adj_graph])\n",
    "            \n",
    "            # Pick up the node_diff with lowest energy\n",
    "            delta_st = margin_f(mst_nodes_bool)\n",
    "            if delta_st == 0:\n",
    "                # SKIP... It's a golden tree\n",
    "                continue\n",
    "            if _debug:\n",
    "                if best_energy > en_st:\n",
    "                    best_node_diff = delta_st\n",
    "                    best_energy = en_st\n",
    "                \n",
    "            # Minimum marginalized energy calculation\n",
    "            marginalized_en = en_st - delta_st\n",
    "            # Minimum marginalized spanning tree : Randomization applied\n",
    "            # if marginalized_en < min_marginalized_energy and select_flag[source]:\n",
    "            if marginalized_en < min_marginalized_energy:\n",
    "                min_marginalized_energy = marginalized_en\n",
    "                min_STx = mst_adj_graph\n",
    "            # Energy diff should all be negative\n",
    "            if _debug:\n",
    "                print('Source: [{}], Node_Diff:{}, Max_Gold_En: {:.3f}, Energy: {:.3f}'.\\\n",
    "                      format(source, np.sum((~gold_nodes_mask)&mst_nodes_bool), energy_gold_max_ST,  np.sum(WScalarMat[mst_adj_graph])))\n",
    "        \n",
    "        if best_energy == np.Inf:\n",
    "            # Whoops all trees are gold\n",
    "            trainingStatus[sentenceObj.sent_id] = True\n",
    "            return\n",
    "        \n",
    "        if _debug:\n",
    "            print('Best Node diff: {} with EN: {}'.format(np.sqrt(best_node_diff), best_energy))\n",
    "        \"\"\" Gradient Descent \"\"\"\n",
    "        # LOSS TYPES -> hinge(0), log-loss(1), square-exponential(2)\n",
    "        Total_Loss, dLdOut, doBpp = self.CalculateLoss_n_Grads(WScalarMat, min_STx, max_st_adj_gold,\\\n",
    "                                                                 loss_type = 0, min_marginalized_energy = min_marginalized_energy)\n",
    "        if doBpp:\n",
    "            if _debug:\n",
    "                print('{}. '.format(sentenceObj.sent_id), end = '')\n",
    "            self.neuralnet.Back_Prop(dLdOut, len(nodelist), featVMat, _debug)\n",
    "        else:\n",
    "            trainingStatus[sentenceObj.sent_id] = True\n",
    "        if _debug:\n",
    "            print(\"\\nFileKey: %s, Loss: %6.3f\" % (sentenceObj.sent_id, Total_Loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainer.neuralnet.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = None\n",
    "def InitModule():\n",
    "    global trainer\n",
    "    trainer = Trainer()\n",
    "InitModule()\n",
    "trainingStatus = defaultdict(lambda: bool(False))\n",
    "# trainer.Load('outputs/train_nnet_t526539685574.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6, 6, 7)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "# if __name__ == '__main__':\n",
    "# loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'))\n",
    "# loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'))\n",
    "# main(loaded_SKT, loaded_DCS)\n",
    "np.set_printoptions(suppress=False)\n",
    "\n",
    "ft = '433682.ds.bz2'\n",
    "fn = ft.replace('.ds.bz2', '.p2')\n",
    "\n",
    "# print(fn)\n",
    "# for _ in range(1):\n",
    "#     trainer.Train(loaded_SKT[fn], loaded_DCS[fn], bz2_input_folder, _debug = True)\n",
    "\n",
    "# # fn = TestFiles[1].replace('.ds.bz2', '.p2')\n",
    "trainer.Test(loaded_SKT[fn], loaded_DCS[fn], os.path.join(bz2_input_folder, ft))\n",
    "# print (\"Not Implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_generator(loaded_SKT, loaded_DCS, bz2_input_folder, n_trainset = -1, filePerBatch = 10, iterationPerBatch = 5, _debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Saved:  outputs/train_t2142685031803/nnet_i0.p\n",
      "Batch:  0\n",
      "['10250.ds.bz2', '428839.ds.bz2', '20035.ds.bz2', '32130.ds.bz2', '418916.ds.bz2', '154658.ds.bz2', '65093.ds.bz2', '266079.ds.bz2', '429657.ds.bz2', '389979.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 0.359\n",
      "Weights Saved:  outputs/train_t2142685031803/nnet.p\n",
      "Batch:  1\n",
      "['365964.ds.bz2', '29623.ds.bz2', '162846.ds.bz2', '296029.ds.bz2', '338298.ds.bz2', '187738.ds.bz2', '171533.ds.bz2', '310155.ds.bz2', '22207.ds.bz2', '12964.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 0.799\n",
      "Weights Saved:  outputs/train_t2142685031803/nnet.p\n",
      "Batch:  2\n",
      "['231593.ds.bz2', '399906.ds.bz2', '194861.ds.bz2', '130103.ds.bz2', '433682.ds.bz2', '164353.ds.bz2', '35530.ds.bz2', '240430.ds.bz2', '37993.ds.bz2', '57802.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 0.484\n",
      "Weights Saved:  outputs/train_t2142685031803/nnet.p\n",
      "Batch:  3\n",
      "['168593.ds.bz2', '34892.ds.bz2', '323508.ds.bz2', '36688.ds.bz2', '14516.ds.bz2', '137350.ds.bz2', '43470.ds.bz2', '41634.ds.bz2', '32517.ds.bz2', '181050.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 0.910\n",
      "Weights Saved:  outputs/train_t2142685031803/nnet.p\n",
      "Batch:  4\n",
      "['120998.ds.bz2', '321200.ds.bz2', '29793.ds.bz2', '124631.ds.bz2', '158557.ds.bz2', '438554.ds.bz2', '164299.ds.bz2', '99737.ds.bz2', '436332.ds.bz2', '155208.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.200\n",
      "Weights Saved:  outputs/train_t2142685031803/nnet.p\n",
      "Batch:  5\n",
      "['128691.ds.bz2', '392212.ds.bz2', '428809.ds.bz2', '59060.ds.bz2', '7841.ds.bz2', '30996.ds.bz2', '331890.ds.bz2', '9659.ds.bz2', '85655.ds.bz2', '109481.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Training paused\n",
      "Weights Saved:  outputs/train_t2142685031803/nnet.p\n"
     ]
    }
   ],
   "source": [
    "# Complete Training\n",
    "# tips: try increasing iterations per batch\n",
    "trainingStatus = defaultdict(lambda: bool(False)) # Reset it after 3 epochs of full-training set\n",
    "train.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking bz2 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bz2_input_folder2 = '../NewData/skt_dcs_DS.bz2_1L_bigram_10K/'\n",
    "dsbz2_name = '226964' + '.ds.bz2'\n",
    "(nodelist_correct2, conflicts_Dict_correct, featVMat_correct, nodelist_to_correct_mapping,\\\n",
    "    nodelist, conflicts_Dict2, featVMat) = open_dsbz2(bz2_input_folder + dsbz2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WD_Node[C: 0, P: 0, brU @(-43) => abravIt],\n",
       " WD_Node[C: 1, P: 0, rAvaRa @(69) => rAvaRam],\n",
       " WD_Node[C: 2, P: 0, sUta @(29) => sUtas],\n",
       " WD_Node[C: 3, P: 0, hita @(71) => hitam],\n",
       " WD_Node[C: 4, P: 1, anunaya @(71) => anunayam],\n",
       " WD_Node[C: 5, P: 0, vacas @(31) => vacas],\n",
       " WD_Node[C: 5, P: 0, vacas @(71) => vacas]]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodelist_correct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 WD_Node[C: 0, P: 0, brU @(-43) => abravIt]\n",
      "1 WD_Node[C: 1, P: 0, rAvaRa @(69) => rAvaRam]\n",
      "2 WD_Node[C: 2, P: 0, sUta @(29) => sUtas]\n",
      "3 WD_Node[C: 2, P: 0, sUta @(29) => sUtas]\n",
      "4 WD_Node[C: 2, P: 0, sU @(-190) => sUtas]\n",
      "5 WD_Node[C: 3, P: 0, hi @(2) => hi]\n",
      "6 WD_Node[C: 3, P: 0, hita @(71) => hitam]\n",
      "7 WD_Node[C: 3, P: 0, DA @(-190) => hitam]\n",
      "8 WD_Node[C: 3, P: 2, tad @(69) => tam]\n",
      "9 WD_Node[C: 3, P: 0, hita @(71) => hitam]\n",
      "10 WD_Node[C: 4, P: 2, nu @(2) => nu]\n",
      "11 WD_Node[C: 4, P: 0, tad @(30) => sA]\n",
      "12 WD_Node[C: 4, P: 0, sa @(3) => sa]\n",
      "13 WD_Node[C: 4, P: 6, yad @(69) => yam]\n",
      "14 WD_Node[C: 4, P: 4, naya @(2) => nayam]\n",
      "15 WD_Node[C: 4, P: 1, anu @(3) => anu]\n",
      "16 WD_Node[C: 4, P: 4, na @(2) => na]\n",
      "17 WD_Node[C: 4, P: 1, anu @(3) => anu]\n",
      "18 WD_Node[C: 4, P: 1, anunaya @(71) => anunayam]\n",
      "19 WD_Node[C: 5, P: 0, vacas @(71) => vacas]\n"
     ]
    }
   ],
   "source": [
    "kx = 0\n",
    "for node in nodelist:\n",
    "    print(kx, node)\n",
    "    kx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2, 3: 6, 4: 18, 5: 20, 6: 19}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodelist_to_correct_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rs/15CS91R05/vishnu/Data/skt_dcs_DS.bz2_compat_10k_check_again/407288.ds.bz2'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bz2_input_folder + dsbz2_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######   THE END\n",
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beyond the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_prob = 0.8\n",
    "dropout_prob = 1- keep_prob\n",
    "r1 = np.random.binomial(1, dropout_prob, size=(10, 1))\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ENV(py3_bishal)",
   "language": "python",
   "name": "py3_bishal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
