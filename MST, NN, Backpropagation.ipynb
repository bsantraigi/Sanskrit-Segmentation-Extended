{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # MST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Graph and Other Data Structures\n",
    "\n",
    "- load skt - DONE\n",
    "- load dcs - DONE\n",
    "\n",
    "\n",
    "- create the full graph - sktwseg_utf8 - DONE\n",
    "- Nodelist - DONE\n",
    "- Features-Adjacency Matrix - DONE\n",
    "- Weight-Adjacency Matrix - DONE\n",
    "- Conflict Matrix - or - dictionary (since it's sparse) - DONE\n",
    "\n",
    "\n",
    "- new paths base on cng probability - DONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from IPython.display import display\n",
    "import json\n",
    "from word_definite import *\n",
    "from romtoslp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'))\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in list(loaded_SKT.keys())[100:]:\n",
    "    loaded_SKT.pop(k, None)\n",
    "    loaded_DCS.pop(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_SKT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_lem2cng_countonly = pickle.load(open('../NewData/ultimate_new_lastsem/mat_lem2cng_countonly.p', 'rb'))\n",
    "mat_cng2lem_countonly = pickle.load(open('../NewData/ultimate_new_lastsem/mat_cng2lem_countonly.p', 'rb'))\n",
    "mat_lem2cng_1D = pickle.load(open('../NewData/ultimate_new_lastsem/mat_lem2cng_1D.p', 'rb'))\n",
    "mat_cng2lem_1D = pickle.load(open('../NewData/ultimate_new_lastsem/mat_cng2lem_1D.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Form Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetTrainingKit(sentenceObj, dcsObj):\n",
    "    nodelist = GetNodes(sentenceObj)\n",
    "    \n",
    "    # Nodelist with only the correct_nodes\n",
    "    nodelist2 = GetNodes(sentenceObj)\n",
    "    nodelist2_to_correct_mapping = {}\n",
    "    nodelist_correct = []\n",
    "    search_key = 0\n",
    "    first_key = 0\n",
    "    for chunk_id in range(len(dcsObj.lemmas)):\n",
    "        while nodelist2[first_key].chunk_id != chunk_id:\n",
    "            first_key += 1\n",
    "        for j in range(len(dcsObj.lemmas[chunk_id])):\n",
    "            search_key = first_key\n",
    "\n",
    "            while (nodelist2[search_key].lemma != rom_slp(dcsObj.lemmas[chunk_id][j])) or (nodelist2[search_key].cng != int(dcsObj.cng[chunk_id][j])):\n",
    "                search_key += 1\n",
    "    #         print((rom_slp(dcsObj.lemmas[chunk_id][j]), dcsObj.cng[chunk_id][j]))\n",
    "    #         print(nodelist[search_key])\n",
    "            nodelist2_to_correct_mapping[len(nodelist_correct)] = search_key\n",
    "            nodelist_correct.append(nodelist2[search_key])\n",
    "    return (nodelist, nodelist_correct, nodelist2_to_correct_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = 72\n",
    "sentenceObj = loaded_SKT[list(loaded_SKT.keys())[fn]]\n",
    "dcsObj = loaded_DCS[list(loaded_SKT.keys())[fn]]\n",
    "# (chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain, qc_pairs) = SentencePreprocess(sentenceObj)\n",
    "(nodelist, nodelist_correct, nodelist_to_correct_mapping) = GetTrainingKit(sentenceObj, dcsObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 4, 3: 5, 4: 20, 5: 10}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodelist_to_correct_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n"
     ]
    }
   ],
   "source": [
    "_edge_vector_dim = len(mat_cng2lem_1D)\n",
    "_full_cnglist = list(mat_cng2lem_1D)\n",
    "print(_edge_vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from word_definite import *\n",
    "from nnet import *\n",
    "from heap_n_PrimMST import *\n",
    "from word_definite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organize_mst_nodes(mst_nodes):\n",
    "    sol_lemmas = []\n",
    "    sol_cngs = []\n",
    "    print()\n",
    "    total_tree_weight = 0\n",
    "    for key, val in mst_nodes.items():\n",
    "        print('{', end = '')\n",
    "        for node in val:\n",
    "            print(node.lemma, (node.cng, node.derived, node.src,), end = ', ')\n",
    "            if node.src != -1:\n",
    "                total_tree_weight += WScalarMat[node.src][node.id]\n",
    "        print('};', end = ' ')\n",
    "    print()\n",
    "    print('Weight of the spanning tree: ', total_tree_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neuralnet = NN(_edge_vector_dim, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation:  [52 77 73 40 86 39 57 27 91 53]\n",
      "ITERATION  0\n",
      "................\n",
      "FileKey: 52, Loss:  0.930, Original MSTScore:  7.726\n",
      "..................................\n",
      "FileKey: 77, Loss:  0.042, Original MSTScore:  2.130\n",
      "...............................................................\n",
      "FileKey: 40, Loss: -3.499, Original MSTScore:  4.152\n",
      ".....................................\n",
      "FileKey: 86, Loss: -2.106, Original MSTScore:  7.731\n",
      "..\n",
      "FileKey: 39, Loss: -0.002, Original MSTScore:  1.001\n",
      ".........................................\n",
      "FileKey: 57, Loss: -1.871, Original MSTScore:  7.373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/bishal/Documents/Sanskrit_project/Bishal/Clean_n_clear/word_definite.py\u001b[0m in \u001b[0;36mGet_Feat_Vec_Matrix\u001b[0;34m(nodelist_new, conflicts_Dict)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         \u001b[0mpleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_lem2cng_countonly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcng_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmat_lem2cng_1D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Bavzyat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-e8db1be4d5e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mconflicts_Dict_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGet_Conflicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodelist_correct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mfeatVMat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGet_Feat_Vec_Matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodelist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicts_Dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mfeatVMat_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGet_Feat_Vec_Matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodelist_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicts_Dict_correct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bishal/Documents/Sanskrit_project/Bishal/Clean_n_clear/word_definite.py\u001b[0m in \u001b[0;36mGet_Feat_Vec_Matrix\u001b[0;34m(nodelist_new, conflicts_Dict)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0;31m# TODO: Some lemma's still missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         \u001b[0mpleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_lem2cng_countonly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcng_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmat_lem2cng_1D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                         \u001b[0mpleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "perm = np.random.permutation(100)[0:10]\n",
    "print('Permutation: ', perm)\n",
    "for iteriter in range(10):\n",
    "    print('ITERATION ', iteriter)\n",
    "    pickle.dump(neuralnet, open('outputs/neuralnet_trained.p', 'wb'))\n",
    "    for fn in perm:\n",
    "        sentenceObj = loaded_SKT[list(loaded_SKT.keys())[fn]]\n",
    "        dcsObj = loaded_DCS[list(loaded_SKT.keys())[fn]]\n",
    "        # (chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain, qc_pairs) = SentencePreprocess(sentenceObj)\n",
    "        try:\n",
    "            (nodelist, nodelist_correct, nodelist_to_correct_mapping) = GetTrainingKit(sentenceObj, dcsObj)\n",
    "        except IndexError:\n",
    "            # print('Error with ', fn)\n",
    "            continue\n",
    "\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "        conflicts_Dict_correct = Get_Conflicts(nodelist_correct)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "        featVMat_correct = Get_Feat_Vec_Matrix(nodelist_correct, conflicts_Dict_correct)\n",
    "\n",
    "        WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        WScalarMat_correct = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat_correct, nodelist_correct, conflicts_Dict_correct, neuralnet)\n",
    "        Total_Loss = 0\n",
    "\n",
    "        # Get MST for the correct nodelist\n",
    "        source = 0\n",
    "        (mst_nodes_correct, mst_adj_graph_correct_0) = MST(nodelist_correct, WScalarMat_correct, conflicts_Dict_correct, source)\n",
    "    #     print('Correct MST Score: ', GetMSTWeight(mst_nodes_correct, WScalarMat_correct))\n",
    "        W_star = GetMSTWeight(mst_nodes_correct, WScalarMat_correct)\n",
    "\n",
    "        # Convert correct spanning tree graph adj matrix to actual marix dimensions\n",
    "        nodelen = len(nodelist)\n",
    "        mst_adj_graph_correct = np.ndarray((nodelen, nodelen), np.bool)*False\n",
    "        for i in range(mst_adj_graph_correct_0.shape[0]):\n",
    "            for j in range(mst_adj_graph_correct_0.shape[1]):\n",
    "                mst_adj_graph_correct[nodelist_to_correct_mapping[i], nodelist_to_correct_mapping[j]] = \\\n",
    "                mst_adj_graph_correct_0[i, j]\n",
    "\n",
    "        # Get all MST\n",
    "        dLdS = np.zeros(WScalarMat.shape)\n",
    "        for source in range(len(nodelist)):\n",
    "        #     print('\\nSOURCE: {}'.format(source))\n",
    "            (mst_nodes, mst_adj_graph) = MST(nodelist, WScalarMat, conflicts_Dict, source)    \n",
    "        #     print('Correct MST Score for Souce {}: {}'.format(source, GetMSTWeight(mst_nodes, WScalarMat)))\n",
    "            print('.', end = '')\n",
    "            Total_Loss += (W_star - GetMSTWeight(mst_nodes, WScalarMat))\n",
    "            dLdS_inner = 1 / WScalarMat\n",
    "            dLdS_inner[mst_adj_graph_correct == 1] *= -1\n",
    "            dLdS_inner = dLdS_inner*(mst_adj_graph^mst_adj_graph_correct)\n",
    "            dLdS += dLdS_inner        \n",
    "        neuralnet.Back_Prop(dLdS/len(nodelist), len(nodelist), featVMat)\n",
    "\n",
    "        Total_Loss /= len(nodelist)\n",
    "        print(\"\\nFileKey: %d, Loss: %6.3f, Original MSTScore: %6.3f\" % (fn, Total_Loss, W_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation:  [22 86 26 32 96 41  3 13 15  0]\n",
      "...................................................\n",
      "Full Match: 0, Partial Match: 1, OutOf 6, NodeCount: 47, \n",
      ".....................................\n",
      "Full Match: 2, Partial Match: 3, OutOf 7, NodeCount: 37, \n",
      "....................\n",
      "Full Match: 3, Partial Match: 4, OutOf 6, NodeCount: 16, \n",
      "..............................................\n",
      "Full Match: 3, Partial Match: 5, OutOf 7, NodeCount: 46, \n",
      "......\n",
      "Full Match: 2, Partial Match: 2, OutOf 2, NodeCount: 4, \n",
      "................\n",
      "Full Match: 5, Partial Match: 6, OutOf 6, NodeCount: 16, \n",
      "..............................................................................\n",
      "Full Match: 3, Partial Match: 3, OutOf 6, NodeCount: 78, \n",
      "\u001b[31mError with 13 \u001b[0m\n",
      "..............................\n",
      "Full Match: 5, Partial Match: 6, OutOf 8, NodeCount: 30, \n",
      "\u001b[31mError with 0 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "minScore = np.inf\n",
    "minMst = None\n",
    "\n",
    "perm = np.random.permutation(100)[0:10]\n",
    "print('Permutation: ', perm)\n",
    "for fn in perm:\n",
    "    sentenceObj = loaded_SKT[list(loaded_SKT.keys())[fn]]\n",
    "    dcsObj = loaded_DCS[list(loaded_SKT.keys())[fn]]\n",
    "    # (chunkDict, lemmaList, wordList, revMap2Chunk, qu, cngList, verbs, tuplesMain, qc_pairs) = SentencePreprocess(sentenceObj)\n",
    "    try:\n",
    "        (nodelist, nodelist_correct, _) = GetTrainingKit(sentenceObj, dcsObj)\n",
    "    except IndexError:\n",
    "        print('\\x1b[31mError with {} \\x1b[0m'.format(fn))\n",
    "        continue\n",
    "        \n",
    "    conflicts_Dict = Get_Conflicts(nodelist)\n",
    "    conflicts_Dict_correct = Get_Conflicts(nodelist_correct)\n",
    "    \n",
    "    featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "    featVMat_correct = Get_Feat_Vec_Matrix(nodelist_correct, conflicts_Dict_correct)\n",
    "    \n",
    "    WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "\n",
    "    # Get all MST\n",
    "    for source in range(len(nodelist)):\n",
    "        (mst_nodes, mst_adj_graph) = MST(nodelist, WScalarMat, conflicts_Dict, source)\n",
    "        print('.', end = '')\n",
    "        score = GetMSTWeight(mst_nodes, WScalarMat)\n",
    "        if(score < minScore):\n",
    "            print('.', end = '')\n",
    "            minScore = score\n",
    "            minMst = mst_nodes\n",
    "    dcsLemmas = [[rom_slp(l) for l in arr]for arr in dcsObj.lemmas]\n",
    "    full_match = 0\n",
    "    partial_match = 0\n",
    "    for chunk_id, wdSplit in mst_nodes.items():\n",
    "        for wd in wdSplit:\n",
    "            # Match lemma\n",
    "            search_result = [i for i, j in enumerate(dcsLemmas[chunk_id]) if j == wd.lemma]\n",
    "            if len(search_result) > 0:\n",
    "                partial_match += 1\n",
    "            # Match CNG\n",
    "            for i in search_result:\n",
    "                if(dcsObj.cng[chunk_id][i] == str(wd.cng)):\n",
    "                    full_match += 1\n",
    "                    # print(wd.lemma, wd.cng)\n",
    "                    break\n",
    "    dcsLemmas = [l for arr in dcsObj.lemmas for l in arr]\n",
    "    print('\\nFull Match: {}, Partial Match: {}, OutOf {}, NodeCount: {}, '.\\\n",
    "          format(full_match, partial_match, len(dcsLemmas), len(nodelist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Match: 2, Partial Match: 5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [WD_Node[C: 0, P: 0, pA @(-38) => pibata]],\n",
       " 1: [WD_Node[C: 1, P: 0, BAgavata @(31) => BAgavatam]],\n",
       " 2: [WD_Node[C: 2, P: 0, rasa @(69) => rasam]],\n",
       " 3: [WD_Node[C: 3, P: 0, Alaya @(71) => Alayam]]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCS ANALYZE\n",
      "---------------\n",
      "pibata BAgavataM rasam Alayam   \n",
      "[['pā'], ['bhāgavata'], ['rasa'], ['ālaya']]\n",
      "Lemmas: ['pA', 'BAgavata', 'rasa', 'Alaya']\n",
      "[['-38'], ['71'], ['69'], ['31']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(mst_nodes)\n",
    "SeeDCS(dcsObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22899\n",
      "SKT ANALYZE\n",
      "---------------\n",
      "pibata BAgavataM rasam Alayam   \n",
      "Analyzing  pibata\n",
      "0 :  pibata ['pA'] [{'verb': ['imp. [1] ac. pl. 2']}]\n",
      "0 :  pA ['pA'] []\n",
      "Analyzing  BAgavatam\n",
      "0 :  BAgavatam ['BAgavata'] [{'noun': ['acc. sg. m.', 'acc. sg. n.', 'nom. sg. n.']}]\n",
      "0 :  BAga ['BAga'] [{'compound': ['iic.']}]\n",
      "0 :  BAs ['BAs'] [{'compound': ['iic.']}]\n",
      "0 :  BAs ['BAs', 'BA'] [{'noun': ['nom. sg. f.']}, {'noun': ['acc. pl. f.', 'nom. pl. f.']}]\n",
      "0 :  BA ['BA'] [{'noun': ['nom. sg. f.']}]\n",
      "1 :  aga ['aga'] [{'compound': ['iic.']}]\n",
      "1 :  a ['a'] [{'compound': ['iic.']}]\n",
      "2 :  ga ['ga'] [{'compound': ['iic.']}]\n",
      "2 :  ga ['ga'] [{'compound': ['iic.']}]\n",
      "4 :  vatam ['van'] [{'noun': ['acc. sg. n.', 'nom. sg. n.', 'acc. sg. m.'], 'verbform': ['\"SKTMW227.html#H_van\"'], 'verb': [['pp.']]}]\n",
      "Analyzing  rasam\n",
      "0 :  rasam ['rasa'] [{'noun': ['acc. sg. m.']}]\n",
      "Analyzing  Alayam\n",
      "0 :  Alayam ['Alaya'] [{'noun': ['acc. sg. m.', 'acc. sg. n.', 'nom. sg. n.']}]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WD_Node[C: 0, P: 0, pA @(-38) => pibata],\n",
       " WD_Node[C: 1, P: 0, BAgavata @(31) => BAgavatam],\n",
       " WD_Node[C: 1, P: 0, BAgavata @(69) => BAgavatam],\n",
       " WD_Node[C: 1, P: 0, BAgavata @(71) => BAgavatam],\n",
       " WD_Node[C: 1, P: 0, BAga @(3) => BAga],\n",
       " WD_Node[C: 1, P: 0, BAs @(3) => BAs],\n",
       " WD_Node[C: 1, P: 0, BAs @(30) => BAs],\n",
       " WD_Node[C: 1, P: 0, BA @(80) => BAs],\n",
       " WD_Node[C: 1, P: 0, BA @(40) => BAs],\n",
       " WD_Node[C: 1, P: 0, BA @(30) => BA],\n",
       " WD_Node[C: 1, P: 1, aga @(3) => aga],\n",
       " WD_Node[C: 1, P: 1, a @(3) => a],\n",
       " WD_Node[C: 1, P: 2, ga @(3) => ga],\n",
       " WD_Node[C: 1, P: 2, ga @(3) => ga],\n",
       " WD_Node[C: 1, P: 4, van @(-190) => vatam],\n",
       " WD_Node[C: 1, P: 4, van @(31) => vatam],\n",
       " WD_Node[C: 1, P: 4, van @(69) => vatam],\n",
       " WD_Node[C: 1, P: 4, van @(71) => vatam],\n",
       " WD_Node[C: 2, P: 0, rasa @(69) => rasam],\n",
       " WD_Node[C: 3, P: 0, Alaya @(31) => Alayam],\n",
       " WD_Node[C: 3, P: 0, Alaya @(69) => Alayam],\n",
       " WD_Node[C: 3, P: 0, Alaya @(71) => Alayam]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 37171.p\n",
    "print(sentenceObj.sent_id)\n",
    "SeeSentence(sentenceObj)\n",
    "nodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        inf,  1.00000141,  0.99999987,  1.00002211,  1.00000411,\n",
       "         1.00000562,  1.0000008 ],\n",
       "       [ 1.00000494,         inf,  0.99999999,  1.00002792,  1.0000046 ,\n",
       "         1.00000294,  1.00000123],\n",
       "       [ 0.99999763,  0.99999997,         inf,  1.00000312,  1.00000036,\n",
       "         1.00000108,  1.00000045],\n",
       "       [ 1.00000537,  1.00000193,  1.00000004,         inf,  1.00000797,\n",
       "         1.00000671,  1.00000124],\n",
       "       [ 1.00000356,  1.00000114,  1.00000002,  1.00002847,         inf,\n",
       "                inf,  1.000001  ],\n",
       "       [ 1.00000655,  1.00000098,  1.00000007,  1.00003223,         inf,\n",
       "                inf,  1.00000144],\n",
       "       [ 1.00000488,  1.00000215,  1.00000016,  1.00003128,  1.0000071 ,\n",
       "         1.00000757,         inf]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat_correct, nodelist_correct, conflicts_Dict_correct, neuralnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
