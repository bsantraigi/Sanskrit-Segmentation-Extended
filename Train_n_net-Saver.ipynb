{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bUILT-iN pACKAGES\n",
    "import sys\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "## lAST sUMMER\n",
    "from romtoslp import *\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "import MatDB\n",
    "import time\n",
    "import bz2\n",
    "import zlib\n",
    "\n",
    "## lAST yEAR\n",
    "# from word_definite import *\n",
    "# from nnet import *\n",
    "# from heap_n_PrimMST import *\n",
    "# from word_definite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import winsound\n",
    "def playBeep():\n",
    "    for i in range(3):\n",
    "        winsound.Beep(2200, 300)\n",
    "        winsound.Beep(2600, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'), encoding=u'utf-8')\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'), encoding=u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_4k_1k = pickle.load(open('../SmallDataset_4K_1K.p', 'rb'))\n",
    "TrainFiles = dataset_4k_1k['TrainFiles']\n",
    "TestFiles = dataset_4k_1k['TestFiles']\n",
    "\n",
    "dataset_6k_3k = pickle.load(open('../SmallDataset_6K_3K.p', 'rb'))\n",
    "TrainFiles_2 = dataset_6k_3k['TrainFiles']\n",
    "TestFiles_2 = dataset_6k_3k['TestFiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matDB = MatDB.MatDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from MatDB import *\n",
    "import word_definite as WD\n",
    "from heap_n_PrimMST import *\n",
    "from nnet import *\n",
    "\"\"\"\n",
    "################################################################################################\n",
    "######################  CREATE SEVERAL DATA STRUCTURES FROM SENTENCE/DCS  ######################\n",
    "###########################  NODELIST, ADJACENCY LIST, GRAPH, HEAP #############################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "def GetTrainingKit(sentenceObj, dcsObj):\n",
    "    nodelist = GetNodes(sentenceObj)\n",
    "    \n",
    "    # Nodelist with only the correct_nodes\n",
    "    nodelist2 = GetNodes(sentenceObj)\n",
    "    nodelist2_to_correct_mapping = {}\n",
    "    nodelist_correct = []\n",
    "    search_key = 0\n",
    "    first_key = 0\n",
    "    for chunk_id in range(len(dcsObj.lemmas)):\n",
    "        while nodelist2[first_key].chunk_id != chunk_id:\n",
    "            first_key += 1\n",
    "        for j in range(len(dcsObj.lemmas[chunk_id])):\n",
    "            search_key = first_key\n",
    "            while (nodelist2[search_key].lemma != rom_slp(dcsObj.lemmas[chunk_id][j])) or (nodelist2[search_key].cng != dcsObj.cng[chunk_id][j]):\n",
    "                search_key += 1\n",
    "                if search_key >= len(nodelist2) or nodelist2[search_key].chunk_id > chunk_id:\n",
    "                    break\n",
    "    #         print((rom_slp(dcsObj.lemmas[chunk_id][j]), dcsObj.cng[chunk_id][j]))\n",
    "    #         print(nodelist[search_key])\n",
    "            nodelist2_to_correct_mapping[len(nodelist_correct)] = search_key\n",
    "            nodelist_correct.append(nodelist2[search_key])\n",
    "    return (nodelist, nodelist_correct, nodelist2_to_correct_mapping)\n",
    "    \n",
    "\n",
    "def GetGraph(nodelist, neuralnet):\n",
    "    if not neuralnet.outer_relu:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat, SigmoidGateOutput)\n",
    "    else:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nEURAL nET wILL bE sAVED hERE:  outputs/train_nnet_t508288462587.p\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  GET A FILENAME TO SAVE WEIGHTS  ################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "import time\n",
    "st = str(int((time.time() * 1e6) % 1e13))\n",
    "log_name = 'logs/train_nnet_t{}.out'.format(st)\n",
    "p_name = 'outputs/train_nnet_t{}.p'.format(st)\n",
    "print('nEURAL nET wILL bE sAVED hERE: ', p_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingStatus = defaultdict(lambda: bool(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.hidden_layer_size = 300\n",
    "        self._edge_vector_dim = WD._edge_vector_dim\n",
    "        # self._full_cnglist = list(WD.mat_cngCount_1D)\n",
    "        self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True)\n",
    "        self.history = defaultdict(lambda: list())\n",
    "\n",
    "    def SaveToMem(self, sentenceObj, dcsObj, _debug = True):\n",
    "        \n",
    "        \"\"\" Pre-Process DCS and SKT to get all Nodes etc. \"\"\"\n",
    "        try:\n",
    "            (nodelist, nodelist_correct, nodelist_to_correct_mapping) = GetTrainingKit(sentenceObj, dcsObj)\n",
    "        except IndexError as e:\n",
    "            # print('\\x1b[31mError with {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "            # print(e)\n",
    "            return\n",
    "        \n",
    "#         startT = time.time()\n",
    "        \"\"\" SKT FEATURE VECTOR MATRIX \"\"\"\n",
    "        conflicts_Dict_correct = Get_Conflicts(nodelist_correct)\n",
    "        featVMat_correct = Get_Feat_Vec_Matrix(nodelist_correct, conflicts_Dict_correct)\n",
    "        \n",
    "        \"\"\" SKT FEATURE VECTOR MATRIX \"\"\"\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "#         print('Nodelen: {}, Time taken to create: {}'.format(len(nodelist), time.time() - startT))\n",
    "        \n",
    "        with bz2.BZ2File('../NewData/skt_dcs_DS.bz2/' + sentenceObj.sent_id + '.ds.bz2', 'w') as f:\n",
    "            pickle.dump({\n",
    "                    'nodelist': nodelist,\n",
    "                    'nodelist_correct': nodelist_correct,\n",
    "                    'nodelist_to_correct_mapping': nodelist_to_correct_mapping,\n",
    "                    'conflicts_Dict_correct': conflicts_Dict_correct,\n",
    "                    'featVMat_correct': featVMat_correct,\n",
    "                    'conflicts_Dict': conflicts_Dict,\n",
    "                    'featVMat': featVMat\n",
    "                }, f)\n",
    "        \n",
    "#         with open('../NewData/skt_dcs_DS/' + sentenceObj.sent_id + '.ds', 'wb') as f:\n",
    "#             pickle.dump({\n",
    "#                     'nodelist': nodelist,\n",
    "#                     'nodelist_correct': nodelist_correct,\n",
    "#                     'nodelist_to_correct_mapping': nodelist_to_correct_mapping,\n",
    "#                     'conflicts_Dict_correct': conflicts_Dict_correct,\n",
    "#                     'featVMat_correct': featVMat_correct,\n",
    "#                     'conflicts_Dict': conflicts_Dict,\n",
    "#                     'featVMat': featVMat\n",
    "#                 }, f)\n",
    "        \n",
    "            \n",
    "#         startT = time.time()\n",
    "#         combined_loader = pickle.load(open('../NewData/skt_dcs_DS/' + sentenceObj.sent_id + '.ds', 'rb'))\n",
    "#         print('Time taken to load from pickle file: {}'.format(time.time() - startT))\n",
    "        \n",
    "#         startT = time.time()\n",
    "#         combined_loader = pickle.load(bz2.BZ2File('../NewData/skt_dcs_DS.bz2/' + sentenceObj.sent_id + '.ds.bz2', 'r'))\n",
    "#         print('Time taken to load from bz2 file: {}\\n'.format(time.time() - startT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trainer.neuralnet.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = None\n",
    "def InitModule(_matDB):\n",
    "    global WD, trainer\n",
    "    WD.word_definite_extInit(_matDB)\n",
    "    trainer = Trainer()\n",
    "InitModule(matDB)\n",
    "trainingStatus = defaultdict(lambda: bool(False))\n",
    "# trainer.Load('outputs/train_nnet_t427031523027.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  TRAIN FUNCTION  ################################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "def save_all_bz2(loaded_SKT, loaded_DCS, n_checkpt = 100):\n",
    "    file_counter = 0\n",
    "\n",
    "    for fn in loaded_DCS.keys():\n",
    "        if file_counter % n_checkpt == 0:\n",
    "            print(file_counter,' Checkpoint... ')\n",
    "            sys.stdout.flush() # Flush IO buffer \n",
    "        _ = trainer.SaveToMem(loaded_SKT[fn], loaded_DCS[fn])\n",
    "        file_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Checkpoint... \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-23d2a6766b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_all_bz2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_SKT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_DCS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_checkpt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-8e8ab1a40c14>\u001b[0m in \u001b[0;36msave_all_bz2\u001b[0;34m(loaded_SKT, loaded_DCS, n_checkpt)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_counter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' Checkpoint... '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Flush IO buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveToMem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_SKT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_DCS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mfile_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-45fbb1208e21>\u001b[0m in \u001b[0;36mSaveToMem\u001b[0;34m(self, sentenceObj, dcsObj, _debug)\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[1;34m'conflicts_Dict'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconflicts_Dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[1;34m'featVMat'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeatVMat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 }, f)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[1;31m#         with open('../NewData/skt_dcs_DS/' + sentenceObj.sent_id + '.ds', 'wb') as f:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\program files\\anaconda3\\lib\\bz2.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_can_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mcompressed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompressed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_all_bz2(loaded_SKT, loaded_DCS, n_checkpt=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Edge Features Time:  6.570472478866577\n",
      "Get Edge Features Time:  1.6002552509307861\n",
      "Get Edge Features Time:  0.5128428936004639\n",
      "Get Edge Features Time:  0.16260337829589844\n",
      "Get Edge Features Time:  0.30678248405456543\n",
      "Get Edge Features Time:  1.3092880249023438\n",
      "Get Edge Features Time:  0.8916211128234863\n",
      "Get Edge Features Time:  1.8754069805145264\n",
      "Get Edge Features Time:  1.14072585105896\n",
      "Get Edge Features Time:  0.27155017852783203\n"
     ]
    }
   ],
   "source": [
    "for fn in TestFiles[:10]:\n",
    "    _ = trainer.Test(loaded_SKT[fn], loaded_DCS[fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD_Node[C: 0, P: 0, Adi @(31) => Adi]\n",
      "WD_Node[C: 1, P: 0, ca @(2) => ca]\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "WD.word_definite_extInit(matDB)\n",
    "# node2 = WD.word_definite('tIkzRam', 'tIkzRa', 31, 0, 0)\n",
    "node1 = WD.word_definite('Adi', 'Adi', 31, 0, 0)\n",
    "node2 = WD.word_definite('ca', 'ca', 2, 0, 1)\n",
    "# node1 = WD.word_definite('koRam', 'koRa', 31, 0, 1)\n",
    "print(node1)\n",
    "print(node2)\n",
    "\n",
    "feats = WD.Get_Features(node1, node2)\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7031.25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.nbytes*900/(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implement Pooled Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Train_n_Save_NNet\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(loaded_SKT, loaded_DCS, n_testSet = -1, _testFiles = None, n_checkpt = 100):\n",
    "    total_lemma = 0;\n",
    "    correct_lemma = 0;\n",
    "\n",
    "    total_word = 0;\n",
    "    total_output_nodes = 0\n",
    "    correct_word = 0;\n",
    "    file_counter = 0\n",
    "    if _testFiles is None:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = TestFiles\n",
    "        else:\n",
    "            _testFiles = TestFiles[0:n_testSet]\n",
    "    else:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = _testFiles\n",
    "        else:\n",
    "            _testFiles = _testFiles[0:n_testSet]\n",
    "            \n",
    "    recalls = []\n",
    "    recalls_of_word = []\n",
    "    precisions = []\n",
    "    precisions_of_words = []\n",
    "    for fn in _testFiles:\n",
    "        if file_counter % n_checkpt == 0:\n",
    "            print(file_counter,' Checkpoint... ')\n",
    "            sys.stdout.flush() # Flush IO buffer \n",
    "        file_counter += 1\n",
    "        sentenceObj = loaded_SKT[fn]\n",
    "        dcsObj = loaded_DCS[fn]        \n",
    "        try:\n",
    "            (word_match, lemma_match, n_dcsWords, n_output_nodes) = trainer.Test(sentenceObj, dcsObj)\n",
    "            \n",
    "            recalls.append(lemma_match/n_dcsWords)\n",
    "            recalls_of_word.append(word_match/n_dcsWords)\n",
    "            \n",
    "            precisions.append(lemma_match/n_output_nodes)\n",
    "            precisions_of_words.append(word_match/n_output_nodes)\n",
    "            \n",
    "            total_lemma += n_dcsWords\n",
    "            total_word += n_dcsWords\n",
    "            \n",
    "            total_output_nodes += n_output_nodes            \n",
    "            \n",
    "            correct_lemma += lemma_match\n",
    "            correct_word += word_match\n",
    "        except (IndexError, KeyError) as e:\n",
    "            print('Failed!')        \n",
    "\n",
    "    print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "    print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "    print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "    print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))\n",
    "    \n",
    "    return (recalls, recalls_of_word, precisions, precisions_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = test(loaded_SKT, loaded_DCS, n_testSet = 3000, _testFiles = TestFiles_2, n_checkpt = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######   THE END\n",
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multiprocessing Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import os\n",
    "import poolTest\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'), encoding=u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done:  9608\n",
      "Done:  5124\n",
      "Done:  13260\n",
      "Done:  7332\n"
     ]
    }
   ],
   "source": [
    "# USING PROCESS OBJECT\n",
    "glob_arr = list(range(100))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "proc_count = 4\n",
    "\n",
    "procs = [None]*proc_count\n",
    "\n",
    "for i in range(proc_count):\n",
    "    procs[i] = Process(target = poolTest.f, args = (i, ))\n",
    "for proc in procs:\n",
    "    proc.start()\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "    print('Done: ', proc.pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sktObjList = list(loaded_SKT.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('14741', 8),\n",
       " ('21171', 9),\n",
       " ('44699', 5),\n",
       " ('286767', 3),\n",
       " ('268297', 3),\n",
       " ('253892', 1),\n",
       " ('42110', 5),\n",
       " ('138854', 3),\n",
       " ('42273', 6),\n",
       " ('320783', 4),\n",
       " ('273356', 5),\n",
       " ('33423', 9),\n",
       " ('40411', 6),\n",
       " ('343323', 4),\n",
       " ('249124', 5),\n",
       " ('249672', 6),\n",
       " ('214118', 6),\n",
       " ('39926', 1),\n",
       " ('437631', 2),\n",
       " ('289985', 5)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USING POOL\n",
    "p = Pool(4)\n",
    "chunk_counts = p.map(poolTest.counter, sktObjList[:20])\n",
    "p.close()\n",
    "\n",
    "chunk_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
