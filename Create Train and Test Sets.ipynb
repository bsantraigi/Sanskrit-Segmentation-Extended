{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "\n",
    "from MatDB import *\n",
    "import word_definite as WD\n",
    "from heap_n_PrimMST import *\n",
    "from nnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'), encoding=u'utf-8')\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'), encoding=u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetTrainingKit(sentenceObj, dcsObj):\n",
    "    nodelist = GetNodes(sentenceObj)\n",
    "    \n",
    "    # Nodelist with only the correct_nodes\n",
    "    nodelist2 = GetNodes(sentenceObj)\n",
    "    nodelist2_to_correct_mapping = {}\n",
    "    nodelist_correct = []\n",
    "    search_key = 0\n",
    "    first_key = 0\n",
    "    for chunk_id in range(len(dcsObj.lemmas)):\n",
    "        while nodelist2[first_key].chunk_id != chunk_id:\n",
    "            first_key += 1\n",
    "        for j in range(len(dcsObj.lemmas[chunk_id])):\n",
    "            search_key = first_key\n",
    "            while (nodelist2[search_key].lemma != rom_slp(dcsObj.lemmas[chunk_id][j])) or (nodelist2[search_key].cng != dcsObj.cng[chunk_id][j]):\n",
    "                search_key += 1\n",
    "                if search_key >= len(nodelist2) or nodelist2[search_key].chunk_id > chunk_id:\n",
    "                    break\n",
    "    #         print((rom_slp(dcsObj.lemmas[chunk_id][j]), dcsObj.cng[chunk_id][j]))\n",
    "    #         print(nodelist[search_key])\n",
    "            nodelist2_to_correct_mapping[len(nodelist_correct)] = search_key\n",
    "            nodelist_correct.append(nodelist2[search_key])\n",
    "    return (nodelist, nodelist_correct, nodelist2_to_correct_mapping)\n",
    "    \n",
    "\n",
    "def GetGraph(nodelist, neuralnet):\n",
    "    if not neuralnet.outer_relu:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat, SigmoidGateOutput)\n",
    "    else:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat)\n",
    "def Test(sentenceObj, dcsObj):\n",
    "    try:\n",
    "        (_, _, _) = GetTrainingKit(sentenceObj, dcsObj)\n",
    "        return True\n",
    "        # nodelist = GetNodes(sentenceObj)\n",
    "    except IndexError:\n",
    "        # print('\\x1b[31mError with {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(loaded_SKT, loaded_DCS, trainSize = 100, testSize = 100):\n",
    "    test_set = set()\n",
    "    train_set = set()\n",
    "    \n",
    "    _skt_file_list = list(loaded_SKT.keys())\n",
    "    perm = np.random.permutation(len(loaded_SKT))\n",
    "\n",
    "    print('Get Train Permutation')\n",
    "\n",
    "    fn = 0\n",
    "    while len(train_set) < trainSize:\n",
    "        sentenceObj = loaded_SKT[_skt_file_list[fn]]\n",
    "        dcsObj = loaded_DCS[_skt_file_list[fn]]\n",
    "        if Test(sentenceObj, dcsObj):\n",
    "            train_set.add(_skt_file_list[fn])\n",
    "            \n",
    "        fn += 1\n",
    "    \n",
    "    print('Get Test Permutation')\n",
    "\n",
    "    # Run few times on same set of files   \n",
    "    while len(test_set) < testSize:\n",
    "        sentenceObj = loaded_SKT[_skt_file_list[fn]]\n",
    "        dcsObj = loaded_DCS[_skt_file_list[fn]]\n",
    "        if Test(sentenceObj, dcsObj):\n",
    "            test_set.add(_skt_file_list[fn])\n",
    "        fn += 1\n",
    "    print('Train set size: ', len(train_set))\n",
    "    print('Test set size: ', len(test_set))\n",
    "    if len(train_set) == len(train_set - test_set):\n",
    "        print('Both set\\'s are disjoint...')\n",
    "    else:\n",
    "        print('Both set\\'s are not disjoint...')\n",
    "    return (list(train_set), list(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Train Permutation\n",
      "Get Test Permutation\n",
      "Train set size:  6000\n",
      "Test set size:  3000\n",
      "Both set's are disjoint...\n"
     ]
    }
   ],
   "source": [
    "TrainFiles, TestFiles = main(loaded_SKT, loaded_DCS, trainSize=6000, testSize=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump({'TestFiles': TestFiles, 'TrainFiles': TrainFiles}, open('../SmallDataset_6K_3K.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Split folder of bz2 compressed files into test and train data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bUILT-iN pACKAGES\n",
    "import sys, os, time, bz2, zlib, pickle, math, json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "## lAST sUMMER\n",
    "from romtoslp import *\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "import MatDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_filelist = os.listdir('../NewData/skt_dcs_DS.bz2_10K/')\n",
    "all_filelist.sort()\n",
    "dsbz2_filelist = []\n",
    "for fn in all_filelist:\n",
    "    if '.ds.bz2' in fn:\n",
    "        dsbz2_filelist.append(fn)\n",
    "        \n",
    "# dsbz2_filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_perm = np.random.permutation(len(dsbz2_filelist))\n",
    "\n",
    "n_trainset = int(file_perm.shape[0]/2)\n",
    "# n_trainset = 400\n",
    "\n",
    "file_perm_train = file_perm[:n_trainset].astype(np.int32)\n",
    "file_perm_test = file_perm[n_trainset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  4046\n",
      "Test set size:  4047\n",
      "Intersection:  0\n"
     ]
    }
   ],
   "source": [
    "Train_bz2Files = []\n",
    "Test_bz2Files = []\n",
    "for x in file_perm_train:\n",
    "    Train_bz2Files.append(dsbz2_filelist[x])\n",
    "for x in file_perm_test:\n",
    "    Test_bz2Files.append(dsbz2_filelist[x])\n",
    "print('Training set size: ', len(Train_bz2Files))\n",
    "print('Test set size: ', len(Test_bz2Files))\n",
    "print('Intersection: ', len(set(Train_bz2Files) - set(Test_bz2Files)) - len(Train_bz2Files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump({\n",
    "        'TrainFiles': Train_bz2Files,\n",
    "        'TestFiles': Test_bz2Files\n",
    "    }, open('../bz2Dataset_10K.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Read Held out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_filelist = []\n",
    "with open('inputs/Baseline4_advSample.csv') as f:\n",
    "    baseline_reader = csv.reader(f)\n",
    "    for line in baseline_reader:\n",
    "        baseline_filelist.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heldout_names = [f.replace('.p', '.ds.bz2') for f in baseline_filelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "for f in heldout_names:\n",
    "    c += os.path.isfile('../NewData/skt_dcs_DS.bz2_10K/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
