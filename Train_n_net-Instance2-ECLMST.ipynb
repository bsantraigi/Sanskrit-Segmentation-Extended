{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bUILT-iN pACKAGES\n",
    "import sys, os, time, bz2, zlib, pickle, math, json, csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "## lAST sUMMER\n",
    "from romtoslp import *\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "import MatDB\n",
    "from heap_n_PrimMST import *\n",
    "from ECL_MST import *\n",
    "\n",
    "## lAST yEAR\n",
    "# from word_definite import *\n",
    "from nnet import *\n",
    "# from heap_n_PrimMST import *\n",
    "# from word_definite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import winsound\n",
    "def playBeep():\n",
    "    for i in range(3):\n",
    "        winsound.Beep(2200, 300)\n",
    "        winsound.Beep(2600, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'), encoding=u'utf-8')\n",
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'), encoding=u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset_4k_1k = pickle.load(open('../SmallDataset_4K_1K.p', 'rb'))\n",
    "# TrainFiles = dataset_4k_1k['TrainFiles']\n",
    "# TestFiles = dataset_4k_1k['TestFiles']\n",
    "\n",
    "# dataset_6k_3k = pickle.load(open('../SmallDataset_6K_3K.p', 'rb'))\n",
    "# TrainFiles_2 = dataset_6k_3k['TrainFiles']\n",
    "# TestFiles_2 = dataset_6k_3k['TestFiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matDB = MatDB.MatDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957 files will not be used for training\n",
      "Size of training set: 8219\n"
     ]
    }
   ],
   "source": [
    "# Create Training File List\n",
    "excluded_files = []\n",
    "with open('inputs/Baseline4_advSample.csv', 'r') as f_handle:\n",
    "    opener = csv.reader(f_handle)\n",
    "    for line in opener:\n",
    "        excluded_files.append(line[1].replace('.p', '.ds.bz2'))\n",
    "\n",
    "bz2_input_folder = '../NewData/skt_dcs_DS.bz2_1L_bigram_10K_d2K/'\n",
    "# bz2_input_folder = '/home/rs/15CS91R05/vishnu/Data/skt_dcs_DS.bz2_compat_10k_check//'\n",
    "all_files = []\n",
    "skipped = 0\n",
    "for f in os.listdir(bz2_input_folder):\n",
    "    if '.ds.bz2' in f:\n",
    "        if f in excluded_files:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        all_files.append(f)\n",
    "\n",
    "print(skipped, 'files will not be used for training')\n",
    "print('Size of training set:', len(all_files))\n",
    "\n",
    "TrainFiles = all_files        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dsbz2(filename):\n",
    "    with bz2.BZ2File(filename, 'r') as f:\n",
    "        loader = pickle.load(f)\n",
    "    \n",
    "    conflicts_Dict_correct = loader['conflicts_Dict_correct']\n",
    "    nodelist_to_correct_mapping = loader['nodelist_to_correct_mapping']\n",
    "    nodelist_correct = loader['nodelist_correct']\n",
    "    featVMat_correct = loader['featVMat_correct']\n",
    "    featVMat = loader['featVMat']\n",
    "    conflicts_Dict = loader['conflicts_Dict']\n",
    "    nodelist = loader['nodelist']\n",
    "    \n",
    "    return (nodelist_correct, conflicts_Dict_correct, featVMat_correct, nodelist_to_correct_mapping,\\\n",
    "            nodelist, conflicts_Dict, featVMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from MatDB import *\n",
    "import word_definite as WD\n",
    "from heap_n_PrimMST import *\n",
    "from nnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "######################  CREATE SEVERAL DATA STRUCTURES FROM SENTENCE/DCS  ######################\n",
    "###########################  NODELIST, ADJACENCY LIST, GRAPH, HEAP #############################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "def GetTrainingKit(sentenceObj, dcsObj):\n",
    "    nodelist = GetNodes(sentenceObj)\n",
    "    \n",
    "    # Nodelist with only the correct_nodes\n",
    "    nodelist2 = GetNodes(sentenceObj)\n",
    "    nodelist2_to_correct_mapping = {}\n",
    "    nodelist_correct = []\n",
    "    search_key = 0\n",
    "    first_key = 0\n",
    "    for chunk_id in range(len(dcsObj.lemmas)):\n",
    "        while nodelist2[first_key].chunk_id != chunk_id:\n",
    "            first_key += 1\n",
    "        for j in range(len(dcsObj.lemmas[chunk_id])):\n",
    "            search_key = first_key\n",
    "            while (nodelist2[search_key].lemma != rom_slp(dcsObj.lemmas[chunk_id][j])) or (nodelist2[search_key].cng != dcsObj.cng[chunk_id][j]):\n",
    "                search_key += 1\n",
    "                if search_key >= len(nodelist2) or nodelist2[search_key].chunk_id > chunk_id:\n",
    "                    break\n",
    "    #         print((rom_slp(dcsObj.lemmas[chunk_id][j]), dcsObj.cng[chunk_id][j]))\n",
    "    #         print(nodelist[search_key])\n",
    "            nodelist2_to_correct_mapping[len(nodelist_correct)] = search_key\n",
    "            nodelist_correct.append(nodelist2[search_key])\n",
    "    return (nodelist, nodelist_correct, nodelist2_to_correct_mapping)\n",
    "    \n",
    "\n",
    "def GetGraph(nodelist, neuralnet):\n",
    "    if not neuralnet.outer_relu:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat, SigmoidGateOutput)\n",
    "    else:\n",
    "        conflicts_Dict = Get_Conflicts(nodelist)\n",
    "\n",
    "        featVMat = Get_Feat_Vec_Matrix(nodelist, conflicts_Dict)\n",
    "\n",
    "        WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        return (conflicts_Dict, featVMat, WScalarMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingStatus = defaultdict(lambda: bool(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  TRAIN FUNCTION  ################################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "def train_generator(loaded_SKT, loaded_DCS, bz2_input_folder, n_trainset = -1, iterationPerBatch = 10, filePerBatch = 20, _debug = True):\n",
    "    # Train\n",
    "    if n_trainset == -1:\n",
    "        n_trainset = len(TrainFiles)\n",
    "        totalBatchToTrain = math.ceil(n_trainset/filePerBatch)\n",
    "    else:\n",
    "        totalBatchToTrain = math.ceil(n_trainset/filePerBatch)\n",
    "    \n",
    "\n",
    "    for iterout in range(totalBatchToTrain):\n",
    "        # Add timer\n",
    "        startT = time.time()\n",
    "\n",
    "        # Change current batch\n",
    "        trainer.Save(p_name)\n",
    "        print('Batch: ', iterout)\n",
    "        files_for_batch = TrainFiles[iterout*filePerBatch:(iterout + 1)*filePerBatch]\n",
    "        print(files_for_batch)\n",
    "        # trainer.Load('outputs/neuralnet_trained.p')\n",
    "        try:\n",
    "            # Run few times on same set of files\n",
    "            for iterin in range(iterationPerBatch):\n",
    "                print('ITERATION IN', iterin)        \n",
    "                for fn in files_for_batch:\n",
    "                    trainFileName = fn.replace('.ds.bz2', '.p2')\n",
    "                    sentenceObj = loaded_SKT[trainFileName]\n",
    "                    dcsObj = loaded_DCS[trainFileName]\n",
    "                    if trainingStatus[sentenceObj.sent_id]:\n",
    "                        continue\n",
    "                    # trainer.Save('outputs/saved_trainer.p')\n",
    "                    try:\n",
    "                        trainer.Train(sentenceObj, dcsObj, bz2_input_folder, _debug)\n",
    "                    except (IndexError, KeyError) as e:\n",
    "                        print('\\x1b[31mFailed: {} \\x1b[0m'.format(sentenceObj.sent_id))\n",
    "                sys.stdout.flush() # Flush IO buffer \n",
    "            finishT = time.time()\n",
    "            print('Avg. time taken by 1 file(1 iteration): {:.3f}'.format((finishT - startT)/(iterationPerBatch*filePerBatch)))\n",
    "        except KeyboardInterrupt:\n",
    "            print('Training paused')\n",
    "            trainer.Save(p_name)\n",
    "            yield 'Hi'\n",
    "    trainer.Save(p_name)\n",
    "    \n",
    "    sys.stdout.flush() # Flush IO buffer \n",
    "                \n",
    "def test(loaded_SKT, loaded_DCS, n_testSet = -1, _testFiles = None, n_checkpt = 100):\n",
    "    total_lemma = 0;\n",
    "    correct_lemma = 0;\n",
    "\n",
    "    total_word = 0;\n",
    "    total_output_nodes = 0\n",
    "    correct_word = 0;\n",
    "    file_counter = 0\n",
    "    if _testFiles is None:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = TestFiles\n",
    "        else:\n",
    "            _testFiles = TestFiles[0:n_testSet]\n",
    "    else:\n",
    "        if n_testSet == -1:\n",
    "            _testFiles = _testFiles\n",
    "        else:\n",
    "            _testFiles = _testFiles[0:n_testSet]\n",
    "            \n",
    "    recalls = []\n",
    "    recalls_of_word = []\n",
    "    precisions = []\n",
    "    precisions_of_words = []\n",
    "    for fn in _testFiles:\n",
    "        if file_counter % n_checkpt == 0:\n",
    "            print(file_counter,' Checkpoint... ')\n",
    "            if file_counter > 0:\n",
    "                print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "                print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "                print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "                print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))\n",
    "            sys.stdout.flush() # Flush IO buffer \n",
    "        \n",
    "        file_counter += 1\n",
    "        \n",
    "        testFileName = fn.replace('.ds.bz2', '.p2')\n",
    "        sentenceObj = loaded_SKT[testFileName]\n",
    "        dcsObj = loaded_DCS[testFileName]    \n",
    "        \n",
    "        try:\n",
    "            (word_match, lemma_match, n_dcsWords, n_output_nodes) = trainer.Test(sentenceObj, dcsObj)\n",
    "            \n",
    "            recalls.append(lemma_match/n_dcsWords)\n",
    "            recalls_of_word.append(word_match/n_dcsWords)\n",
    "            \n",
    "            precisions.append(lemma_match/n_output_nodes)\n",
    "            precisions_of_words.append(word_match/n_output_nodes)\n",
    "            \n",
    "            total_lemma += n_dcsWords\n",
    "            total_word += n_dcsWords\n",
    "            \n",
    "            total_output_nodes += n_output_nodes            \n",
    "            \n",
    "            correct_lemma += lemma_match\n",
    "            correct_word += word_match\n",
    "        except (IndexError, KeyError) as e:\n",
    "            print('Failed!')        \n",
    "\n",
    "    print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "    print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "    print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "    print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))\n",
    "    \n",
    "    return (recalls, recalls_of_word, precisions, precisions_of_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOW-OLD FUNCTION\n",
    "# def GetLoss(_mst_adj_graph, _mask_de_correct_edges, _negLogLikelies):\n",
    "#     _negLogLikelies = _negLogLikelies.copy()\n",
    "#     _negLogLikelies[~_mst_adj_graph] = 0\n",
    "#     _negLogLikelies[~_mask_de_correct_edges] *= -1 # BAKA!!! Check before you try to fix this again\n",
    "#     return np.sum(_negLogLikelies)    \n",
    "\n",
    "# NEW FUNCTION\n",
    "def GetLoss(_mst_adj_graph, _mask_de_correct_edges, _WScalarMat):\n",
    "    _WScalarMat = _WScalarMat.copy()\n",
    "    _WScalarMat[_mst_adj_graph&(~_mask_de_correct_edges)] *= -1 # BAKA!!! Check before you try to fix this again\n",
    "    _WScalarMat[~_mst_adj_graph] = 0\n",
    "    return np.sum(_WScalarMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nEURAL nET wILL bE sAVED hERE:  outputs/train_nnet_t1568073772239.p\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "##############################  GET A FILENAME TO SAVE WEIGHTS  ################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "import time\n",
    "st = str(int((time.time() * 1e6) % 1e13))\n",
    "log_name = 'logs/train_nnet_t{}.out'.format(st)\n",
    "p_name = 'outputs/train_nnet_t{}.p'.format(st)\n",
    "print('nEURAL nET wILL bE sAVED hERE: ', p_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, modelFile = None):\n",
    "        if modelFile is None:\n",
    "            self.hidden_layer_size = 1800\n",
    "            self._edge_vector_dim = 2000\n",
    "            # self._edge_vector_dim = WD._edge_vector_dim\n",
    "            # self._full_cnglist = list(WD.mat_cngCount_1D)\n",
    "            \n",
    "            self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True)\n",
    "            self.history = defaultdict(lambda: list())\n",
    "        else:\n",
    "            loader = pickle.load(open(filename, 'rb'))\n",
    "            \n",
    "            self.neuralnet.hidden_layer_size = loader['n']\n",
    "            self.neuralnet._edge_vector_dim = loader['d']\n",
    "\n",
    "            self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size, outer_relu=True)\n",
    "\n",
    "            self.neuralnet.U = loader['U']\n",
    "            self.neuralnet.W = loader['W']\n",
    "            self.neuralnet.B1 = loader['B1']\n",
    "            self.neuralnet.B2 = loader['B2']\n",
    "            \n",
    "            self.history = defaultdict(lambda: list())\n",
    "            \n",
    "        # SET LEARNING RATES\n",
    "        self.neuralnet.etaW = 3e-4\n",
    "        self.neuralnet.etaB1 = 1e-4\n",
    "        \n",
    "        self.neuralnet.etaU = 1e-4\n",
    "        self.neuralnet.etaB2 = 1e-4\n",
    "            \n",
    "    def Reset(self):\n",
    "        self.neuralnet = NN(self._edge_vector_dim, self.hidden_layer_size)\n",
    "        self.history = defaultdict(lambda: list())\n",
    "        \n",
    "    def Save(self, filename):\n",
    "        print('Weights Saved: ', p_name)\n",
    "        pickle.dump({\n",
    "                'U': self.neuralnet.U,\n",
    "                'W': self.neuralnet.W,\n",
    "                'n': self.neuralnet.n,\n",
    "                'd': self.neuralnet.d,\n",
    "                'B1': self.neuralnet.B1,\n",
    "                'B2': self.neuralnet.B2\n",
    "            }, open(p_name, 'wb'))\n",
    "        return\n",
    "        \n",
    "    \n",
    "    def Load(self, filename):\n",
    "        loader = pickle.load(open(filename, 'rb'))\n",
    "        self.neuralnet.U = loader['U']\n",
    "        self.neuralnet.W = loader['W']\n",
    "        self.neuralnet.B1 = loader['B1']\n",
    "        self.neuralnet.B2 = loader['B2']\n",
    "        self.neuralnet.hidden_layer_size = loader['n']\n",
    "        self.neuralnet._edge_vector_dim = loader['d']\n",
    "        \n",
    "    def Test(self, sentenceObj, dcsObj, dsbz2_name):\n",
    "        neuralnet = self.neuralnet\n",
    "        minScore = np.inf\n",
    "        minMst = None\n",
    "        \n",
    "        # dsbz2_name = sentenceObj.sent_id + '.ds.bz2'\n",
    "        (nodelist_correct, conflicts_Dict_correct, featVMat_correct, nodelist_to_correct_mapping,\\\n",
    "            nodelist, conflicts_Dict, featVMat) = open_dsbz2(dsbz2_name)\n",
    "        \n",
    "        # if len(nodelist) > 50:\n",
    "        #     return None\n",
    "\n",
    "        if not self.neuralnet.outer_relu:\n",
    "            (WScalarMat, SigmoidGateOutput) = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        else:\n",
    "            WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, neuralnet)\n",
    "        \n",
    "        # print('NeuralNet Time: ', time.time() - startT)\n",
    "        # startT = time.time()\n",
    "        \n",
    "        # Get all MST\n",
    "        for source in range(len(nodelist)):\n",
    "            #(mst_nodes, mst_adj_graph, _) = MST(nodelist, WScalarMat, conflicts_Dict, source)\n",
    "            (mst_nodes_bool,mst_nodes,mst_adj_graph)=MST_ECL(nodelist,WScalarMat,conflicts_Dict,source)\n",
    "            # print('.', end = '')\n",
    "            score = GetMSTWeight(mst_adj_graph, WScalarMat)\n",
    "            if(score < minScore):\n",
    "                minScore = score\n",
    "                minMst = mst_nodes\n",
    "        dcsLemmas = [[rom_slp(l) for l in arr]for arr in dcsObj.lemmas]\n",
    "        word_match = 0\n",
    "        lemma_match = 0\n",
    "        n_output_nodes = 0\n",
    "        for chunk_id, wdSplit in minMst.items():\n",
    "            for wd in wdSplit:\n",
    "                n_output_nodes += 1\n",
    "                # Match lemma\n",
    "                search_result = [i for i, j in enumerate(dcsLemmas[chunk_id]) if j == wd.lemma]\n",
    "                if len(search_result) > 0:\n",
    "                    lemma_match += 1\n",
    "                # Match CNG\n",
    "                for i in search_result:\n",
    "                    if(dcsObj.cng[chunk_id][i] == str(wd.cng)):\n",
    "                        word_match += 1\n",
    "                        # print(wd.lemma, wd.cng)\n",
    "                        break\n",
    "        dcsLemmas = [l for arr in dcsObj.lemmas for l in arr]\n",
    "        \n",
    "        # print('All MST Time: ', time.time() - startT)\n",
    "        # print('Node Count: ', len(nodelist))\n",
    "#         print('\\nFull Match: {}, Partial Match: {}, OutOf {}, NodeCount: {}, '.\\\n",
    "#               format(word_match, lemma_match, len(dcsLemmas), len(nodelist)))\n",
    "        return (word_match, lemma_match, len(dcsLemmas), n_output_nodes)\n",
    "    \n",
    "    def Train(self, sentenceObj, dcsObj, bz2_input_folder, _debug = True):\n",
    "        # Hyperparameter for hinge loss: m\n",
    "        m_hinge_param = 14\n",
    "        \n",
    "        dsbz2_name = sentenceObj.sent_id + '.ds.bz2'\n",
    "        (nodelist_correct, conflicts_Dict_correct, featVMat_correct, nodelist_to_correct_mapping,\\\n",
    "            nodelist, conflicts_Dict, featVMat) = open_dsbz2(bz2_input_folder + dsbz2_name)\n",
    "        # Train for large graphs separately\n",
    "#         if len(nodelist) < 40:\n",
    "#             return\n",
    "        \n",
    "        \"\"\" FORM MAXIMUM(ENERGY) SPANNING TREE OF THE GOLDEN GRAPH : WORST GOLD STRUCTURE \"\"\"\n",
    "        WScalarMat_correct = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat_correct, nodelist_correct,\\\n",
    "                                                                      conflicts_Dict_correct, self.neuralnet)\n",
    "        source = 0\n",
    "        \"\"\" Find the max spanning tree : negative Weight matrix passed \"\"\"\n",
    "#         (max_st_gold_ndict, max_st_adj_gold_small, _) =\\\n",
    "#             MST(nodelist_correct, -WScalarMat_correct, conflicts_Dict_correct, source)\n",
    "        (max_st_gold_ndict, max_st_adj_gold_small, _) =\\\n",
    "            MST(nodelist_correct, -WScalarMat_correct, conflicts_Dict_correct, source)\n",
    "        energy_gold_max_ST = np.sum(WScalarMat_correct[max_st_adj_gold_small])\n",
    "        \n",
    "        \"\"\" Convert correct spanning tree graph adj matrix to full marix dimensions \"\"\"\n",
    "        \"\"\" Create full-size adjacency matrix for correct_mst_small \"\"\"\n",
    "        nodelen = len(nodelist)\n",
    "        max_st_adj_gold = np.ndarray((nodelen, nodelen), np.bool)*False # T_STAR\n",
    "        for i in range(max_st_adj_gold_small.shape[0]):\n",
    "            for j in range(max_st_adj_gold_small.shape[1]):\n",
    "                max_st_adj_gold[nodelist_to_correct_mapping[i], nodelist_to_correct_mapping[j]] =\\\n",
    "                    max_st_adj_gold_small[i, j]\n",
    "        \n",
    "        \"\"\" Delta(Margin) Function : MASK FOR WHICH NODES IN NODELIST BELONG TO DCS \"\"\"\n",
    "        gold_nodes_mask = np.array([False]*len(nodelist))\n",
    "        gold_nodes_mask[list(nodelist_to_correct_mapping.values())] = True\n",
    "        margin_f = lambda nodes_mask: np.sum(nodes_mask&(~gold_nodes_mask))**2\n",
    "        \n",
    "        \"\"\" FOR ALL POSSIBLE MST FROM THE COMPLETE GRAPH \"\"\"\n",
    "        WScalarMat = Get_W_Scalar_Matrix_from_FeatVect_Matrix(featVMat, nodelist, conflicts_Dict, self.neuralnet)\n",
    "\n",
    "        \"\"\" For each node - Find MST with that source\"\"\"\n",
    "        min_STx = None # Min Energy spanning tree with worst margin with gold_STx\n",
    "        min_marginalized_energy = np.inf\n",
    "        \n",
    "        # Generate random set of nodes from which mSTs are to be considered\n",
    "        n_nodes = len(nodelist)\n",
    "        selection_prob = 0.4\n",
    "        select_flag = np.random.rand(n_nodes) < selection_prob\n",
    "        # Fix if all zeros\n",
    "        if np.sum(select_flag) == 0:\n",
    "            select_flag[np.random.randint(n_nodes)] = 1\n",
    "        \n",
    "        for source in range(len(nodelist)):\n",
    "            (mst_nodes, mst_adj_graph, mst_nodes_bool) = MST(nodelist, WScalarMat, conflicts_Dict, source) # T_X\n",
    "            # print('.', end = '')\n",
    "           \n",
    "            marginalized_en = np.sum(WScalarMat[mst_adj_graph]) - margin_f(mst_nodes_bool)\n",
    "            # Minimum marginalized spanning tree : Randomization applied\n",
    "            # if marginalized_en < min_marginalized_energy and select_flag[source]:\n",
    "            if marginalized_en < min_marginalized_energy:\n",
    "                min_marginalized_energy = marginalized_en\n",
    "                min_STx = mst_adj_graph\n",
    "            # Energy diff should all be negative\n",
    "            if _debug:\n",
    "                print('Source: [{}], Node_Diff:{}, Max_Gold_En: {:.3f}, Energy: {:.3f}'.\\\n",
    "                      format(source, np.sum((~gold_nodes_mask)&mst_nodes_bool), energy_gold_max_ST,  np.sum(WScalarMat[mst_adj_graph])))\n",
    "\n",
    "        \"\"\" Gradient Descent \"\"\"\n",
    "        # FOR MOST OFFENdING Y\n",
    "        doBpp = False\n",
    "        \n",
    "        Total_Loss = energy_gold_max_ST - min_marginalized_energy\n",
    "        if Total_Loss > 0:\n",
    "            dLdOut = np.zeros_like(WScalarMat)\n",
    "            dLdOut[max_st_adj_gold&(~min_STx)] = 1\n",
    "            dLdOut[(~max_st_adj_gold)&min_STx] = -1\n",
    "            if _debug:\n",
    "                print('{}. '.format(sentenceObj.sent_id), end = '')\n",
    "            self.neuralnet.Back_Prop(dLdOut, len(nodelist), featVMat, _debug)\n",
    "        else:\n",
    "            trainingStatus[sentenceObj.sent_id] = True\n",
    "        if _debug:\n",
    "            print(\"\\nFileKey: %s, Loss: %6.3f\" % (sentenceObj.sent_id, Total_Loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainer.neuralnet.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = None\n",
    "def InitModule():\n",
    "    global trainer\n",
    "    trainer = Trainer()\n",
    "InitModule()\n",
    "trainingStatus = defaultdict(lambda: bool(False))\n",
    "# trainer.Load('outputs/train_nnet_t526539685574.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "\"\"\"\n",
    "# if __name__ == '__main__':\n",
    "# loaded_SKT = pickle.load(open('../Simultaneous_CompatSKT_10K.p', 'rb'))\n",
    "# loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'))\n",
    "# main(loaded_SKT, loaded_DCS)\n",
    "np.set_printoptions(suppress=False)\n",
    "\n",
    "fn = TrainFiles[5].replace('.ds.bz2', '.p2')\n",
    "print(fn)\n",
    "for _ in range(1):\n",
    "    trainer.Train(loaded_SKT[fn], loaded_DCS[fn], bz2_input_folder, _debug = True)\n",
    "\n",
    "# # fn = TestFiles[1].replace('.ds.bz2', '.p2')\n",
    "# trainer.Test(loaded_SKT[fn], loaded_DCS[fn])\n",
    "# print (\"Not Implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_generator(loaded_SKT, loaded_DCS, bz2_input_folder, n_trainset = -1, filePerBatch = 10, iterationPerBatch = 5, _debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  0\n",
      "['10250.ds.bz2', '428839.ds.bz2', '20035.ds.bz2', '32130.ds.bz2', '418916.ds.bz2', '154658.ds.bz2', '65093.ds.bz2', '266079.ds.bz2', '429657.ds.bz2', '389979.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 0.966\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  1\n",
      "['365964.ds.bz2', '29623.ds.bz2', '162846.ds.bz2', '296029.ds.bz2', '338298.ds.bz2', '187738.ds.bz2', '171533.ds.bz2', '310155.ds.bz2', '22207.ds.bz2', '12964.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.539\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  2\n",
      "['231593.ds.bz2', '399906.ds.bz2', '194861.ds.bz2', '130103.ds.bz2', '433682.ds.bz2', '164353.ds.bz2', '35530.ds.bz2', '240430.ds.bz2', '37993.ds.bz2', '57802.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.429\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  3\n",
      "['168593.ds.bz2', '34892.ds.bz2', '323508.ds.bz2', '36688.ds.bz2', '14516.ds.bz2', '137350.ds.bz2', '43470.ds.bz2', '41634.ds.bz2', '32517.ds.bz2', '181050.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.400\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  4\n",
      "['120998.ds.bz2', '321200.ds.bz2', '29793.ds.bz2', '124631.ds.bz2', '158557.ds.bz2', '438554.ds.bz2', '164299.ds.bz2', '99737.ds.bz2', '436332.ds.bz2', '155208.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 3.429\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  5\n",
      "['128691.ds.bz2', '392212.ds.bz2', '428809.ds.bz2', '59060.ds.bz2', '7841.ds.bz2', '30996.ds.bz2', '331890.ds.bz2', '9659.ds.bz2', '85655.ds.bz2', '109481.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.535\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  6\n",
      "['386211.ds.bz2', '236716.ds.bz2', '20645.ds.bz2', '161314.ds.bz2', '430973.ds.bz2', '57570.ds.bz2', '241016.ds.bz2', '43339.ds.bz2', '16125.ds.bz2', '42843.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.035\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  7\n",
      "['59557.ds.bz2', '1597.ds.bz2', '37601.ds.bz2', '99841.ds.bz2', '122222.ds.bz2', '354874.ds.bz2', '280862.ds.bz2', '163949.ds.bz2', '252313.ds.bz2', '20000.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.827\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  8\n",
      "['437924.ds.bz2', '430895.ds.bz2', '55888.ds.bz2', '95602.ds.bz2', '44493.ds.bz2', '214523.ds.bz2', '66506.ds.bz2', '289965.ds.bz2', '8600.ds.bz2', '22916.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.977\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  9\n",
      "['34603.ds.bz2', '423217.ds.bz2', '252786.ds.bz2', '162128.ds.bz2', '31109.ds.bz2', '1705.ds.bz2', '43198.ds.bz2', '44485.ds.bz2', '17892.ds.bz2', '593.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 3.436\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  10\n",
      "['356776.ds.bz2', '336892.ds.bz2', '150074.ds.bz2', '16184.ds.bz2', '532.ds.bz2', '159141.ds.bz2', '24910.ds.bz2', '30104.ds.bz2', '226964.ds.bz2', '298163.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.659\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  11\n",
      "['433268.ds.bz2', '329883.ds.bz2', '95126.ds.bz2', '26656.ds.bz2', '137171.ds.bz2', '100120.ds.bz2', '71008.ds.bz2', '174989.ds.bz2', '166645.ds.bz2', '29496.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 4.138\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  12\n",
      "['410744.ds.bz2', '391602.ds.bz2', '432870.ds.bz2', '110487.ds.bz2', '154417.ds.bz2', '389732.ds.bz2', '107466.ds.bz2', '125691.ds.bz2', '392190.ds.bz2', '22815.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.627\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  13\n",
      "['12870.ds.bz2', '419533.ds.bz2', '286862.ds.bz2', '134464.ds.bz2', '82867.ds.bz2', '265051.ds.bz2', '148506.ds.bz2', '293969.ds.bz2', '308926.ds.bz2', '24547.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.880\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  14\n",
      "['207917.ds.bz2', '28945.ds.bz2', '290768.ds.bz2', '322523.ds.bz2', '354589.ds.bz2', '23036.ds.bz2', '248960.ds.bz2', '284113.ds.bz2', '34599.ds.bz2', '257527.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.195\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  15\n",
      "['24426.ds.bz2', '2317.ds.bz2', '17348.ds.bz2', '33875.ds.bz2', '67629.ds.bz2', '7404.ds.bz2', '24061.ds.bz2', '385407.ds.bz2', '405726.ds.bz2', '384681.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 5.139\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  16\n",
      "['30936.ds.bz2', '87323.ds.bz2', '128787.ds.bz2', '158615.ds.bz2', '254906.ds.bz2', '253026.ds.bz2', '415529.ds.bz2', '286769.ds.bz2', '45415.ds.bz2', '410304.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.103\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  17\n",
      "['419443.ds.bz2', '380290.ds.bz2', '164067.ds.bz2', '321328.ds.bz2', '345000.ds.bz2', '438508.ds.bz2', '42961.ds.bz2', '29027.ds.bz2', '212227.ds.bz2', '432910.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.614\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  18\n",
      "['114745.ds.bz2', '27939.ds.bz2', '277191.ds.bz2', '158081.ds.bz2', '38513.ds.bz2', '257115.ds.bz2', '4355.ds.bz2', '170157.ds.bz2', '388713.ds.bz2', '366040.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.705\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  19\n",
      "['229232.ds.bz2', '165502.ds.bz2', '39680.ds.bz2', '153306.ds.bz2', '43274.ds.bz2', '65757.ds.bz2', '236582.ds.bz2', '354609.ds.bz2', '321009.ds.bz2', '397168.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.423\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  20\n",
      "['64412.ds.bz2', '27224.ds.bz2', '356720.ds.bz2', '421901.ds.bz2', '287190.ds.bz2', '22140.ds.bz2', '14753.ds.bz2', '420893.ds.bz2', '356731.ds.bz2', '1308.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.992\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  21\n",
      "['151145.ds.bz2', '435886.ds.bz2', '297875.ds.bz2', '441974.ds.bz2', '267396.ds.bz2', '214010.ds.bz2', '22308.ds.bz2', '289395.ds.bz2', '31279.ds.bz2', '348906.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.486\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  22\n",
      "['46195.ds.bz2', '35223.ds.bz2', '8282.ds.bz2', '4891.ds.bz2', '125234.ds.bz2', '393637.ds.bz2', '433461.ds.bz2', '289598.ds.bz2', '19852.ds.bz2', '17521.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.560\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  23\n",
      "['312016.ds.bz2', '181047.ds.bz2', '171854.ds.bz2', '235980.ds.bz2', '96186.ds.bz2', '78515.ds.bz2', '220102.ds.bz2', '357487.ds.bz2', '151375.ds.bz2', '6120.ds.bz2']\n",
      "ITERATION IN 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.626\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  24\n",
      "['407205.ds.bz2', '331734.ds.bz2', '7608.ds.bz2', '397770.ds.bz2', '9174.ds.bz2', '16297.ds.bz2', '281361.ds.bz2', '358204.ds.bz2', '179227.ds.bz2', '205930.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 3.363\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  25\n",
      "['24816.ds.bz2', '32806.ds.bz2', '396217.ds.bz2', '390969.ds.bz2', '28467.ds.bz2', '2777.ds.bz2', '25953.ds.bz2', '45790.ds.bz2', '30615.ds.bz2', '159020.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 3.929\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  26\n",
      "['273342.ds.bz2', '366430.ds.bz2', '383693.ds.bz2', '337041.ds.bz2', '74230.ds.bz2', '25598.ds.bz2', '378810.ds.bz2', '272439.ds.bz2', '33128.ds.bz2', '38121.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 5.519\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  27\n",
      "['24944.ds.bz2', '381911.ds.bz2', '18264.ds.bz2', '10998.ds.bz2', '271628.ds.bz2', '303032.ds.bz2', '27407.ds.bz2', '246973.ds.bz2', '127807.ds.bz2', '192423.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.405\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  28\n",
      "['278199.ds.bz2', '33996.ds.bz2', '87627.ds.bz2', '96388.ds.bz2', '38066.ds.bz2', '5734.ds.bz2', '186023.ds.bz2', '365301.ds.bz2', '295769.ds.bz2', '248563.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.953\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  29\n",
      "['415470.ds.bz2', '43687.ds.bz2', '107861.ds.bz2', '369328.ds.bz2', '312664.ds.bz2', '161643.ds.bz2', '225925.ds.bz2', '191904.ds.bz2', '380637.ds.bz2', '304786.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.563\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  30\n",
      "['34113.ds.bz2', '13654.ds.bz2', '440604.ds.bz2', '314699.ds.bz2', '168982.ds.bz2', '16183.ds.bz2', '171507.ds.bz2', '5656.ds.bz2', '425473.ds.bz2', '152857.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 4.701\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  31\n",
      "['117937.ds.bz2', '165768.ds.bz2', '253892.ds.bz2', '33428.ds.bz2', '310436.ds.bz2', '109645.ds.bz2', '425882.ds.bz2', '85451.ds.bz2', '2749.ds.bz2', '27893.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.824\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  32\n",
      "['36261.ds.bz2', '243101.ds.bz2', '66703.ds.bz2', '129663.ds.bz2', '147637.ds.bz2', '152300.ds.bz2', '150083.ds.bz2', '29606.ds.bz2', '167814.ds.bz2', '124723.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.228\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  33\n",
      "['251764.ds.bz2', '412133.ds.bz2', '274635.ds.bz2', '42656.ds.bz2', '241945.ds.bz2', '28273.ds.bz2', '25074.ds.bz2', '35464.ds.bz2', '433315.ds.bz2', '44744.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.315\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  34\n",
      "['136675.ds.bz2', '222563.ds.bz2', '8449.ds.bz2', '334349.ds.bz2', '351975.ds.bz2', '124562.ds.bz2', '9867.ds.bz2', '122701.ds.bz2', '418372.ds.bz2', '359625.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.092\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  35\n",
      "['295517.ds.bz2', '33039.ds.bz2', '248995.ds.bz2', '4545.ds.bz2', '428140.ds.bz2', '41137.ds.bz2', '127512.ds.bz2', '286767.ds.bz2', '21345.ds.bz2', '5215.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.564\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  36\n",
      "['199010.ds.bz2', '41224.ds.bz2', '34899.ds.bz2', '354791.ds.bz2', '71179.ds.bz2', '272227.ds.bz2', '308673.ds.bz2', '264454.ds.bz2', '4298.ds.bz2', '322812.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.899\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  37\n",
      "['367387.ds.bz2', '255339.ds.bz2', '171173.ds.bz2', '41164.ds.bz2', '45428.ds.bz2', '4206.ds.bz2', '28515.ds.bz2', '133913.ds.bz2', '29901.ds.bz2', '43786.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 3.892\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  38\n",
      "['327557.ds.bz2', '142708.ds.bz2', '347295.ds.bz2', '284586.ds.bz2', '158979.ds.bz2', '175366.ds.bz2', '7431.ds.bz2', '18023.ds.bz2', '433586.ds.bz2', '82009.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.903\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  39\n",
      "['35026.ds.bz2', '313101.ds.bz2', '405731.ds.bz2', '242884.ds.bz2', '379500.ds.bz2', '106030.ds.bz2', '18577.ds.bz2', '106637.ds.bz2', '30179.ds.bz2', '128688.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.293\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  40\n",
      "['149667.ds.bz2', '44484.ds.bz2', '34546.ds.bz2', '3619.ds.bz2', '377978.ds.bz2', '149295.ds.bz2', '292275.ds.bz2', '292732.ds.bz2', '254941.ds.bz2', '44457.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.544\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  41\n",
      "['421364.ds.bz2', '15539.ds.bz2', '120991.ds.bz2', '107598.ds.bz2', '37653.ds.bz2', '186613.ds.bz2', '7570.ds.bz2', '172249.ds.bz2', '29364.ds.bz2', '300273.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.830\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  42\n",
      "['69823.ds.bz2', '422245.ds.bz2', '313147.ds.bz2', '36663.ds.bz2', '414859.ds.bz2', '6395.ds.bz2', '253936.ds.bz2', '31600.ds.bz2', '34972.ds.bz2', '341397.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.216\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  43\n",
      "['185435.ds.bz2', '44911.ds.bz2', '230014.ds.bz2', '11047.ds.bz2', '440746.ds.bz2', '4462.ds.bz2', '32706.ds.bz2', '18388.ds.bz2', '2327.ds.bz2', '59977.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.438\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  44\n",
      "['322833.ds.bz2', '416082.ds.bz2', '22152.ds.bz2', '321244.ds.bz2', '403770.ds.bz2', '36843.ds.bz2', '4346.ds.bz2', '411697.ds.bz2', '416564.ds.bz2', '197837.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.886\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  45\n",
      "['24063.ds.bz2', '17026.ds.bz2', '187331.ds.bz2', '1256.ds.bz2', '423341.ds.bz2', '223933.ds.bz2', '421606.ds.bz2', '257986.ds.bz2', '427914.ds.bz2', '171824.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.333\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  46\n",
      "['316642.ds.bz2', '109446.ds.bz2', '2526.ds.bz2', '234730.ds.bz2', '289145.ds.bz2', '236631.ds.bz2', '17630.ds.bz2', '113311.ds.bz2', '183695.ds.bz2', '45508.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.601\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  47\n",
      "['144531.ds.bz2', '126863.ds.bz2', '433738.ds.bz2', '441056.ds.bz2', '102862.ds.bz2', '157559.ds.bz2', '108861.ds.bz2', '322071.ds.bz2', '44304.ds.bz2', '37291.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.866\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  48\n",
      "['21410.ds.bz2', '421530.ds.bz2', '212722.ds.bz2', '28812.ds.bz2', '328144.ds.bz2', '46158.ds.bz2', '330967.ds.bz2', '352631.ds.bz2', '332137.ds.bz2', '256951.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.454\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  49\n",
      "['19533.ds.bz2', '25499.ds.bz2', '2669.ds.bz2', '22782.ds.bz2', '166813.ds.bz2', '320985.ds.bz2', '84313.ds.bz2', '333611.ds.bz2', '282848.ds.bz2', '376933.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.949\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  50\n",
      "['312771.ds.bz2', '87337.ds.bz2', '26400.ds.bz2', '409503.ds.bz2', '1759.ds.bz2', '129998.ds.bz2', '426328.ds.bz2', '343387.ds.bz2', '328991.ds.bz2', '309986.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 2.202\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  51\n",
      "['385481.ds.bz2', '27717.ds.bz2', '23303.ds.bz2', '252408.ds.bz2', '12766.ds.bz2', '277212.ds.bz2', '366341.ds.bz2', '37776.ds.bz2', '6288.ds.bz2', '33142.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.905\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  52\n",
      "['108076.ds.bz2', '19867.ds.bz2', '159808.ds.bz2', '407588.ds.bz2', '336670.ds.bz2', '234643.ds.bz2', '275279.ds.bz2', '348665.ds.bz2', '6639.ds.bz2', '180095.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n",
      "ITERATION IN 2\n",
      "ITERATION IN 3\n",
      "ITERATION IN 4\n",
      "Avg. time taken by 1 file(1 iteration): 1.654\n",
      "Weights Saved:  outputs/train_nnet_t1543721276199.p\n",
      "Batch:  53\n",
      "['110678.ds.bz2', '54828.ds.bz2', '104145.ds.bz2', '63111.ds.bz2', '104063.ds.bz2', '17084.ds.bz2', '140228.ds.bz2', '78949.ds.bz2', '186431.ds.bz2', '114120.ds.bz2']\n",
      "ITERATION IN 0\n",
      "ITERATION IN 1\n"
     ]
    }
   ],
   "source": [
    "# Complete Training\n",
    "# tips: try increasing iterations per batch\n",
    "trainingStatus = defaultdict(lambda: bool(False)) # Reset it after 3 epochs of full-training set\n",
    "train.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test on training set\n",
    "test(loaded_SKT, loaded_DCS, n_testSet=100, _testFiles=TrainFiles, n_checkpt = 10)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Checkpoint... \n",
      "50  Checkpoint... \n",
      "100  Checkpoint... \n",
      "150  Checkpoint... \n",
      "200  Checkpoint... \n",
      "250  Checkpoint... \n",
      "300  Checkpoint... \n",
      "350  Checkpoint... \n",
      "400  Checkpoint... \n",
      "450  Checkpoint... \n",
      "500  Checkpoint... \n",
      "550  Checkpoint... \n",
      "600  Checkpoint... \n",
      "650  Checkpoint... \n",
      "Avg. Micro Recall of Lemmas: 0.8369512331454708\n",
      "Avg. Micro Recall of Words: 0.6885798895409492\n",
      "Avg. Micro Precision of Lemmas: 0.78979303122867\n",
      "Avg. Micro Precision of Words: 0.6511872539596757\n"
     ]
    }
   ],
   "source": [
    "# POST TEST 1K SET\n",
    "_ = test(loaded_SKT, loaded_DCS, n_testSet = -1, _testFiles = TestFiles, n_checkpt = 50)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test on Larger Set\n",
    "_ = test(loaded_SKT, loaded_DCS, n_testSet = 3000, _testFiles = TestFiles_2, n_checkpt = 100)\n",
    "playBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Saved:  outputs/train_nnet_t526539685574.p\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.neuralnet.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.neuralnet.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10250.p2\n"
     ]
    }
   ],
   "source": [
    "for fn in TrainFiles[:10]:\n",
    "    dsbz2_name = bz2_input_folder + fn\n",
    "    fn = fn.replace('.ds.bz2', '.p2')\n",
    "    print(fn)\n",
    "    ret = trainer.Test(loaded_SKT[fn], loaded_DCS[fn], dsbz2_name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD_Node[C: 0, P: 0, Adi @(31) => Adi]\n",
      "WD_Node[C: 1, P: 0, ca @(2) => ca]\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "WD.word_definite_extInit(matDB)\n",
    "# node2 = WD.word_definite('tIkzRam', 'tIkzRa', 31, 0, 0)\n",
    "node1 = WD.word_definite('Adi', 'Adi', 31, 0, 0)\n",
    "node2 = WD.word_definite('ca', 'ca', 2, 0, 1)\n",
    "# node1 = WD.word_definite('koRam', 'koRa', 31, 0, 1)\n",
    "print(node1)\n",
    "print(node2)\n",
    "\n",
    "feats = WD.Get_Features(node1, node2)\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.44093286,  0.54536647,  0.26590788,  0.89111612,\n",
       "         0.33948032, -0.03414859],\n",
       "       [ 0.84916065,  0.        ,  1.1908067 ,  1.00455444,  1.54897616,\n",
       "         0.94725297,  1.38592746],\n",
       "       [-0.40401637, -0.45824241,  0.        , -0.66569737,  0.02653288,\n",
       "        -0.6492698 , -0.28316029],\n",
       "       [-0.28461703, -0.35128149, -0.57814776,  0.        ,  0.53596003,\n",
       "        -0.61933034, -0.76127807],\n",
       "       [ 0.50401231,  0.48448779,  1.19736903,  0.747994  ,  0.        ,\n",
       "         0.75652086,  1.44398605],\n",
       "       [ 0.22824487,  0.24474065,  0.41312113,  0.37012238,  0.5754286 ,\n",
       "         0.        ,  0.5124145 ],\n",
       "       [-0.39726662, -0.46351106, -0.49387717, -0.53360749, -0.06269749,\n",
       "        -0.61960008,  0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wmat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 4, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######   THE END\n",
    "-----------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ENV(py3_bishal)",
   "language": "python",
   "name": "py3_bishal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
