{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bUILT-iN pACKAGES\n",
    "import sys, os, time, bz2, zlib, pickle, math, json, csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "from IPython.display import display\n",
    "\n",
    "## lAST sUMMER\n",
    "from romtoslp import *\n",
    "from sentences import *\n",
    "from DCS import *\n",
    "import MatDB\n",
    "from heap_n_PrimMST import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_dsbz2(filename):\n",
    "    with bz2.BZ2File(filename, 'r') as f:\n",
    "        loader = pickle.load(f)\n",
    "    \n",
    "#     conflicts_Dict_correct = loader['conflicts_Dict_correct']\n",
    "#     nodelist_to_correct_mapping = loader['nodelist_to_correct_mapping']\n",
    "#     nodelist_correct = loader['nodelist_correct']\n",
    "#     featVMat_correct = loader['featVMat_correct']\n",
    "#     featVMat = loader['featVMat']\n",
    "#     conflicts_Dict = loader['conflicts_Dict']\n",
    "#     nodelist = loader['nodelist']\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_DCS = pickle.load(open('../Simultaneous_DCS_10K.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_folder = '../NewData/skt_dcs_DS.bz2_10K/'\n",
    "file_list = os.listdir(in_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "f_k = 10\n",
    "f_name = file_list[f_k]\n",
    "f_path = os.path.join(in_folder, f_name)\n",
    "loader = open_dsbz2(f_path)\n",
    "nodelist = loader['nodelist']\n",
    "conflicts_Dict = loader['conflicts_Dict']\n",
    "nodelist_correct = loader['nodelist_correct']\n",
    "n_nodes = len(nodelist)\n",
    "\n",
    "dcsObj = loaded_DCS[f_name.replace('.ds.bz2', '.p2')]\n",
    "print(len(nodelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['nodelist', 'nodelist_to_correct_mapping', 'conflicts_Dict', 'featVMat', 'conflicts_Dict_correct', 'nodelist_correct', 'featVMat_correct'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mark_further(_partial_sentence, small_nodelist, conflicts_Dict, glob_to_local, full_set, local_i):\n",
    "    if local_i < len(_partial_sentence):\n",
    "        while _partial_sentence[local_i] == 2:\n",
    "            local_i += 1\n",
    "            if local_i == len(_partial_sentence):\n",
    "                break\n",
    "            continue\n",
    "        \n",
    "    if local_i == len(_partial_sentence):\n",
    "#         print(_partial_sentence)\n",
    "        full_set.append(_partial_sentence)\n",
    "        return\n",
    "    \n",
    "    global_i = small_nodelist[local_i].id\n",
    "    \n",
    "    if local_i < len(small_nodelist) - 1:\n",
    "        partial_sentence = _partial_sentence[:]\n",
    "        partial_sentence[local_i] = 0\n",
    "        mark_further(partial_sentence, small_nodelist, conflicts_Dict, glob_to_local, full_set, local_i+1)\n",
    "    \n",
    "    partial_sentence = _partial_sentence[:]\n",
    "    partial_sentence[local_i] = 1\n",
    "    for conflict in conflicts_Dict[global_i]:\n",
    "        partial_sentence[glob_to_local[conflict]] = 2\n",
    "    mark_further(partial_sentence, small_nodelist, conflicts_Dict, glob_to_local, full_set, local_i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options:  18\n",
      "Options:  1\n",
      "Options:  87\n",
      "Options:  348\n",
      "Total sentences:  544968\n"
     ]
    }
   ],
   "source": [
    "max_cid = 0\n",
    "node_dict = defaultdict(list)\n",
    "for i in range(len(nodelist)):\n",
    "    node = nodelist[i]\n",
    "    node.id = i\n",
    "    max_cid = max(node.chunk_id, max_cid)\n",
    "    node_dict[node.chunk_id].append(node)\n",
    "    \n",
    "\n",
    "full_words = []\n",
    "for word in re.split('[ ]+', dcsObj.sentence.strip()):\n",
    "    full_words.append(rom_slp(word))\n",
    "\n",
    "valid_set = defaultdict(list)\n",
    "for cid in range(max_cid + 1):\n",
    "    word = full_words[cid]\n",
    "    \n",
    "    full_set = []\n",
    "    \n",
    "    small_nodelist = node_dict[cid]\n",
    "    glob_to_local = {}\n",
    "    for i in range(len(small_nodelist)):\n",
    "        glob_to_local[small_nodelist[i].id] = i\n",
    "    \n",
    "    sentence_mark = [0]*len(small_nodelist)\n",
    "    mark_further(sentence_mark, small_nodelist, conflicts_Dict, glob_to_local, full_set, 0)\n",
    "    \n",
    "    \n",
    "    for S in full_set:\n",
    "        total_possible_len = 0\n",
    "        S = np.array(S)\n",
    "        S[S==2] = 0\n",
    "        Y = np.where(S)[0]\n",
    "        nc = Y.shape[0]\n",
    "        for y in Y:\n",
    "            total_possible_len += len(small_nodelist[y].derived)\n",
    "        if total_possible_len >= len(word) - 1:\n",
    "            repeat = False\n",
    "            for other_s in valid_set[cid]:\n",
    "                if np.sum(other_s^S) == len(small_nodelist):\n",
    "                    repeat = True\n",
    "            if not repeat:\n",
    "                valid_set[cid].append(S)\n",
    "#             print('Valid: ', S)\n",
    "#         else:\n",
    "#             print('InValid: ', S)\n",
    "valid_set = dict(valid_set)\n",
    "total_valid_sets = 1\n",
    "for cid, ss in valid_set.items():\n",
    "    valid_set[cid] = list(map(lambda x: np.where(x)[0], ss))\n",
    "    print('Options: ', len(valid_set[cid]))\n",
    "    total_valid_sets *= len(valid_set[cid])\n",
    "print('Total sentences: ', total_valid_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WD_Node[C: 0, P: 0, vAc @(3) => vAk],\n",
       " WD_Node[C: 0, P: 0, vAc @(30) => vAk],\n",
       " WD_Node[C: 0, P: 0, vAgdaRqa @(3) => vAgdaRqa],\n",
       " WD_Node[C: 0, P: 8, ja @(31) => jam],\n",
       " WD_Node[C: 0, P: 8, ja @(69) => jam],\n",
       " WD_Node[C: 0, P: 8, ja @(71) => jam],\n",
       " WD_Node[C: 0, P: 3, daRqa @(3) => daRqa],\n",
       " WD_Node[C: 1, P: 0, ca @(2) => ca],\n",
       " WD_Node[C: 2, P: 0, pAruzya @(31) => pAruzyam],\n",
       " WD_Node[C: 2, P: 0, pAruzya @(69) => pAruzyam],\n",
       " WD_Node[C: 2, P: 0, pAruzya @(71) => pAruzyam],\n",
       " WD_Node[C: 3, P: 0, kroDa @(3) => kroDa],\n",
       " WD_Node[C: 3, P: 5, ja @(29) => jas],\n",
       " WD_Node[C: 4, P: 0, api @(2) => api],\n",
       " WD_Node[C: 5, P: 0, gaRa @(29) => gaRas],\n",
       " WD_Node[C: 6, P: 0, azwaka @(29) => azwakas]]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sArAmuKo', 'dIrGaSUko', 'loDraSUkaH', 'suganDikaH']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodelist_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WD_Node[C: 0, P: 0, sArAmuKa @(1) => sArAmuKa],\n",
       " WD_Node[C: 1, P: 0, dIrGaSUka @(1) => dIrGaSUka],\n",
       " WD_Node[C: 2, P: 0, loDraSUka @(1) => loDraSUka],\n",
       " WD_Node[C: 3, P: 0, suganDika @(1) => suganDika]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ENV(py3_bishal)",
   "language": "python",
   "name": "py3_bishal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
