{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Header"
    ]
   },
   "source": [
    "#### MP based asynchronous testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "tags": [
     "Header"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import TestPool_Unit\n",
    "from shutil import copyfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Evaluate(result_arr):\n",
    "    print('Files Processed: ', len(result_arr))\n",
    "    recalls = []\n",
    "    recalls_of_word = []\n",
    "    precisions = []\n",
    "    precisions_of_words = []\n",
    "    fully_Correct_l = 0\n",
    "    fully_Correct_w = 0\n",
    "    for entry in result_arr:\n",
    "        (word_match, lemma_match, n_dcsWords, n_output_nodes) = entry\n",
    "        recalls.append(lemma_match/n_dcsWords)\n",
    "        recalls_of_word.append(word_match/n_dcsWords)\n",
    "\n",
    "        precisions.append(lemma_match/n_output_nodes)\n",
    "        precisions_of_words.append(word_match/n_output_nodes)\n",
    "        if lemma_match == n_dcsWords:\n",
    "            fully_Correct_l += 1\n",
    "        if word_match == n_dcsWords:\n",
    "            fully_Correct_w += 1\n",
    "    print('Avg. Micro Recall of Lemmas: {}'.format(np.mean(np.array(recalls))))\n",
    "    print('Avg. Micro Recall of Words: {}'.format(np.mean(np.array(recalls_of_word))))\n",
    "    print('Avg. Micro Precision of Lemmas: {}'.format(np.mean(np.array(precisions))))\n",
    "    print('Avg. Micro Precision of Words: {}'.format(np.mean(np.array(precisions_of_words))))\n",
    "    rl = np.mean(np.array(recalls))\n",
    "    pl = np.mean(np.array(precisions))\n",
    "    print('F-Score of Lemmas: ', (2*pl*rl)/(pl+rl))\n",
    "    print('Fully Correct Lemmawise: {}'.format(fully_Correct_l/len(recalls_of_word)))\n",
    "    print('Fully Correct Wordwise: {}'.format(fully_Correct_w/len(recalls_of_word)))\n",
    "    print('[{:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}]'.format(100*np.mean(np.array(recalls)), 100*np.mean(np.array(recalls_of_word)), 100*np.mean(np.array(precisions)), \\\n",
    "           100*np.mean(np.array(precisions_of_words)), 100*(2*pl*rl)/(pl+rl), 100*fully_Correct_l/len(recalls_of_word),\\\n",
    "           100*fully_Correct_w/len(recalls_of_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child process with vpid:0, pid:22127 started.\n",
      "Child process with vpid:1, pid:22128 started.\n",
      "Child process with vpid:2, pid:22131 started.\n",
      "Child process with vpid:3, pid:22134 started.\n",
      "Keep Prob = 0.6, Dropout = 0.4\n",
      "Keep Prob = 0.6, Dropout = 0.4\n",
      "Keep Prob = 0.6, Dropout = 0.4\n",
      "Keep Prob = 0.6, Dropout = 0.4\n",
      "vpid:2: Range is 4800 -> 7200 / 9577\n",
      "vpid:0: Range is 0 -> 2400 / 9577\n",
      "vpid:1: Range is 2400 -> 4800 / 9577\n",
      "vpid:3: Range is 7200 -> 9600 / 9577\n"
     ]
    }
   ],
   "source": [
    "modelFile = 'outputs/train_t2756013734745/nnet_e1_i400.p' #PM3\n",
    "_dump = False\n",
    "if _dump:\n",
    "    _outFile = 'outputs/dump_predictions/PM3_NLoss'\n",
    "else:\n",
    "    _outFile = None\n",
    "\n",
    "# Backup the model file\n",
    "copyfile(modelFile, modelFile + '.bk')\n",
    "\n",
    "# Create Queue, Result array\n",
    "queue = mp.Queue()\n",
    "result_arr = []\n",
    "\n",
    "# Start 6 workers - 8 slows down the pc\n",
    "proc_count = 4\n",
    "procs = [None]*proc_count\n",
    "for i in range(proc_count):\n",
    "    vpid = i\n",
    "    procs[i] = mp.Process(target = TestPool_Unit.pooled_Test, args = \\\n",
    "                          (modelFile, vpid, queue, '../NewData/skt_dcs_DS.bz2_1L_pmi_mir_heldout_again/', int(9600/proc_count), _dump, _outFile))\n",
    "# Start Processes\n",
    "for i in range(proc_count):\n",
    "    procs[i].start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files Processed:  5985\n",
      "Avg. Micro Recall of Lemmas: 0.8412894676055079\n",
      "Avg. Micro Recall of Words: 0.6757667871931413\n",
      "Avg. Micro Precision of Lemmas: 0.7936086846797896\n",
      "Avg. Micro Precision of Words: 0.6385993466612693\n",
      "F-Score of Lemmas:  0.816753786024\n",
      "Fully Correct Lemmawise: 0.38395989974937345\n",
      "Fully Correct Wordwise: 0.1241436925647452\n",
      "[84.13, 67.58, 79.36, 63.86, 81.68, 38.40, 12.41]\n"
     ]
    }
   ],
   "source": [
    "# Fetch partial results\n",
    "while not queue.empty():\n",
    "    result_arr.append(queue.get())\n",
    "# Evaluate results till now\n",
    "Evaluate(result_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process with\t vpid: 0\t ->\t pid: 22127\t ->\t running status: True\n",
      "Process with\t vpid: 1\t ->\t pid: 22128\t ->\t running status: True\n",
      "Process with\t vpid: 2\t ->\t pid: 22131\t ->\t running status: True\n",
      "Process with\t vpid: 3\t ->\t pid: 22134\t ->\t running status: True\n"
     ]
    }
   ],
   "source": [
    "# Check status\n",
    "for i in range(proc_count):\n",
    "    p = procs[i]\n",
    "    print('Process with\\t vpid: {}\\t ->\\t pid: {}\\t ->\\t running status: {}'.format(i, p.pid, p.is_alive()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Properly Join\n",
    "for i in range(proc_count):\n",
    "    procs[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Force Terminate\n",
    "for p in procs:\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7729026036644164"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([a[2] for a in result_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 1, Samples: 146, Mean: 0.7328767123287672\n",
      "C: 2, Samples: 313, Mean: 0.8482428115015974\n",
      "C: 3, Samples: 216, Mean: 0.875\n",
      "C: 4, Samples: 191, Mean: 0.8756544502617801\n",
      "C: 5, Samples: 478, Mean: 0.9041841004184101\n",
      "C: 6, Samples: 1479, Mean: 0.9195402298850575\n",
      "C: 7, Samples: 1976, Mean: 0.9254626951995374\n",
      "C: 8, Samples: 1403, Mean: 0.9167854597291518\n",
      "C: 9, Samples: 519, Mean: 0.9096553200599445\n",
      "C: 10, Samples: 152, Mean: 0.9151315789473685\n",
      "C: 11, Samples: 69, Mean: 0.9064558629776023\n",
      "C: 12, Samples: 57, Mean: 0.8947368421052633\n",
      "C: 13, Samples: 20, Mean: 0.8807692307692309\n",
      "C: 14, Samples: 11, Mean: 0.9220779220779222\n",
      "C: 15, Samples: 13, Mean: 0.8871794871794874\n",
      "C: 16, Samples: 6, Mean: 0.8958333333333334\n",
      "C: 17, Samples: 9, Mean: 0.8888888888888888\n",
      "C: 18, Samples: 7, Mean: 0.9285714285714286\n",
      "C: 19, Samples: 4, Mean: 0.8552631578947368\n",
      "C: 20, Samples: 1, Mean: 0.95\n",
      "C: 21, Samples: 4, Mean: 0.9523809523809523\n",
      "C: 22, Samples: 1, Mean: 0.6363636363636364\n",
      "C: 23, Samples: 2, Mean: 0.8913043478260869\n",
      "C: 24, Samples: 1, Mean: 0.75\n",
      "C: 26, Samples: 2, Mean: 0.8846153846153846\n",
      "C: 28, Samples: 2, Mean: 0.8928571428571428\n",
      "C: 30, Samples: 1, Mean: 0.9333333333333333\n",
      "C: 32, Samples: 1, Mean: 0.8125\n",
      "C: 35, Samples: 1, Mean: 0.8571428571428571\n",
      "C: 37, Samples: 1, Mean: 0.918918918918919\n",
      "C: 46, Samples: 1, Mean: 0.9347826086956522\n"
     ]
    }
   ],
   "source": [
    "result_arr2 = [list(x) for x in result_arr]\n",
    "\n",
    "recalls = defaultdict(list)\n",
    "for i in range(len(result_arr2)):\n",
    "    result_arr2[i][0] /= result_arr2[i][2]\n",
    "    result_arr2[i][1] /= result_arr2[i][2]\n",
    "    recalls[result_arr2[i][2]].append(result_arr2[i][1])\n",
    "\n",
    "for c in sorted(recalls.keys()):\n",
    "    print('C: {}, Samples: {}, Mean: {}'.format(c, len(recalls[c]), np.mean(recalls[c])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ENV(py3_bishal)",
   "language": "python",
   "name": "py3_bishal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
